{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2caf5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "#import AEs\n",
    "from autoencoder_classes import(\n",
    "    Autoencoder1, Autoencoder2, Autoencoder3, Autoencoder4, Autoencoder5, Autoencoder6, Autoencoder7, Autoencoder8, Autoencoder9, \n",
    "    Autoencoder10, Autoencoder11, Autoencoder12, Autoencoder13, Autoencoder14, Autoencoder15\n",
    ")\n",
    "\n",
    "\n",
    "AUTOENCODER_CLASSES = {\n",
    "    \"Autoencoder1\": Autoencoder1,\n",
    "    \"Autoencoder2\": Autoencoder2,\n",
    "    \"Autoencoder3\": Autoencoder3,\n",
    "    \"Autoencoder4\": Autoencoder4,\n",
    "    \"Autoencoder5\": Autoencoder5,\n",
    "    \"Autoencoder6\": Autoencoder6,\n",
    "    \"Autoencoder7\": Autoencoder7,\n",
    "    \"Autoencoder8\": Autoencoder8,\n",
    "    \"Autoencoder9\": Autoencoder9,\n",
    "    \"Autoencoder10\": Autoencoder10,\n",
    "    \"Autoencoder11\": Autoencoder11,\n",
    "    \"Autoencoder12\": Autoencoder12,\n",
    "    \"Autoencoder13\": Autoencoder13,\n",
    "    \"Autoencoder14\": Autoencoder14,\n",
    "    \"Autoencoder15\": Autoencoder15\n",
    "}\n",
    "\n",
    "\n",
    "#import MT\n",
    "import sys\n",
    "sys.path.insert(0, '/data/shiyu/projects/MT/MT_ICML_OOP')\n",
    "from MT_networks_wrapper import MTNetworksWrapper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    # random.seed(seed) # Python built‑in RNG\n",
    "    np.random.seed(seed) # NumPy RNG\n",
    "    torch.manual_seed(seed) # Torch CPU RNG\n",
    "\n",
    "    # Torch CUDA RNG (all GPUs)\n",
    "    torch.cuda.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # For deterministic behavior (may slow down training):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) #env‑level reproducibility\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58117ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_multiplications_per_sample_AE(model):\n",
    "    total_mults = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            mults = m.in_features * m.out_features\n",
    "            total_mults += mults\n",
    "    return total_mults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b171ad",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29560553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "Loading test data...\n",
      "Training data shape: (100000, 2048)\n",
      "Test data shape: (20000, 2048)\n",
      "Ambient dimension (D): 2048\n",
      "Intrinsic dimension (d): 2\n",
      "Noise level (sigma): 0.01\n"
     ]
    }
   ],
   "source": [
    "sigma = 0.01\n",
    "d = 2 \n",
    "\n",
    "\"\"\"Load training and test data.\"\"\"\n",
    "print(\"Loading training data...\")\n",
    "train_file = os.path.join('/data/shiyu/projects/MT/MT_ICML_OOP/data/GW_data', 'datawaves_100000_spinsFalse_nonuniform.npy')\n",
    "train_waves = np.load(train_file)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_file = os.path.join('/data/shiyu/projects/MT/MT_ICML_OOP/data/GW_data', 'datawaves_20000_spinsFalse_nonuniform.npy')\n",
    "test_waves = np.load(test_file)\n",
    "\n",
    "\n",
    "\n",
    "X_natural_train = train_waves\n",
    "X_natural_test = test_waves\n",
    "\n",
    "\n",
    "N_train = X_natural_train.shape[0]\n",
    "N_test = X_natural_test.shape[0]\n",
    "D = X_natural_train.shape[1]\n",
    "\n",
    "\n",
    "X_train = X_natural_train + sigma * np.random.randn(*X_natural_train.shape)\n",
    "X_test = X_natural_test + sigma * np.random.randn(*X_natural_test.shape)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Ambient dimension (D): {D}\")\n",
    "print(f\"Intrinsic dimension (d): {d}\")\n",
    "print(f\"Noise level (sigma): {sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d16561",
   "metadata": {},
   "source": [
    "# train and analyze AEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c36fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d32c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Autoencoder1\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.1597599655389786\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.14144807457923889\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.12246029168367385\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.10338999897241592\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.08533403754234314\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.06911351770162583\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.05511708825826645\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.043418491780757906\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.03388651967048645\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.026269968450069427\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.020271028354763986\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.015595545768737793\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.011979794576764107\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.009200317151844502\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.007074038572609425\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.005454024504870176\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.004223942765966058\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.0032926121819764377\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.002589141624048352\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.0020587899442762138\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.0016595304198563099\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.001359255309216678\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.0011335372226312756\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.0009638582356274128\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.0008362251869402826\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.000740094764623791\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.0006675440724939108\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.0006126342224888504\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.0005709235509857535\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.000539096649736166\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.0005146826803684234\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.0004958421341143548\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.0004812062392011285\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00046975630219094457\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.0004607334767933935\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.0004535713791847229\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0004478459747042507\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00044323835987597704\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.0004395071777980775\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.000436468351399526\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.0004339803964830935\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.00043193378020077943\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00043024314334616065\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.0004288414062466472\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.0004276755370665342\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00042670320137403904\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00042589038261212406\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.000425209499662742\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.0004246380948461592\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00042415778851136567\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.0004237534757703543\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.000423412648960948\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.0004231249250005931\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.0004228816321119666\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.0004226755711715668\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00042250071419402957\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.0004223520192317665\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00042222528834827243\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00042211703024804594\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.0004220242949668318\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00042194464360363783\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.000421876012114808\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.0004218166822101921\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.0004217652278020978\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00042172043933533135\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.00042168133193627\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00042164705228060485\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.0004216169111896306\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.0004215903289150447\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.0004215667839162052\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.0004215458547696471\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.00042152718990109863\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00042151048430241647\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.0004214954748749733\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.0004214819346088916\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.0004214696818962693\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.0004214585607405752\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.0004214484244585037\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.0004214391682762653\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00042143068858422337\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.0004214228969067335\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.0004214157199021429\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00042140907375141977\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00042140290956012907\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00042139717028476297\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.00042139180470258\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.0004213867639191449\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.0004213820374570787\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00042137759970501065\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.0004213734285440296\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00042136948206461966\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.0004213657439686358\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00042136220377869904\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00042135883588343857\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.0004213556426111609\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00042135260184295476\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.0004213496937882155\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.0004213469196110964\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00042134424089454117\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.0004213416762650013\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00042133920593187215\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00042133682523854077\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00042133452487178144\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00042133230017498134\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.0004213301429990679\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00042132803588174284\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.0004213259927928448\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.000421324004419148\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.0004213220498058945\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00042132014059461655\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00042131828144192695\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00042131645255722106\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.0004213146504480392\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.0004213128762785345\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00042131111957132816\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00042130935355089605\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.000421307566575706\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00042130577145144344\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00042130399262532594\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.000421302200993523\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00042130038258619606\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.0004212987353093922\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.0004213662550318986\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.0004213240440003574\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.0004213149042334408\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.0004213025269564241\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00042129606008529664\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.0004212918400298804\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.0004212913371156901\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.0004212891240604222\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00042128630680963395\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00042128163506276905\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.0004212786816060543\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.0004212770937010646\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.0004213520255871117\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00042128846398554743\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00042128674569539724\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.0004212788410950452\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.0004212741192895919\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.0004212795733474195\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00042128007975406946\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.0004212774010375142\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00042127378517761824\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.0004212910996284336\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.0004212782299146056\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00042126509244553743\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00042126398300752044\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00042126367101445793\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00042126240907236934\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00042126058135181664\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.0004212574055418372\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00042125467909500004\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.0004212485300377011\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.0004212407604791224\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.0004212515091057867\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.0004212478257250041\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.0004212565952911973\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.0004212560853920877\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.0004212253447622061\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00042122404323890805\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.0004212345811538398\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.0004212754813488573\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.0004212126473430544\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00042120361118577423\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.0004211982956621796\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00042139817262068393\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.0004212650062981993\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.0004211845446843654\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00042117702774703504\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.0004211713757831603\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.0004211663245223463\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.0004211627796757966\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.0004212702310178429\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.000421170350164175\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00042115471907891336\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.000421145660802722\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00042115356191061435\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.0004212359758093953\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00042114428826607765\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00042116083321161566\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.000421123526757583\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.0004211107851006091\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00042118207900784907\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00042130791349336504\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00042115141986869274\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.00042109019472263753\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.0004210827429778874\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.0004210790665820241\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00042108717607334254\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.0004211147001478821\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.0004210508079268038\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.0004210464691277593\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00042104349471628667\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00042109751724638044\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00042111029266379775\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00042105786153115335\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00042100253864191473\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00042099279235117135\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00042099795304238795\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.0004210203350521624\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.0004209646873641759\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00042109688627533614\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.00042101723374798894\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.0004209446965251118\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.0004209205275401473\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.0004208973655477166\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00042088020825758576\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.0004208654013928026\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.0004208747984375805\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.000421122710686177\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00042089403024874627\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.0004208382195793092\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.0004208245384506881\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.0004207964858505875\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00042078272788785396\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00042077272199094297\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.0004207547090481967\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00042073461227118967\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.0004207249137107283\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00042071340489201246\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00042065485729835927\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00042063748813234267\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00042061896296218036\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00042064037406817076\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00042061141808517275\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.0004205597203690559\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.0004205240681767464\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.0004204995883628726\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00042045818176120517\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00042046843795105814\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.0004204437346197665\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00042038482846692206\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.0004203380469698459\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00042032861383631823\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00042028056690469385\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00042024452006444334\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.000420197790954262\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00042017598869279024\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.0004201414424460381\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00042008512071333825\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.00042002039728686216\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00041996879735961556\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.000419925453606993\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.0004199103417340666\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.0004198243468999863\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.000419746208935976\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.0004196723725181073\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00041963014751672745\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00041958050103858114\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00041951500228606166\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00041947147459723053\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00041936265188269317\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.0004192649584729224\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00041917423135600984\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.0004190812283195555\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.000419087375048548\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00041893884423188865\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.000418833636213094\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00041873616515658796\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00041862924816086887\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.0004185024998150766\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00041837724740616977\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.0004182496725115925\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.00041814257390797137\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00041799518978223203\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00041788313421420753\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.0004177388118114322\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00041758273844607176\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00041739449254237117\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00041721168323419986\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.0004170527763199061\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.0004168554139323533\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00041666765115223826\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00041646151221357285\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00041625280980952086\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.0004160041827708483\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.0004157479503192008\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00041549301822669805\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.0004153384664095938\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00041498279548250137\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.00041466461261734365\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.0004143476206809282\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.00041403343086130915\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.0004137167043518275\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.0004133556433953345\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00041303969686850905\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00041261555859819056\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00041217725141905247\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.0004117335856426507\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00041131684673018754\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00041080545517615977\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.0004103897511959076\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00040978961857035756\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.0004092318972107023\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.0004086315585300326\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.0004080200882162899\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00040748865227214993\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00040681693819351496\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.0004060274746734649\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00040527511038817465\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00040449812426231803\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00040370052680373193\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00040285111987031996\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00040200586547143756\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00040104407467879356\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.00040021608816459775\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00039910807157866657\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.0003979950153734535\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00039687463431619107\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00039572125882841646\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.0003945203963667154\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.0003933349612634629\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.0003920221666339785\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00039063239470124247\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.00038924788357689975\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00038795201689936223\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.0003863665566314012\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.0003847556549590081\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00038316233432851733\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00038152621942572296\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.0003798484953586012\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.0003781292785424739\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.0003763701464049518\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.00037466093548573556\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00037284244899637997\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.0003709174646064639\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.00036912216804921625\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.000367094143293798\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00036516685388050975\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.0003632234688848257\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.0003612695273477584\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.00035931359860114754\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.0003573572193272412\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00035542300436645745\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.0003538860869593918\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.0003516111325006932\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.0003496422176249325\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.0003477325849235058\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.0003458615636918694\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.0003440209128893912\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00034221083275042473\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00034043371444568036\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.0003386923647485673\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00033698893035762013\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.0003354328509885818\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.0003337832633405924\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.0003321512974798679\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00033060562680475413\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00032910938723944126\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00032765941577963533\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.0003262554120738059\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.0003248996951151639\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.0003236369974911213\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.0003225587180349976\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.00032116862712427974\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.0003199407109059393\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.0003188103996217251\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.0003177236253395677\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.00031667880131863057\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.000315675747115165\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.0003147140273358673\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00031398543855175375\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.0003130030317697674\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00031210200628265737\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.0003112757357303053\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00031049929675646125\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.0003097487555351108\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00030904270708560945\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00030836695688776674\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00030771797173656523\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00030709545477293433\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00030650211730971934\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.0003059373621363193\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.0003054060856811702\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.000305249507073313\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.0003045007318723947\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.0003039714356418699\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00030352513305842877\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.000303083990002051\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00030267654568888247\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00030229264637455345\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00030192731879651546\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.00030159106478095055\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.0003012714651413262\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.0003009458107408136\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.0003006433486007154\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.000300407181493938\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00030008135247044265\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.0002998025272972882\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.0002995488385204226\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00029929961310699583\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00029906969401054083\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.0002988703176379204\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00029870701604522764\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.0002984031056985259\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.0002981909562367946\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.0002979838044848293\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00029779301723465324\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00029761593439616265\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.0002975556976161897\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.0002973276621196419\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.0002971453953068703\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.0002969526895321906\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00029675088590011003\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00029658651445060966\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00029642714536748827\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00029627441195771096\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.0002961164037697017\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00029596550273709\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.0002958158787805587\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00029566539567895235\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.0002955170930363238\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.0002953889104537666\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.0002952961344271898\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.0002950615750160068\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.0002948618680238724\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.0002947003662120551\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00029455176438204946\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00029438988654874266\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00029422373860143124\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00029406631365418433\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.0002938966848887503\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.0002937119582202285\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.0002935402526054531\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00029339970089495185\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.0002932361769489944\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.0002930453442968428\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.0002928071445785463\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00029258387628942727\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.0002923651412129402\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.0002921575796790421\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.0002919524302706122\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00029175137053243815\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00029165587853640317\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.00029129025293514135\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00029105568886734546\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.0002908116672188044\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00029053182457573713\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00029025297379121185\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.0002899889275431633\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.0002896942594088614\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.0002894028183072805\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00028910210821777584\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00028881284641101956\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.00028849906171672044\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.0002881726331543177\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00028779051383025944\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.0002874138555489481\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.00028704331023618576\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.0002866616449318826\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.0002862609294243157\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.00028584875050000844\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00028542028157971797\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.0002849848777987063\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.00028458248125389217\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.0002840723539702594\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.0002836023480631411\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00028307676198892297\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00028259222279302775\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.0002820106386207044\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00028141585527919233\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00028083200217224655\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.0002802054805215448\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.0002795779926236719\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.0002789772779215127\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.0002783111098688096\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.0002775884105358273\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.00027684809756465254\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.00027610557619482277\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00027533361804671586\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.0002745151636190712\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00027368443785235285\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00027283707167953254\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.00027193427667953073\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00027103339438326656\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.0002700513124000281\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.0002690498495940119\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.0002680410863831639\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00026698686531744897\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.0002659153414424509\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00026480726897716524\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.0002636634837836027\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00026252328068949283\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00026126670418307184\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.0002599951473530382\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.0002587190014310181\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00025740368757396935\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.0002560667193029076\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.0002546907507348806\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00025330008706077935\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.00025188333238475026\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.0002504792797844857\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.00024895705399103463\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.0002474511828040704\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00024590636545326563\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.0002443639817647636\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00024280790530610829\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.0002412452711723745\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.000239685436245054\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00023813041858375072\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.00023656673030927777\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00023511586361564695\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00023349795257672666\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00023189296713098883\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00023036964936181903\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00022887028695549815\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.0002273937757126987\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.0002261280087986961\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.0002245814265916124\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.00022316690650768577\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00022178701241500675\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.0002204700285801664\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.0002192009356804192\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00021797452645841986\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00021679367986507714\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00021565689123235644\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00021456416638102383\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00021352997631765902\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00021272800920996816\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.00021171438856981695\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.00021068769274279475\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00020981206034775823\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00020899059309158473\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00020821459067519755\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00020748190698213875\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.0002067954611266032\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00020614429842680693\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.0002055960864527151\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.0002050205017440021\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.0002044330263743177\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00020394377293996512\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00020344081160146742\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.0002029608009615913\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00020252495363820344\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00020216462318785487\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00020180687424726785\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.0002014516422059387\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.0002010926412185654\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00020078697067219765\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.0002004910266259685\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00020038095011841504\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.0002000557631254196\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.0001997670077253133\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00019954686285927892\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.0001993363141082227\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.0001991510030347854\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00019897591671906413\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00019882933178450913\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.0001986828091321513\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.0001985534798586741\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00019843322632368655\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00019832247984595595\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00019813596620224416\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00019802109512966125\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.0001979179447516799\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.00019782219780609012\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.0001977342227473855\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.0001976556552108377\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00019758043403271584\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00019751678919419647\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00019761890231166036\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.00019748628546949475\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.0001973654393805191\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00019728114712052046\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00019722150638699533\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00019717198272701353\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00019713007204700262\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00019709059153683484\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00019705075887031852\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.0001970140397315845\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.000196979395695962\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00019694977963808925\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.0001972165674669668\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00019704637816175818\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00019690824556164443\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.00019687825988512486\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.0001968412025598809\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.00019682084210216998\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.000196801305282861\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.00019678152923006565\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.0001967648632125929\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00019674848997965454\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.00019673175702337176\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.000196717040380463\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00019670507521368563\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00019677744421642274\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.00019672209862619638\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00019688046770170332\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.00019672624592203646\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00019666658889036626\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.0001966557401465252\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00019664897525217384\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00019664145656861365\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.00019663483544718474\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00019663104205392302\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.0001966944901505485\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.0001967737212544307\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00019668273627758027\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00019661680271383376\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.0001966065989108756\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00019660490565001965\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00019661184109281748\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00019662718870677053\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.00019675389165058733\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.00019666635198518635\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00019663310376927257\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.0001966479781549424\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00019661240163259207\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00019658881530631333\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.0001965873537119478\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.00019658673903904855\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.0001965901884250343\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00019658840028569102\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00019658087752759455\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.0001965738763101399\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00019657156721223146\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.0001966822729445994\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.0001967491611139849\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00019662758044432848\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00019661173049826175\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.0001966153329703957\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00019659420941025018\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00019660974619910122\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.0001965882896911353\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.0001965877925977111\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00019659007142763585\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.00019659113720990718\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.0001966027991147712\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00019662458973471075\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00019661872240249069\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.0001965977338841185\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.0001965889142593369\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00019658610166516154\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.0001965972816105932\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00019660560763441027\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00019661960075609386\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00019664631108753383\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00019662038481328638\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00019657493627164513\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00019656958640553056\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00019656378135550766\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00019656421849504114\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00019656607881188393\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00019656542339362203\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00019656213175039738\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00019656976510304958\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00019675158953759818\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00019667345855850727\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00019662044884171336\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00019656695774756373\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.0001965609210310504\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00019655842625070362\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00019656053802464158\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00019656843622215092\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.0001965859712800011\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.000196613899897784\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00019662396225612612\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00019661797035951166\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00019659761455841361\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00019657393801026046\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00019659549521747977\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00019661659724079072\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.0001965927769197151\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00019658655452076346\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.0001965705811744556\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00019656871096231045\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.00019656148040667176\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00019665240892209114\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.00019666276581119745\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.0001965789234964177\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00019656523247249424\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.00019657070573884992\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.00019658970937598498\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.0001966368203284219\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.0001965857739560306\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.0001966010231990367\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.0001965759281301871\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00019657771219499409\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00019659537880215795\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00019657465687487275\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.00019665142463054508\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.0001965995685895905\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00019663288025185467\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00019661411584820599\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00019662673526909201\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00019658267789054662\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00019656078307889402\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.00019656070624478163\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00019656277494505046\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00019657151133287699\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00019657488621305675\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.000196572023560293\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00019657817319966852\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.0001966637250734493\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00019665985717438162\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.0001965746335918084\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00019657872035168112\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00019662511011119932\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.0001965763117186725\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.00019661444646771998\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00019659921701531858\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00019661647558677942\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00019659807439893485\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.0001966088666813448\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.00019659570592921226\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00019656650081742555\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.000196557876188308\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00019656116608530284\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.0001965633250074461\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.0001965664024464786\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.0001965653750812635\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00019657589436974376\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.00019667446380481124\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00019663825165480375\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00019660265592392535\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.0001965932681923732\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00019660050340462477\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.0001966014289064333\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00019660922640468925\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00019657977332826704\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.00019658950972370804\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00019658906967379153\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00019659633690025658\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00019660864141769709\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.00019663984247017652\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00019662791455630214\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.00019657099153846502\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.00019656033953651785\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.0001965601451229304\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00019656103977467866\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00019656341697555036\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.0001965625281445682\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00019655938260257244\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00019656277203466743\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00019684629689436405\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.0001966160733718425\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.0001965932355960831\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.0001965616107918322\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00019655613694339992\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00019655362528283148\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00019655460317153484\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00019655468175187708\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00019655397918540984\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00019655301759485155\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.0001965785858919844\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00019668213906697928\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00019663199898786843\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00019660985271912067\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00019660178804770112\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00019660587015096098\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.0001966360997175798\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.000196596835157834\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00019657712080515922\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.0001965755398850888\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00019656447519082577\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00019656507938634605\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00019657322845887393\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.00019661712110973894\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.0001966633164556697\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.0001965820015175268\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00019658164237625896\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00019658084667753428\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00019656930700875819\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00019661498779896647\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.00019657652417663486\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00019661858677864075\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.0001966139063006267\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00019662499136757105\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00019661689817439764\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.00019657215045299381\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00019657541182823478\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00019657775992527605\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.0001965788414236158\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.000196587570826523\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.0001966034120414406\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.00019661016529425978\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00019663103681523354\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00019662249949760734\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00019657227268908174\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00019657529832329602\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00019657091703265906\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00019657877332065254\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.0001965803391067311\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00019659915182273835\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00019659957382827997\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00019657948694657535\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00019659920712001623\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00019661216356325895\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.0001966045470908284\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00019658875244203954\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.0001965668553020805\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00019657340715639293\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00019656111137010158\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.0001965778530575335\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.0001966315641766414\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00019658602483104914\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00019658247707411648\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00019659932935610412\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00019661363272462041\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00019667582528200002\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00019659553130622952\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.00019657982047647238\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.0001965799118625\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.0001965847733663395\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.0001965845056110993\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.0001966052857460454\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00019659612735267729\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.000196600187337026\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00019661591912154108\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.0001965768594527617\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.00019656332093290985\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00019656971737276762\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00019657552766148001\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00019659918441902846\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00019660396676044912\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00019665476051159203\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.0001965985487913713\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.0001965621206909418\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.0001965556904906407\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00019656633900012822\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00019661983067635448\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00019658023433294147\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.0001966385281411931\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.000196605158271268\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.0001965742721222341\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00019657215860206635\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00019655988435260952\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00019655874639283865\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.0001965717790881172\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00019661727361381054\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00019675292249303312\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00019664439838379622\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.0001965648017358035\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00019655815500300378\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00019655730458907782\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.0001965563918929547\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00019655557465739549\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.0001965535612544045\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.0001965528930304572\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00019666069711092858\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00019662222417537122\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00019660987425595521\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00019659540324937553\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00019661886151880025\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00019657405675388873\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00019657410797663033\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00019658403180073948\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.0001965748576913029\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.0001966144103789702\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00019657729426398874\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00019660840975120662\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.0001965886855032295\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00019658907665871084\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00019661143654957412\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00019661453319713473\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.00019662808568682522\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.0001965910696890205\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00019656541582662612\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.00019656363117974253\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00019656069285701962\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.000196565777878277\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.0001965760882012546\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.0001966910168994218\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00019661573227494955\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00019658051489386706\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00019658009870909154\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00019658055331092328\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.00019657139084301888\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00019658495322801172\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00019659619953017683\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.00019659037643577905\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00019658174889627844\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00019666639156639575\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00019659577927086502\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00019660994410514832\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.0001966002641711384\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00019659374665934593\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00019660183927044272\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.00019657674245536329\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.0001965703011956066\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00019657461962196976\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.0001965754438424483\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00019658009288832544\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00019659441721159965\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00019661308091599493\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.0001966127392370254\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00019663134124130012\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00019661067752167584\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00019656740420032293\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.0001965681102592498\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.00019657801778521388\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.0001965767063666135\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00019655941461678593\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00019656192453112453\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00019658743985928595\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.00019658343691844493\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.000196657499182038\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00019660067453514785\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.0001965949556324631\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00019658297707792372\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00019659291137941182\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00019660631543956696\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00019657603232190013\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.0001965626422315836\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.0001966188591904938\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.0001966331450967118\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00019657982920762151\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.0001965642673894763\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.0001965630176709965\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.00019655826210509986\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.0001965829764958471\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.00019658036762848496\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.000196765354485251\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00019665884145069867\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.000196599664632231\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00019656233722344042\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00019655822310596705\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00019655511539895087\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.0001965528348227963\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00019655432784929872\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00019655536569189281\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00019655453972518443\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.0001965527981519699\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00019656801014207305\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00019670070905704052\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00019662484410218895\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.0001966080436250195\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.0001966165954945609\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.0001965846912935376\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.0001965870545245707\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00019660023448523134\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00019658750330563634\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.00019659506040625274\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00019656815915368496\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.00019660089106764644\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.0001965848926920444\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00019657922966871412\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.0001966104400344193\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00019658100209198892\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.0001966021570842713\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.0001966131676454097\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00019664038438349963\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.0001965942414244637\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00019657320925034583\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00019657578784972428\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00019658286881167443\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00019659213663544506\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.00019660435616970063\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00019663386920001357\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00019661186437588184\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00019657208526041358\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.0001965690142242238\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00019656208343803882\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00019656716438475996\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.0001965679315617308\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00019656998745631426\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.0001966002251720056\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00019663224869873375\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.0001966535480460152\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.0001965792052214965\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.0001965596224181354\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.000196563673671335\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00019659355806652456\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00019662383536342532\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00019659147830680012\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00019657454162370414\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.0001965757005382329\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.00019658135366626084\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.0001966266258386895\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00019657319411635398\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.0001965792285045609\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.0001965888193808496\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.0001965741702588275\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00019661174155771732\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00019658430363051594\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00019661728758364915\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.0001966383185936138\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00019660340389236807\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.00019663819810375571\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.0001965979964006692\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00019656629650853573\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.0001965629868209362\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.0001965601573465392\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.00019656045362353324\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00019656424236018212\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.0001965790061512962\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.00019658448349218815\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00019664870167616756\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00019660742895212025\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00019657149678096176\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00019656859221868218\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.00019666872976813464\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.0001965828234096989\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00019661054946482182\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00019660674326587468\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00019659337936900556\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00019660779042169452\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.00019658439967315644\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.00019656908931210638\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00019657467026263476\n",
      "Autoencoder1 training DONE... TOTAL TIME = 141.79408407211304\n",
      "start evaluation on test data for Autoencoder1\n",
      "MSE is 0.1963001833861604\n",
      "mutls per sample is 8192\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder2\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.000226884389994666\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.00010770672553917394\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 9.482224675593897e-05\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 8.846419485053047e-05\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 8.333447854965925e-05\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 7.899177900981158e-05\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 7.54516082815826e-05\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 7.414103660266847e-05\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 7.018814765615389e-05\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 6.721539160935208e-05\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 6.496443063952028e-05\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 6.292464036960155e-05\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 6.108007277362048e-05\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 5.9412474365672095e-05\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 5.791289935586974e-05\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 5.657371788402088e-05\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 5.538704790524207e-05\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 5.503729087649845e-05\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 5.383140116464347e-05\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 5.269908695481718e-05\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 5.200776475248858e-05\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 5.143400834640488e-05\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 5.095020038424991e-05\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 5.0541489035822454e-05\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 5.0197193486383186e-05\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 4.990739020286128e-05\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 4.9663042736938224e-05\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 4.9456292326794934e-05\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 4.928045775159262e-05\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 4.966302643879317e-05\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 4.917043872410431e-05\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 4.891814314760268e-05\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 4.87948844966013e-05\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 4.8702367348596456e-05\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 4.862009445787408e-05\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 4.854487677221186e-05\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 4.847547519602813e-05\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 4.841086702072062e-05\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 4.83498940593563e-05\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 4.8295088345184925e-05\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 4.893874109257013e-05\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 4.8342046356992794e-05\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 4.815718290046789e-05\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 4.80929524928797e-05\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 4.8045201547211034e-05\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 4.7994972119340675e-05\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 4.794822089024819e-05\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 4.838122113142163e-05\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 4.793111904291436e-05\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 4.783631913596764e-05\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 4.7944259422365575e-05\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 4.7777250729268416e-05\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 4.7704695170978083e-05\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 4.7641623968956995e-05\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 4.7590119647793473e-05\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 4.754784647957422e-05\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 4.769402075908147e-05\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 4.7529750445391984e-05\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 4.809284306247718e-05\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 4.759671181091107e-05\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 4.734653572086245e-05\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 4.7292563394876196e-05\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 4.724860016722232e-05\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 4.720747005194426e-05\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 4.716711555374786e-05\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 4.7135566419456154e-05\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 4.817116569029167e-05\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 4.752536042360589e-05\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 4.709749060566537e-05\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 4.7003727522678674e-05\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 4.696063799201511e-05\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 4.691898051532917e-05\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 4.687748194555752e-05\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 4.6836843248456715e-05\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 4.679949066485278e-05\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 4.7883327351883056e-05\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 4.6898380387574436e-05\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 4.6960173494881016e-05\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 4.670611320761964e-05\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 4.664700536523014e-05\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 4.6605816896772013e-05\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 4.663794912630692e-05\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 4.682266240706667e-05\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 4.651059731259011e-05\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 4.646398898330517e-05\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 4.6427715278696266e-05\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 4.639582461095415e-05\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 4.6364713634829966e-05\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 4.6334380749613045e-05\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 4.630807175999507e-05\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 4.6599915804108606e-05\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 4.628008959116414e-05\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 4.622475302312523e-05\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 4.619294049916789e-05\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 4.61638170236256e-05\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 4.621860454790294e-05\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 4.657386904000305e-05\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 4.624359164154157e-05\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 4.60765378375072e-05\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 4.6040138695389036e-05\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 4.6028697834117335e-05\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 4.599220061209053e-05\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 4.598706611432135e-05\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 4.616829581209458e-05\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 4.600851854775101e-05\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 4.590643395204097e-05\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 4.588038587826304e-05\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 4.588276526192203e-05\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 4.582740672049113e-05\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 4.579649263177998e-05\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 4.57983065280132e-05\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 4.580574750434607e-05\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 4.63929888792336e-05\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 4.624757726560347e-05\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 4.57438180455938e-05\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 4.570253164274618e-05\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 4.5663468044949694e-05\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 4.563036636682227e-05\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 4.5606981439050286e-05\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 4.558413798804395e-05\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 4.5557578996522355e-05\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 4.593060933984816e-05\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 4.567406329442747e-05\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 4.555504536256194e-05\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 4.5501437707571316e-05\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 4.5475804945454e-05\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 4.545606352621689e-05\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 4.5454860373865816e-05\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 4.569614378851838e-05\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 4.5797833299729975e-05\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 4.550839163130149e-05\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 4.539982386631891e-05\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 4.5366445992840456e-05\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 4.534219129709527e-05\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 4.5323648519115525e-05\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 4.534627718385309e-05\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 4.542563547147438e-05\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 4.536622509476729e-05\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 4.558169850497507e-05\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 4.532616512733512e-05\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 4.525184995145537e-05\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 4.5225524518173186e-05\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 4.520449583651498e-05\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 4.518725283560343e-05\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 4.5342973025981335e-05\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 4.5647899532923475e-05\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 4.520319605944678e-05\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 4.5191433309810235e-05\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 4.515871856710873e-05\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 4.516425164183602e-05\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 4.510677346843295e-05\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 4.509601669269614e-05\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 4.5082623400958255e-05\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 4.557580730761401e-05\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 4.5347177365329114e-05\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 4.509010686888359e-05\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 4.512485655141063e-05\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 4.5060519769322125e-05\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 4.5047255407553164e-05\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 4.5022974227322265e-05\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 4.500112539972179e-05\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 4.53208532417193e-05\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 4.503103278693743e-05\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 4.507753677899018e-05\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 4.5009282475803045e-05\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 4.497988949879073e-05\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 4.494358858210035e-05\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 4.492671592743136e-05\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 4.4933366880286484e-05\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 4.53357030346524e-05\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 4.5131434162613005e-05\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 4.49388354900293e-05\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 4.493067463045009e-05\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 4.4901210494572296e-05\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 4.487178535782732e-05\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 4.492925290833227e-05\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 4.486080608330667e-05\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 4.49294458667282e-05\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 4.4879879424115644e-05\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 4.483914584852755e-05\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 4.484623146709055e-05\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 4.489538638154045e-05\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 4.487811223953031e-05\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 4.484370059799403e-05\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 4.479551469557919e-05\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 4.47870466450695e-05\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 4.513075618888252e-05\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 4.491875821258873e-05\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 4.4776198046747594e-05\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 4.4832629937445746e-05\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 4.476673842873424e-05\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 4.477359470911324e-05\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 4.474728557397611e-05\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 4.479542563785799e-05\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 4.477378650335595e-05\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 4.4860116468044e-05\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 4.477991999010555e-05\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 4.472287662792951e-05\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 4.475559617276303e-05\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 4.4749041553586724e-05\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 4.4783854536945e-05\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 4.4724465697072445e-05\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 4.468476152396761e-05\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 4.486850491957739e-05\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 4.478064016439021e-05\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 4.474075074540451e-05\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 4.46819637727458e-05\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 4.505166099988855e-05\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 4.469213861739263e-05\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 4.465939797228202e-05\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 4.465771649847739e-05\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 4.464121389901265e-05\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 4.478463844861835e-05\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 4.466097409022041e-05\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 4.469792853342369e-05\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 4.467560909688473e-05\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 4.462241573492065e-05\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 4.460555384866893e-05\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 4.4599081156775356e-05\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 4.470821411814541e-05\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 4.484606426558457e-05\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 4.4614455982809884e-05\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 4.469763211091049e-05\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 4.459578587557189e-05\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 4.459016301552765e-05\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 4.4596210354939106e-05\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 4.457718037883751e-05\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 4.4580110552487897e-05\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 4.465607562451623e-05\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 4.4755668786820026e-05\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 4.4563504197867586e-05\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 4.454970010556281e-05\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 4.459481031517498e-05\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 4.458334908122197e-05\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 4.457090239156969e-05\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 4.478430011658929e-05\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 4.459702104213648e-05\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 4.45740319264587e-05\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 4.457271978026256e-05\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 4.452975612366572e-05\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 4.45925745589193e-05\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 4.453299712622538e-05\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 4.4715282856486736e-05\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 4.4543343683471903e-05\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 4.454319743672386e-05\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 4.451062093721703e-05\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 4.450236679986119e-05\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 4.482190721319057e-05\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 4.459554009372369e-05\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 4.4525285775307564e-05\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 4.451107670320198e-05\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 4.454723806702532e-05\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 4.451945977052674e-05\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 4.4529812585096805e-05\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 4.4496552291093395e-05\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 4.4559602974914016e-05\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 4.450203719898127e-05\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 4.452087159734219e-05\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 4.4626646413234994e-05\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 4.4465432874858376e-05\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 4.4504263787530366e-05\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 4.4484999089036135e-05\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 4.449189684237354e-05\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 4.450349457329139e-05\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 4.448755062185228e-05\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 4.449087020475417e-05\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 4.447746599907987e-05\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 4.448951585800387e-05\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 4.453866815310903e-05\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 4.453405577805824e-05\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 4.4465653627412396e-05\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 4.447790182894096e-05\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 4.446727136382833e-05\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 4.451704910025e-05\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 4.4466273102443666e-05\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 4.4425140513340013e-05\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 4.44511414389126e-05\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 4.451517961570062e-05\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 4.4492133310996e-05\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 4.443858793820254e-05\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 4.4410697009880094e-05\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 4.442309989826754e-05\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 4.464328128960915e-05\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 4.44836427050177e-05\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 4.4434233714127914e-05\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 4.4445561943575737e-05\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 4.441270910319872e-05\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 4.4462689984356985e-05\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 4.4429309928091245e-05\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 4.439977958099917e-05\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 4.4438643235480414e-05\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 4.4416027085389944e-05\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 4.445441809366457e-05\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 4.458960334886797e-05\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 4.439592055859976e-05\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 4.440059943590313e-05\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 4.438561241840944e-05\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 4.450827022083104e-05\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 4.438919058884494e-05\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 4.447489714948461e-05\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 4.4395840523066e-05\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 4.4425468950066715e-05\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 4.440949880518019e-05\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 4.436882649315521e-05\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 4.436801275005564e-05\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 4.437678726390004e-05\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 4.43710146646481e-05\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 4.444118225364946e-05\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 4.43938813987188e-05\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 4.439371841726825e-05\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 4.44343795243185e-05\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 4.435418624780141e-05\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 4.4361872860463335e-05\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 4.440837859874591e-05\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 4.4395241420716044e-05\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 4.452031149412505e-05\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 4.4376238802215087e-05\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 4.435737209860235e-05\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 4.434507485711947e-05\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 4.434032845892943e-05\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 4.439289885340258e-05\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 4.436225470271893e-05\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 4.4381891784723846e-05\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 4.4494345056591554e-05\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 4.433541325852275e-05\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 4.436559218447655e-05\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 4.433491470990703e-05\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 4.432841844391078e-05\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 4.439953554538079e-05\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 4.4408383109839634e-05\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 4.433936177520082e-05\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 4.432426168932579e-05\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 4.453680914593861e-05\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 4.43450192688033e-05\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 4.431967827258632e-05\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 4.4347249495331195e-05\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 4.434596121427603e-05\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 4.444642734597437e-05\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 4.432802626979537e-05\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 4.4319982698652894e-05\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 4.4323046022327614e-05\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 4.433550231624395e-05\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 4.4369240058586e-05\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 4.4383981730788946e-05\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 4.430812012287788e-05\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 4.429763561347499e-05\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 4.433301248354837e-05\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 4.435946757439524e-05\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 4.432812886079773e-05\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 4.4367150403559206e-05\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 4.436446499312296e-05\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 4.431442968780175e-05\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 4.430859931744635e-05\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 4.4319237786112354e-05\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 4.429181615705602e-05\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 4.436370960320346e-05\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 4.4333714758977296e-05\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 4.430847460753284e-05\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 4.432967791217379e-05\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 4.429339343914762e-05\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 4.431401903275401e-05\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 4.447883009561338e-05\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 4.439069962245412e-05\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 4.429671389516443e-05\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 4.4318784348433836e-05\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 4.43223919137381e-05\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 4.428083804668859e-05\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 4.428063650266267e-05\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 4.430247077834792e-05\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 4.429053937201388e-05\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 4.45548580319155e-05\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 4.429641558090225e-05\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 4.433788708411157e-05\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 4.430867425980978e-05\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 4.4302887545200065e-05\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 4.428993037436157e-05\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 4.429929351317696e-05\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 4.426798288477585e-05\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 4.4352524128044024e-05\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 4.446693666977808e-05\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 4.4299532746663317e-05\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 4.4272960658418014e-05\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 4.42696426762268e-05\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 4.429874199558981e-05\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 4.429978245752864e-05\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 4.426915518706665e-05\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 4.4311756937531754e-05\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 4.4274835381656885e-05\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 4.432461835676804e-05\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 4.4311061647022146e-05\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 4.437701078131795e-05\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 4.427821797435172e-05\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 4.4280112197157e-05\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 4.428808868397027e-05\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 4.427212115842849e-05\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 4.427681706147269e-05\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 4.436114861164242e-05\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 4.4267355115152895e-05\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 4.427594059961848e-05\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 4.4272852246649567e-05\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 4.432332614669576e-05\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 4.427592124557123e-05\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 4.4274342071730645e-05\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 4.4358088489389045e-05\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 4.435527487657964e-05\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 4.4265558826737106e-05\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 4.4280432775849475e-05\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 4.425561302923597e-05\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 4.433543144841678e-05\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 4.4333537953207266e-05\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 4.433329071616754e-05\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 4.428089741850272e-05\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 4.424967628438026e-05\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 4.424404876772314e-05\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 4.431256340467371e-05\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 4.4445809471653775e-05\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 4.427134161232971e-05\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 4.4250848150113596e-05\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 4.4336137943901124e-05\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 4.4276568514760585e-05\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 4.425843959324993e-05\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 4.425629886100069e-05\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 4.42866982484702e-05\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 4.43245614587795e-05\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 4.427168270922266e-05\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 4.4263273011893033e-05\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 4.435167371411808e-05\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 4.427742256666534e-05\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 4.4260844006203116e-05\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 4.4351667165756224e-05\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 4.429305394296535e-05\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 4.4278964487602935e-05\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 4.4301684683887286e-05\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 4.425257051480003e-05\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 4.4250556093174965e-05\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 4.4243295124033464e-05\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 4.432678935700096e-05\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 4.449455795111135e-05\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 4.42513583402615e-05\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 4.4238001282792536e-05\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 4.423604696057737e-05\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 4.431169028976001e-05\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 4.425576233188622e-05\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 4.425788050866686e-05\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 4.432410438312217e-05\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 4.425276667461731e-05\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 4.424415456014685e-05\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 4.423196398420259e-05\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 4.425652135978453e-05\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 4.426904764841311e-05\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 4.427859297720716e-05\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 4.4269130448810756e-05\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 4.4283896713750436e-05\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 4.4238773261895405e-05\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 4.432911038747989e-05\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 4.4242256844881925e-05\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 4.4259442947804925e-05\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 4.423730817507021e-05\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 4.426697050803341e-05\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 4.4331081880955026e-05\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 4.424234692123719e-05\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 4.4231681094970554e-05\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 4.422319660079666e-05\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 4.43125911988318e-05\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 4.431853667483665e-05\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 4.423320788191631e-05\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 4.4221912394277754e-05\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 4.4221147254575045e-05\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 4.428784959600307e-05\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 4.425433202413842e-05\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 4.423368212883361e-05\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 4.426337269251235e-05\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 4.423029531608336e-05\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 4.429210588568821e-05\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 4.422552476171404e-05\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 4.426987492479384e-05\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 4.421259902301244e-05\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 4.4280351867200805e-05\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 4.427455351105891e-05\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 4.426664061611518e-05\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 4.422485857503489e-05\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 4.4212714565219356e-05\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 4.422252284712158e-05\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 4.424656726769172e-05\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 4.425648439791985e-05\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 4.423901307745837e-05\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 4.423777252668515e-05\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 4.422380327014253e-05\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 4.433142952620983e-05\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 4.4217764807399364e-05\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 4.422111960593611e-05\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 4.425202874699607e-05\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 4.421391262440011e-05\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 4.424318030942232e-05\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 4.4244846503715965e-05\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 4.423797901836224e-05\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 4.4222371507203206e-05\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 4.42171779286582e-05\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 4.42375113198068e-05\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 4.429703258210793e-05\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 4.422301368322223e-05\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 4.424361904966645e-05\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 4.42374225531239e-05\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 4.4214459776412696e-05\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 4.422005062224344e-05\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 4.424549886607565e-05\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 4.4219709670869634e-05\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 4.421915524289943e-05\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 4.422674319357611e-05\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 4.4268896308494734e-05\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 4.4211000495124606e-05\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 4.4305152696324515e-05\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 4.4216157257324084e-05\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 4.425550097948872e-05\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 4.420969722559675e-05\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 4.421176912728697e-05\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 4.423524253070355e-05\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 4.422889876877889e-05\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 4.42082155495882e-05\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 4.428404456120916e-05\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 4.421104458742775e-05\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 4.4206907914485783e-05\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 4.420308672706597e-05\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 4.421076606377028e-05\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 4.431208042660728e-05\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 4.421286808792502e-05\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 4.424001308507286e-05\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 4.4369291863404215e-05\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 4.420009820023552e-05\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 4.4209383486304435e-05\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 4.4211396743776274e-05\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 4.420089186169207e-05\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 4.4201976124895735e-05\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 4.435959417605773e-05\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 4.4208826002432035e-05\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 4.4194203219376505e-05\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 4.4190563494339584e-05\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 4.4201739656273275e-05\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 4.426077022799291e-05\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 4.421014251420274e-05\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 4.4194984366185966e-05\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 4.421006146003492e-05\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 4.420520272105932e-05\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 4.424880840815604e-05\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 4.421922436449677e-05\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 4.419576245709322e-05\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 4.423815596965142e-05\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 4.420609911903739e-05\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 4.418218988575973e-05\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 4.4228421611478555e-05\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 4.424572136485949e-05\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 4.419666016474366e-05\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 4.423185891937465e-05\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 4.4232136278878894e-05\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 4.420231533003971e-05\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 4.418738826643676e-05\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 4.420513330842368e-05\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 4.418058437295258e-05\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 4.437547642737627e-05\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 4.421571546117775e-05\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 4.421746052685194e-05\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 4.419364093337208e-05\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 4.422695506946184e-05\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 4.4213229848537594e-05\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 4.419910052092746e-05\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 4.422374535351992e-05\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 4.4222187716513874e-05\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 4.419256802066229e-05\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 4.4189268664922564e-05\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 4.41765075083822e-05\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 4.425826031365432e-05\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 4.4205166486790406e-05\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 4.421130492119119e-05\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 4.422319747391157e-05\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 4.419202930876054e-05\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 4.423748163389973e-05\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 4.420840603415854e-05\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 4.4203716824995354e-05\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 4.417925170855597e-05\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 4.423510763444938e-05\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 4.4190150656504554e-05\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 4.418067255755887e-05\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 4.424762330017984e-05\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 4.4235506356926636e-05\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 4.418696436914615e-05\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 4.4173185451654717e-05\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 4.4191833294462415e-05\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 4.420137862325646e-05\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 4.4289142533671114e-05\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 4.41927021893207e-05\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 4.419461925863288e-05\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 4.419228425831534e-05\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 4.429363165399991e-05\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 4.417373973410576e-05\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 4.4164078426547347e-05\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 4.42278414266184e-05\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 4.420572440722026e-05\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 4.4178448006277905e-05\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 4.423632504767738e-05\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 4.418534284923226e-05\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 4.4222430151421573e-05\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 4.4183589634485545e-05\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 4.419581382535398e-05\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 4.4190923654241486e-05\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 4.4182971032569184e-05\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 4.426144994795323e-05\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 4.418027747306041e-05\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 4.417941134306602e-05\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 4.421617079060525e-05\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 4.4189244363224134e-05\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 4.418082011397928e-05\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 4.418901036842726e-05\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 4.424249040312134e-05\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 4.420949742780067e-05\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 4.4173650530865415e-05\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 4.4210506603121756e-05\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 4.4181149860378355e-05\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 4.420719764311798e-05\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 4.416995943756774e-05\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 4.4239302369533106e-05\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 4.417996009578929e-05\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 4.421242512762547e-05\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 4.420411147293635e-05\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 4.419781485921703e-05\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 4.419338802108541e-05\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 4.417520423885435e-05\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 4.4178833777550606e-05\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 4.419598990352824e-05\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 4.4182646524859594e-05\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 4.428230400662869e-05\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 4.417005708091892e-05\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 4.4169453758513555e-05\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 4.424924132763408e-05\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 4.419664503075182e-05\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 4.4192132336320355e-05\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 4.417630538227968e-05\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 4.417802265379578e-05\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 4.419878343469463e-05\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 4.4177251984365286e-05\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 4.420836470671929e-05\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 4.4190599728608506e-05\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 4.418918455485255e-05\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 4.4167917803861204e-05\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 4.42112390010152e-05\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 4.4183652207721024e-05\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 4.423596546985209e-05\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 4.417421514517628e-05\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 4.418935946887359e-05\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 4.4195166701683774e-05\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 4.417886986630037e-05\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 4.418200391228311e-05\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 4.41732605395373e-05\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 4.417959673446603e-05\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 4.4194745714776215e-05\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 4.418884200276807e-05\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 4.4188810570631175e-05\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 4.425869759870693e-05\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 4.4182179699419067e-05\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 4.417395990458317e-05\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 4.41729549493175e-05\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 4.41746880824212e-05\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 4.419382457854226e-05\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 4.419671560754068e-05\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 4.420637604198418e-05\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 4.416898620547727e-05\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 4.417003292473964e-05\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 4.42224316066131e-05\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 4.421909761731513e-05\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 4.4168216845719144e-05\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 4.416763142216951e-05\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 4.418575699673966e-05\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 4.419379751197994e-05\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 4.417551856022328e-05\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 4.417718533659354e-05\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 4.418148368131369e-05\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 4.418658965732902e-05\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 4.4167190935695544e-05\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 4.4187431922182443e-05\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 4.4180184049764646e-05\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 4.420968805789016e-05\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 4.4180328550282867e-05\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 4.4180572149343786e-05\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 4.416874508024193e-05\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 4.419908538693562e-05\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 4.4198894465807824e-05\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 4.4165724975755435e-05\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 4.420202327310108e-05\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 4.417427931912243e-05\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 4.419085860718042e-05\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 4.415571602294222e-05\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 4.4168131717015056e-05\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 4.423501188284718e-05\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 4.42131761519704e-05\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 4.416423354996368e-05\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 4.416433090227656e-05\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 4.417290285346098e-05\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 4.4229696795810016e-05\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 4.417054951773025e-05\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 4.4172330235596745e-05\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 4.416003881487995e-05\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 4.419674733071588e-05\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 4.417089701746591e-05\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 4.4164743594592435e-05\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 4.4198598334332925e-05\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 4.418698837980628e-05\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 4.4163076236145574e-05\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 4.416987227159552e-05\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 4.421643330715597e-05\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 4.416171257616952e-05\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 4.417761098011397e-05\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 4.4188912870595235e-05\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 4.417086631292477e-05\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 4.418715281644836e-05\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 4.416004405356944e-05\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 4.4226070895092565e-05\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 4.4165330473333596e-05\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 4.417131160153076e-05\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 4.41556096484419e-05\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 4.418354816152714e-05\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 4.4181580597069116e-05\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 4.417867108713836e-05\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 4.416232230141759e-05\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 4.417723583173938e-05\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 4.4198499235790226e-05\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 4.417331350850873e-05\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 4.416048017446883e-05\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 4.4173164060339335e-05\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 4.4218289840500805e-05\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 4.415745890582912e-05\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 4.4168429012643174e-05\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 4.416069990838878e-05\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 4.4194421061547476e-05\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 4.418201133375988e-05\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 4.4158857490401716e-05\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 4.418106735101901e-05\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 4.4167872256366534e-05\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 4.419142584083602e-05\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 4.4195312366355214e-05\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 4.415275063365698e-05\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 4.415293878992088e-05\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 4.42241586279124e-05\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 4.417376345372759e-05\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 4.415946241351776e-05\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 4.416331503307447e-05\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 4.416095151100308e-05\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 4.417799616931006e-05\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 4.4154242641525346e-05\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 4.424888305948116e-05\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 4.416737298015505e-05\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 4.4162061240058396e-05\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 4.417551739607006e-05\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 4.416485186084173e-05\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 4.416061521624215e-05\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 4.4153079215902835e-05\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 4.418628159328364e-05\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 4.417015486978926e-05\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 4.422029800480232e-05\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 4.416205178131349e-05\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 4.415438321302645e-05\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 4.418805518071167e-05\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 4.417083488078788e-05\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 4.415775329107419e-05\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 4.416094598127529e-05\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 4.417776202899404e-05\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 4.4159709505038334e-05\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 4.417278352775611e-05\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 4.415823088493198e-05\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 4.4216785754542796e-05\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 4.4170300825499e-05\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 4.4153995258966464e-05\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 4.416902855155058e-05\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 4.4159621465951205e-05\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 4.4272627565078435e-05\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 4.415650953887962e-05\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 4.414888666360639e-05\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 4.414282564539462e-05\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 4.41780430264771e-05\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 4.417101503349841e-05\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 4.415161805809475e-05\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 4.419352335389703e-05\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 4.415723524289206e-05\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 4.4155397627037016e-05\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 4.417826785356738e-05\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 4.416697847773321e-05\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 4.415048606460914e-05\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 4.418032665853389e-05\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 4.4156612129881975e-05\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 4.4162918929941955e-05\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 4.416739771841094e-05\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 4.417863907292485e-05\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 4.423531572683714e-05\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 4.415686955326237e-05\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 4.4144866697024556e-05\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 4.413685950567015e-05\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 4.415819785208441e-05\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 4.4253201776882635e-05\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 4.4165588333271445e-05\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 4.4150141038699073e-05\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 4.414280178025365e-05\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 4.4146938307676464e-05\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 4.419842196512036e-05\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 4.4158369564684105e-05\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 4.4152828777441755e-05\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 4.420015524374321e-05\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 4.41453255189117e-05\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 4.414455310325138e-05\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 4.4200953125255184e-05\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 4.4179747492307795e-05\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 4.414989540237002e-05\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 4.414205177454278e-05\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 4.414191207615659e-05\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 4.417444419232197e-05\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 4.415816438267939e-05\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 4.416960451635532e-05\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 4.4173020723974334e-05\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 4.415858958964236e-05\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 4.416602474520914e-05\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 4.415796487592161e-05\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 4.422225698363036e-05\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 4.414622264448553e-05\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 4.4142307742731646e-05\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 4.414727387484163e-05\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 4.419654796947725e-05\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 4.41650343418587e-05\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 4.417479169205762e-05\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 4.414935247041285e-05\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 4.4153529015602546e-05\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 4.4154392526252195e-05\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 4.4156332733109594e-05\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 4.418536540470086e-05\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 4.4156007206765934e-05\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 4.415552975842729e-05\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 4.417420961544849e-05\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 4.416247509652749e-05\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 4.416824071086012e-05\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 4.415418457938358e-05\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 4.416460797074251e-05\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 4.414476294186897e-05\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 4.4144829735159875e-05\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 4.4183161662658677e-05\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 4.417363743414171e-05\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 4.415393152157776e-05\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 4.415684365085326e-05\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 4.4179627730045466e-05\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 4.416170588228851e-05\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 4.4141920225229116e-05\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 4.416374722495675e-05\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 4.4212903594598175e-05\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 4.415478149894625e-05\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 4.4140994432382286e-05\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 4.415379953570664e-05\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 4.416278577991761e-05\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 4.414740178617649e-05\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 4.417935240780935e-05\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 4.415648960275575e-05\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 4.414936280227266e-05\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 4.414450289914385e-05\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 4.416257695993409e-05\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 4.418543656356633e-05\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 4.416073235915974e-05\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 4.419640288688243e-05\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 4.413796559674665e-05\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 4.413743954501115e-05\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 4.4156170770293104e-05\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 4.416210213094018e-05\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 4.416063311509788e-05\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 4.4141908874735235e-05\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 4.414091890794225e-05\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 4.423951919307001e-05\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 4.4158600794617086e-05\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 4.4155661889817565e-05\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 4.416927986312657e-05\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 4.413764007040299e-05\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 4.415222720126622e-05\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 4.415141767822206e-05\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 4.415293922647834e-05\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 4.4155770010547715e-05\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 4.4158511736895887e-05\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 4.4145917490823196e-05\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 4.417596574057825e-05\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 4.41465710173361e-05\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 4.415461138705723e-05\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 4.415330084157176e-05\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 4.416206575115211e-05\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 4.4198443210916596e-05\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 4.415529037942179e-05\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 4.413804796058685e-05\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 4.418496464495547e-05\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 4.414276496390812e-05\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 4.414960785652511e-05\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 4.413860995555297e-05\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 4.4145792053313926e-05\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 4.417425428982824e-05\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 4.415614952449687e-05\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 4.416946394485422e-05\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 4.415673960465938e-05\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 4.413786955410614e-05\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 4.4141121034044776e-05\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 4.420995290274732e-05\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 4.4159007229609417e-05\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 4.414672803250141e-05\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 4.4156262738397345e-05\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 4.41481037705671e-05\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 4.4163086713524535e-05\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 4.4140227255411444e-05\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 4.414397073560394e-05\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 4.415360002894886e-05\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 4.415180563228205e-05\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 4.415502742631361e-05\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 4.4143179984530434e-05\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 4.4176493975101036e-05\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 4.4171707413624974e-05\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 4.414939918206073e-05\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 4.4155231735203414e-05\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 4.414162685861811e-05\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 4.417115895194002e-05\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 4.4145266292616726e-05\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 4.4157107768114654e-05\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 4.414534123498015e-05\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 4.4145204592496155e-05\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 4.414842900587246e-05\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 4.418949029059149e-05\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 4.415119488839991e-05\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 4.41559859609697e-05\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 4.4140108511783185e-05\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 4.4195090886205435e-05\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 4.4145436986582355e-05\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 4.414133640239015e-05\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 4.414124457980506e-05\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 4.41522071196232e-05\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 4.42010720144026e-05\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 4.413915332406759e-05\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 4.4142559927422555e-05\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 4.4135530770290644e-05\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 4.4146996078779924e-05\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 4.418687254656106e-05\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 4.4154462520964444e-05\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 4.417150776134804e-05\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 4.414216135046445e-05\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 4.412643509567715e-05\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 4.4139349192846566e-05\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 4.414745446410962e-05\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 4.414600247400813e-05\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 4.4196354574523864e-05\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 4.415115487063303e-05\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 4.414967843331396e-05\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 4.414450726471841e-05\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 4.4170180626679215e-05\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 4.41444200987462e-05\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 4.4146274449303745e-05\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 4.414046692545526e-05\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 4.418908327352256e-05\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 4.4144072307972235e-05\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 4.413369373651221e-05\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 4.415280374814756e-05\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 4.4148931629024446e-05\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 4.41438244888559e-05\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 4.414926719618961e-05\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 4.4188641768414525e-05\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 4.414670023834333e-05\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 4.413289309013635e-05\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 4.4168671884108335e-05\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 4.4177972595207395e-05\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 4.4132935290690514e-05\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 4.413427726831287e-05\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 4.4140702520962804e-05\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 4.414974871906452e-05\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 4.421276229550131e-05\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 4.4149394962005315e-05\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 4.4140226527815685e-05\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 4.413758186274208e-05\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 4.4145275751361623e-05\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 4.413615868543275e-05\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 4.416081137605943e-05\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 4.415513831190765e-05\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 4.414467795868404e-05\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 4.413652583025396e-05\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 4.417843752889894e-05\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 4.414547889609821e-05\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 4.413427610415965e-05\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 4.415399496792816e-05\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 4.417732488946058e-05\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 4.413941758684814e-05\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 4.413236427353695e-05\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 4.4137266231700776e-05\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 4.4177283853059636e-05\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 4.415520408656448e-05\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 4.413615504745394e-05\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 4.416460506035946e-05\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 4.418533804710023e-05\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 4.41316282376647e-05\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 4.4136784999864176e-05\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 4.413822724018246e-05\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 4.4205487502040344e-05\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 4.41352887719404e-05\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 4.413547590957023e-05\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 4.412964102812111e-05\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 4.4144479325041175e-05\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 4.413723654579371e-05\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 4.4245809986023235e-05\n",
      "Autoencoder2 training DONE... TOTAL TIME = 261.0626859664917\n",
      "start evaluation on test data for Autoencoder2\n",
      "MSE is 0.10252974767610988\n",
      "mutls per sample is 4194304\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder3\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.004738612305372953\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.000658382581314072\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.00043846041546203196\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.00040319187333807347\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.0003244503680616617\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.0002610800607362762\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00020791937655303627\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00019678039359860122\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.0001954822998959571\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.00019471456529572605\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.00019328461086843163\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00018938204098958522\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.00018206886190455407\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00017159879789687693\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00016132958407979458\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.00015437069640029222\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.0001448840869124979\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00013242627726867796\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.00012555463821627198\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.0001222635235171765\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00012022519222227856\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00011866124055813998\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00011738064786186442\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.00011633987975073979\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.00011547778791282326\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00011473207792732865\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00011419222020776943\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.00011369272484444081\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00011322740436298773\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00011285319546004757\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00011255596415139735\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00011240926716709509\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00011227447510464116\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00011194341845111921\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00011163172719534486\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00011145956435939298\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.00011129798396723345\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00011118467635242269\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00011115145374787972\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00011109550774563104\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00011089045234257356\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.00011082951765274629\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.0001111366818076931\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.00011092714325059206\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.0001107575255446136\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00011042799771530553\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00011029493936803191\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00011023212369764224\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00011022045480785892\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.0001108249876415357\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00011012586968718096\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00011007002205587924\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00011000197497196496\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.0001099939679261297\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00011007005581632256\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00011095061170635746\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00011030587018467485\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00011004258558386937\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.0001097582545480691\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00010968869872158394\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00010969412600388751\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00010987112211296334\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.0001106817545951344\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00010996925528161227\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00010985222674207762\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.00010979746613884345\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00010982706997310743\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.00010991352173732594\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00011015362455509604\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00011018677090760321\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00011003109102603048\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.00011012556933565065\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00010972992953611538\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.00010946291877189651\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00010945188259938731\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.00010944694018689915\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00011095193272922188\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00010976946505252272\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00010953697230434045\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00010946993890684098\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00010950135358143597\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.0001095759856980294\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.0001103863341268152\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00011057670548325404\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00010981101397192106\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.00011013152456143871\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00011015572352334857\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.0001097292811027728\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00010933207115158438\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.0001092409441480413\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00010969513852614909\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.00010958995553664864\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00010973499855026602\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00010950848401989787\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00010951888514682651\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00010957719205180183\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.000109910957980901\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00011012585688149556\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00011016716132871806\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00010988490364979953\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00010961089021293446\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00011004914675140754\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00011019831261364743\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00010956463782349601\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00010901975794695317\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00010883009934332222\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00010884432296734303\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.0001089603835134767\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.000110069943475537\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00010919508466031402\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.0001089829474221915\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00010894797218497842\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00010895046376390383\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.00010909307427937166\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00010970634582918137\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00011030324036255479\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.00010909943433944136\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00010903033049544319\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00010919696127530187\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00010919691325398162\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00010941172222374007\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00010978952806908637\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.0001100853004027158\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00010979967890307308\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00010864895361009986\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00010848183621419593\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00010859312867978588\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00010867450822843238\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.00010874254017835483\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00010883551003644243\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00010987801681039855\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00010934535355772824\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00010893273225519807\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00010875900683458895\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00010860721115022897\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00010858011024538428\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00010859688889468089\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.0001092445824178867\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00011017105309292674\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00010875617561396212\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00010892904043430463\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00010915882332483306\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00010958487371681258\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.00010957072692690417\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00010865740332519636\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00010837180452654138\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00010838739981409163\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00010843533207662404\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00010854183841729536\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00010863922594580799\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00010900597175350413\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00010901495552388951\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.0001089603325817734\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00010853551590116695\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00010835561348358169\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00010837855807039886\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.00011020823963917791\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00010909664910286664\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00010852542851353064\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00010871332924580201\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00010890888253925368\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.00010911464400123805\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.00010887599986745045\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00010848461097339168\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00010844025877304375\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00010861977352760732\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00010889518656767905\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00010908444615779444\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00010940300388028845\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00010968003218295053\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00010943462344584986\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00010860801819944754\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.000108350892260205\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00010842473682714626\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00010849749436601996\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.00010856513952603564\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00010853616055101156\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.00010821940493769944\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00010796761838719249\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00010786634520627558\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00010783294710563496\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.00010783451987663284\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00010782537166960538\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00010782435070723295\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00010780435259221122\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.00010779100266518071\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.00010777297895401716\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00010775507282232865\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00010774051886983216\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00010771925066364929\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.00010770146240247413\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.00010768508567707613\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00010767010156996548\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00010764763923361898\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00010768038308015093\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00010818213631864637\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.0001079720602137968\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00010779628559248522\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00010751687077572569\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00010748095606686547\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00010752478701760992\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00010755172377685085\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.00010761781333712861\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00010781083314213902\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00010784947109641507\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00010790127766085788\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00010745408391812816\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00010745301784481853\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00010754160641226917\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.00010764061502413825\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00010777854855405167\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00010799306211993098\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00010832697880687192\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00010862805851502344\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00010872301616473124\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00010917674982920289\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00010854920808924362\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00010716490360209718\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.00010681499872589484\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.0001068542679422535\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00010689263872336597\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.0001069172500865534\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00010692669049603864\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00010692377254599706\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00010692070878576488\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00010692010197089985\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00010692566283978521\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00010691732168197632\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00010690632916521281\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00010689432790968568\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00010687803965993226\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00010685019718948751\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.00010683204920496791\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00010679943312425167\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00010677129379473627\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00010674015298718587\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.00010671297699445859\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00010669608222087846\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00010667213733540848\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00010664957168046385\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.00010667472757631913\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00010670197050785646\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00010721010999986902\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.00010657751350663602\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00010647039453033358\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00010648690891684965\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.00010651659395080059\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00010652734054019674\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00010653565055690706\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00010654020094079897\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00010654633777448908\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00010656387137714774\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00010659714695066214\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00010666073008906096\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00010679350525606423\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00010708152170991525\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00010728879133239388\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00010673619428416714\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00010660050815204158\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00010656430356903001\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00010661765059921891\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00010678560909582302\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00010710302391089499\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.0001072109243250452\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00010666749411029741\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.000106316335441079\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00010622754838550463\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00010622016416164115\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00010620341345202178\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00010616849787766114\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00010612256417516619\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00010607945150695742\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00010603494360111654\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00010599552799249068\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00010596752865239978\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00010593238228466362\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.0001058980479137972\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00010587535973172635\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.0001058472425211221\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00010581425332929939\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.00010579059016890824\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00010577276756521315\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.00010575968684861436\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00010574660147540271\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00010572837607469409\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00010570787242613732\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00010568792931735516\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00010567408957285806\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.0001056578746647574\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00010564530035480857\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00010563173942500726\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00010561426053754985\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00010559109126916155\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00010557769506704062\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00010556493245530873\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.00010555880813626572\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00010554610373219474\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00010564227646682411\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00010554772306932136\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00010587588272755965\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00010552492254646495\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00010551281593507156\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00010553749918472021\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.0001055989196174778\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00010572547209449113\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.00010586160758975894\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00010572800820227713\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00010552983643719926\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00010552624589763582\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00010564071359112858\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00010571375692961737\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00010570084035862238\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.00010559324087807908\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00010549663129495457\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.00010544460645178332\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00010542374482611194\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00010542704490944744\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00010541502182604745\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00010537738766288385\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00010532603366300464\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.00010525893361773342\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.00010518108116229997\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00010513355722650886\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.00010511516738915816\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00010511683387449012\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.00010513462242670358\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.0001051467756042257\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00010516496375203133\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00010520035837544128\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00010525806486839428\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00010532372078159825\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.00010535824781982228\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.00010532430344028398\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00010522919445065781\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00010510299995075912\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00010499184631044045\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00010493419540580362\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00010492654866538941\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00010494241665583104\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00010496516450075432\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00010499088268261402\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00010502894612727686\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00010507466824492439\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00010512556094909087\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00010517378163058311\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.00010519425413804128\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00010510430904105306\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00010495513473870233\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00010489809181308374\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00010492773697478696\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00010501366690732539\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00010511318338103592\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.00010503677214728668\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00010480411670869216\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.00010474581649759785\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.0001047332608141005\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.00010472254187334329\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00010471719084307551\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.00010469336149981245\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00010464926337590441\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.0001045930670807138\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00010454548959387466\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00010451028909301385\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00010448307148180902\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00010446928383316844\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.0001044564280891791\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00010444869491038844\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00010444528190419078\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00010444307990837842\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00010444438870763406\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00010444251674925908\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00010442864178912714\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00010441246326081455\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.00010439757315907627\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.00010438259749207646\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00010437302844366059\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.00010436271579237654\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.0001043496048077941\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.00010433960182126611\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.0001043247216148302\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00010431441420223564\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00010430035035824403\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.00010428695328300819\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00010426969762193038\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00010425388318253681\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.0001042449235683307\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00010424141713883727\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00010425108834169805\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00010426852677483111\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00010440906102303416\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00010416944307507947\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00010414991236757488\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.0001041463710134849\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00010416034609079362\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.0001041808005538769\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00010424714913824573\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.00010439288889756426\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00010434192081447691\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00010413031268399208\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00010404776461655275\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00010401074716355652\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.00010398219688795507\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00010396125784609467\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00010394896642537788\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00010394537093816325\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00010394670651294291\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00010394689423264935\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.00010395176592282951\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00010395753663033246\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00010396299127023668\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00010397143167210743\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00010398486367193982\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00010400983184808865\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00010406168934423476\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00010413231007987634\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.00010424103442346677\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.00010434266412630678\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00010430165013531223\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00010415428463602439\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00010401667765108869\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00010394867364084348\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.00010391918593086302\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00010396378231234848\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.0001039033746928908\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00010383123328210785\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00010373679659096524\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00010373405530117452\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00010372911085141822\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00010373554046964272\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.00010374690522439778\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00010378922277595848\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00010385427653091028\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00010396407422376797\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00010392751835752278\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.00010377742466516793\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00010376190097304061\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00010378928680438549\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00010381882602814586\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00010382731619756669\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00010384985303971916\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00010386757465312257\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00010387528978753834\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00010385594447143376\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00010380533029092476\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.00010377492581028491\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.0001037497608922422\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00010375519486842677\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00010377759608672931\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.00010378901817603036\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00010381794447312132\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00010385222383774818\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.00010389849892817438\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00010395231685834006\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.0001040084514534101\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.0001040086371358484\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00010395292367320507\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00010382024018326774\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00010375720885349437\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00010375044279498979\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00010373825556598603\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00010364903369918466\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.0001035260988282971\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00010345001763198525\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.00010342004388803616\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.00010340580513002351\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.00010339340195059777\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00010338581807445735\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.00010337456973502412\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.00010336620238376781\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00010335490835132077\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00010334315738873557\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.0001033330344944261\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00010332101839594543\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.00010330921737477184\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00010329818789614364\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.00010328973323339596\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.00010328194388421252\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.0001032720145303756\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00010326185962185264\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.0001032498897984624\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.0001032429569750093\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00010323530004825443\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00010323039052309468\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00010321926703909411\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.0001032093635876663\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00010319864610210061\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00010319042252376675\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00010318383021513\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00010317951469914987\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00010317799693439156\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.0001031715958379209\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00010317130043404177\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.00010316215513739735\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.00010314774321159348\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00010313238948583603\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00010312266211258247\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00010311849764548242\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00010311449354048818\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00010311110090697185\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00010310819896403701\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.00010310098063200713\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00010310370067600161\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.0001031025915290229\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.0001030776952393353\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00010309063480235637\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00010308747499948367\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00010307265241863206\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00010306848853360861\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.0001030657539376989\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.00010306249867426232\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00010305144591256977\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.00010305093630449846\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00010303981543984264\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.0001030424132477492\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00010303632385330274\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00010303336312063038\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00010302682931069284\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00010302941111149266\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.0001030330458888784\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.00010303034854587167\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.00010304577765055\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00010316344036255031\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.0001031348729156889\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00010301660076947883\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.0001030284323496744\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.00010304159426596016\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00010308424156391993\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00010313724604202435\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00010309095290722325\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00010298845154466107\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00010297671338776127\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00010298238310497255\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00010297366126906127\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00010297336324583739\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00010298230801708997\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.0001029933663085103\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00010300633497536183\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.00010302933544153347\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00010304607858415693\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00010304386465577409\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.0001030221683322452\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.00010300769354216754\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.0001029943220783025\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00010298243170836941\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00010297127912053838\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00010296095453668386\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00010295458312612027\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00010293794475728646\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.00010292507475242019\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.0001029058825224638\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00010289193276548759\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00010288868914358318\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00010292486986145377\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00010300682886736467\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00010313440026948228\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.00010305284813512116\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00010289474914316088\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.00010283599869580939\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00010281367838615551\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00010279353271471337\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00010278646135702729\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.00010277388704707846\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.00010276897955918685\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00010276273096678778\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00010276164364768192\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00010276115674059838\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00010276500426698477\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00010276801855070516\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00010276120039634406\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00010275764856487512\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00010275670298142359\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00010275917098624632\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00010276287648594006\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00010276176442857831\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00010276427812641486\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.00010276484856149182\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00010276496526785194\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.00010275385167915374\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00010273381019942462\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.00010270993778249248\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00010268289304804056\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00010265787917887792\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.0001026376208756119\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.0001026210974669084\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00010260672308504582\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00010259432747261599\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.00010258208611048758\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00010257027519401162\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.00010255949076963589\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00010254884662572295\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.00010253934917273\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00010253092506900429\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00010252299514831975\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.0001025132997892797\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00010250486200675369\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.00010249813523842021\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00010249158862279728\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00010248344653518871\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00010247447877191007\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00010246858961181716\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00010246010991977527\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00010245300509268418\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00010244746459648013\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.00010244076896924526\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.00010243370721582323\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00010242648393614217\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.00010242125921649858\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00010241599316941574\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00010241061478154734\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00010240585717838257\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.00010240045696264133\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.00010239410970825701\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00010239171737339348\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00010238962335279211\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.0001023875895771198\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00010238649207167328\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00010238734219456092\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.00010238775983452797\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00010238607181236148\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00010238418035442009\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00010238302173092961\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00010238478193059563\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00010238669230602682\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00010238899441901595\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.00010239541530609131\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00010238849296001718\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.00010239268012810499\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00010240194504149258\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00010241231299005448\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00010244426870485768\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00010250847903080285\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00010253110813209786\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00010253887274302542\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00010258357360726222\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00010252127860439941\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.0001024444805807434\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.0001024239559774287\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00010242321237456053\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.0001024356289417483\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00010245405021123588\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00010248854960082099\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00010253270447719842\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.0001025774807203561\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00010266216908348724\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00010276305547449738\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00010277091641910374\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.0001026146876392886\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00010247428028378636\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00010243880649795755\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00010245575569570064\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.0001025019321241416\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00010255352535750716\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00010248886625049635\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00010238146816845983\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00010233153414446861\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00010230998421320692\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.0001022950338665396\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00010228197206743061\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.0001022703800117597\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00010225981881376356\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00010225290345260873\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00010224602330708876\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00010223979421425611\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00010223509540082886\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00010223170596873387\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00010222513199551031\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.00010221743199508638\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.0001022100422414951\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.0001022023384575732\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00010219280520686879\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00010218669805908576\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.00010218054434517399\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.00010217599658062681\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.000102170764002949\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00010216540802503005\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.00010216383641818538\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.00010215951886493712\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00010215495363809168\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00010214985377388075\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00010214777896180749\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.00010214695328613743\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.0001021438295720145\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.0001021418091841042\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00010213721718173474\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00010213442845270038\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00010213528410531581\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00010213357891188934\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.0001021321234293282\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00010213295463472605\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00010213125875452534\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.000102129498263821\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.00010212752415100112\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.0001021263559232466\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.00010213026718702167\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00010213712637778371\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00010214942798484117\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00010216277121799066\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00010218624433036894\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00010221691394690424\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.00010226550803054124\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00010230603773379698\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00010229405626887455\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00010222702636383474\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00010219952033367007\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.00010218470502877608\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00010218283860012889\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00010217987932264805\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00010218697338132188\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00010219993564533069\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00010221947100944816\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00010224518948234618\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00010226830461760983\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.00010228357539745048\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.0001022723427740857\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00010223351884633303\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00010218774114036932\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00010214864043518901\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00010212505410891027\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00010211028711637482\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00010210362350335345\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.00010209948435658589\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00010209496365860105\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00010209685540758074\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00010209587053395807\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.0001020976813742891\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00010209522588411346\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.00010209311411017552\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.00010209204367129133\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.00010209127998678013\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00010208957799477503\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00010208829713519663\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00010208467807387933\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00010208112391410395\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00010207539948169142\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00010207569081103429\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00010207363171502947\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.00010207550891209393\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.00010207505227299407\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00010207695740973576\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00010207917133811862\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00010208134481217712\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00010208722349489108\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00010210194915998726\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00010211434302618727\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00010213485831627622\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00010214740294031799\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00010215368180070072\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00010215322196017951\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00010215659131063148\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00010215018060989678\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00010215744085144251\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.00010216385882813483\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00010216769063845277\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00010214470705250279\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00010211324028205127\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.0001020873078959994\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00010207017068751156\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.00010205711994785815\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00010204800491919741\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.0001020461562438868\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00010204243124462665\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00010204480495303869\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00010204168997006491\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00010204461897956208\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.00010204373102169484\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00010205536178546026\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00010205564874922857\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00010206575447227806\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00010208750987658277\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.0001021152941393666\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00010215429269010201\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00010216000286163763\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00010211615182925015\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.00010204954684013501\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00010201612225500867\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.0001019993209047243\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00010199533629929646\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00010199271055171266\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.0001019865638227202\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00010198531061178073\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00010198024479905143\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00010197928757406772\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00010197553143370897\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00010197314055403695\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00010196671326411888\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00010196247312705964\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.0001019616099074483\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00010195710259722546\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00010195470415055752\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.0001019497073139064\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.0001019474005443044\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00010194443631917239\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00010194238799158483\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00010194031608989462\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00010193764610448852\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00010193272813921795\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00010192991583608092\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00010192665649810806\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.0001019239955348894\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00010192045563599095\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00010191989131271839\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.0001019183115568012\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00010191616864176467\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00010191227775067092\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.00010190793080255389\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00010190667118877173\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00010190410510404036\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.00010189950931817293\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00010189462773269042\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00010189022053964436\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.00010188737011048942\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00010188245156314224\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00010188250191276893\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00010187926236540079\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00010188028507400304\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00010188184241997079\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00010187905136262997\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.00010187806299654766\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.0001018762809690088\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00010187989886617288\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00010187868727371097\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00010188057087361812\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.000101880575530231\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00010188422776991502\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.00010188472486333922\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00010188756248680875\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00010189403081312776\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00010190363216679544\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.0001019165909383446\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.0001019311128766276\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.0001019453298067674\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00010196414979873224\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00010198684787610546\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00010200888849794865\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00010203031823039055\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.00010201960132690147\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.0001019886284484528\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.0001019637385616079\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.00010195848648436367\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00010195661307079717\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00010197481315117329\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00010201128199696541\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00010207196173723787\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00010211135202553123\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00010210446664132177\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00010207745857769623\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00010204008547589183\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00010200996621279046\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.00010197872732533142\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00010196252318564803\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00010194876929745078\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.00010193273745244369\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.000101920579909347\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.0001019102890859358\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00010190212575253099\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.00010189252934651449\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.00010188245301833377\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00010187406733166426\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.0001018697299878113\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.000101863065501675\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00010186049446929246\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.0001018538695643656\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00010184819024289027\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00010184506245423109\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00010184325568843633\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00010183858423260972\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.0001018352760002017\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.00010183383536059409\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00010183224512729793\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00010182601399719716\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.0001018220340483822\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00010182175290537998\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00010181821708101779\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.0001018170325551182\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00010181776364333928\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.00010181872930843383\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00010181662626564503\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00010181511053815484\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.00010181195859331637\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00010180843324633315\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00010180523036979139\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00010179826087551191\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00010179225821048021\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00010178529599215836\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00010177929827477783\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00010177055344684049\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00010176504496484994\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00010176027106354013\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00010175269417231903\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00010175291012274102\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.00010174693510634824\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00010174109949730337\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00010173746617510915\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00010173443064559251\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00010173098999075592\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.0001017300080275163\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.00010172764945309609\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00010172649723244831\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00010172615700867027\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00010172575479373335\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00010172878974117339\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00010172821639571339\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00010172849433729426\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00010172574664466083\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.0001017266054986976\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.00010172476002480835\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00010172493493882939\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.00010172260663239285\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00010171998175792396\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.00010172235779464245\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00010172539099585264\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.00010173097281949594\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.00010173480521189049\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00010173774207942188\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00010174207476666197\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00010175582283409313\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.0001017694192705676\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00010178394935792312\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00010181498801102862\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00010185539431404323\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00010194011265411973\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00010206022212514653\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.00010208923311438411\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00010198685602517798\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00010191614564973861\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00010190175526076928\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00010189110325882211\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00010187978099565953\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.00010186220315517857\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.000101843221927993\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.0001018251755158417\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00010181577818002552\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.00010180781973758712\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00010180567274801432\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.00010179941746173427\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00010179504926782101\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00010179139382671565\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.00010178899101447314\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00010178969940170646\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.0001017864083405584\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.00010178363416343927\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00010178254247875884\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.0001017801501438953\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00010178408003412187\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00010179103788686917\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00010179890523431823\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00010179630859056487\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.00010179984732531011\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00010180765559198335\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00010181422519963235\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.0001018222831771709\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00010182702593738214\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00010183170408708975\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00010183396836509927\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.0001018283754820004\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.0001018237613607198\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00010181441408349201\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00010179694480029866\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00010177900549024343\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.00010176967916777357\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00010175738105317578\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00010174791823374108\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00010173975344514474\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00010172799695283174\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00010171282017836348\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00010170559777179733\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.00010169392538955435\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.0001016827969579026\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.0001016789794084616\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00010167446278501302\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.0001016648006043397\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.00010166203486733138\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.0001016593462554738\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00010165458399569616\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00010165199666516855\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00010164805687963963\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00010164198611164465\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.0001016336603788659\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.0001016307997633703\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00010162641672650352\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00010162448539631441\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.00010162247577682138\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.00010162366001168266\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.00010162478254642337\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00010162444523302838\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.00010162276099435985\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.00010161874874029309\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00010161678714212031\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00010161416430491954\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00010161096812225878\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.0001016079488908872\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.00010160474252188579\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00010160263191210106\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00010160261095734313\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00010159899975406005\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00010159928002394736\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00010159855883102864\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.00010159867641050368\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.00010159430530620739\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.0001015943253878504\n",
      "Autoencoder3 training DONE... TOTAL TIME = 201.03290915489197\n",
      "start evaluation on test data for Autoencoder3\n",
      "MSE is 0.005002777853191912\n",
      "mutls per sample is 2099200\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder4\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.0002764511865098029\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.00011548058100743219\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.00010709114256314934\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.00010447498731082305\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.00010320370667614042\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.00010266414668876678\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00010135512769920751\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00010070503718452528\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.00010045428236480802\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.00010024793620686978\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.0001001112227095291\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00010008548008045182\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 9.964865777874366e-05\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 9.979444963391871e-05\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 9.90915612783283e-05\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 9.954750130418688e-05\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 9.869931178400292e-05\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 9.947838232619688e-05\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 9.853108058450743e-05\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 9.79714360437356e-05\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 9.860557707725093e-05\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 9.734717110404745e-05\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 9.727849712362513e-05\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 9.802415675949305e-05\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 9.646662161685526e-05\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 9.585707448422909e-05\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 9.526074572931975e-05\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 9.455703751882538e-05\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 9.418111381819472e-05\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 9.394389082444831e-05\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 9.400259994436056e-05\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 9.260160761186852e-05\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 9.223617147654296e-05\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 9.146295342361554e-05\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 9.15942620486021e-05\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 9.194062411552295e-05\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 9.091328654903919e-05\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 8.993813942652196e-05\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 8.942838612711058e-05\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 8.924205059884115e-05\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 8.883962378604338e-05\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 9.022298851050436e-05\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 9.112716768868268e-05\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 9.466307732509449e-05\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 9.986514109186828e-05\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00010422275430755689\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 9.634504822315648e-05\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 9.334106638561935e-05\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 9.019003016874194e-05\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 8.846057869959622e-05\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 9.194708662107586e-05\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 8.785268990322947e-05\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 8.722312690224498e-05\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 8.665881585329772e-05\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 8.632360084448009e-05\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 8.592697995482012e-05\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 8.561641443520785e-05\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 8.592180354753509e-05\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 8.49289377219975e-05\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 8.494090172462166e-05\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 8.442412887234241e-05\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 8.433455805061385e-05\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 8.39127623476088e-05\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 8.417346631176769e-05\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 8.357014157809318e-05\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 8.354653749847784e-05\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 8.335427613928914e-05\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 8.309866796480492e-05\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 8.307633019285277e-05\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 8.251848368672653e-05\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 8.25044108205475e-05\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 8.214393339585513e-05\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 8.234224311308936e-05\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 8.254965970991179e-05\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 8.177512791007757e-05\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 8.186153223505244e-05\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 8.147352666128427e-05\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 8.145566069288179e-05\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 8.111483504762872e-05\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 8.087669411906972e-05\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 8.096229837974534e-05\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 8.095086668618024e-05\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 8.493415924021975e-05\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 9.808246715692804e-05\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 8.832004416035488e-05\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 8.597358537372201e-05\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 8.310222212458029e-05\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 8.602766378317028e-05\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 8.179622411262244e-05\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 8.110808936180547e-05\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 8.02244944497943e-05\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 8.002919639693573e-05\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 7.962936128024012e-05\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 7.993452803930268e-05\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 7.93262358638458e-05\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 7.915790483821183e-05\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 7.90520099690184e-05\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 7.907892111688852e-05\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 7.881331432145089e-05\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 7.877757510868833e-05\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 7.885420898674057e-05\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 7.849647430703043e-05\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 7.852318201912568e-05\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 7.833026378648356e-05\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 7.840185950044542e-05\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 7.829826470697299e-05\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 7.828676578355953e-05\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 7.817297737346962e-05\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 7.870890229241923e-05\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 7.962954929098487e-05\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 8.567510027205571e-05\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 8.445391227724031e-05\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 9.666373051004485e-05\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 9.145085728960111e-05\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 8.267197059467434e-05\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 8.125860273139551e-05\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 7.913023844594136e-05\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 7.896805997006595e-05\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 7.811138115357608e-05\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 7.89577467367053e-05\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 7.802702457411214e-05\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 7.750895660137758e-05\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 7.734296756098047e-05\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 7.743151625618339e-05\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 7.715780142461881e-05\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 7.722431211732328e-05\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 7.700240501435474e-05\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 7.706697069806979e-05\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 7.681465591304004e-05\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 7.672258885577321e-05\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 7.676447654375807e-05\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 7.661051466129721e-05\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 7.655038149096072e-05\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 7.645540870726108e-05\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 7.66273311455734e-05\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 7.627122919075191e-05\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 7.678218273213133e-05\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 7.621060300152748e-05\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 7.62616441352293e-05\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 7.611234148498625e-05\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 7.615006994456053e-05\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 7.618726609507575e-05\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 7.613680936628953e-05\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 7.584990438772366e-05\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 7.612643967149779e-05\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 7.601221790537238e-05\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 7.572195987449959e-05\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 7.628228253452107e-05\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 7.622659555636346e-05\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 8.189929183572531e-05\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 8.531343773938715e-05\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 7.809208065737039e-05\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 7.68572188098915e-05\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 7.580077071906998e-05\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 7.545368338469415e-05\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 7.596129114972427e-05\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 7.525584165705367e-05\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 7.53000020631589e-05\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 7.521240069763735e-05\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 7.514490658650175e-05\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 7.522741798311472e-05\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 7.507032336434349e-05\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 7.507749804062769e-05\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 7.520004641264677e-05\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 7.487841387046501e-05\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 7.509064569603652e-05\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 7.498745020711794e-05\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 7.488993171136826e-05\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 7.47317890636623e-05\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 7.514684170018881e-05\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 7.520125538576394e-05\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 7.534249307354912e-05\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 8.990296832052991e-05\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 9.74471028894186e-05\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 8.627851289929822e-05\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 9.036399249453098e-05\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 8.304770424729213e-05\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 7.692568062338977e-05\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 8.146658539772034e-05\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 7.594757742481307e-05\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 7.516436977311969e-05\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 7.531195995397866e-05\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 7.469359348760918e-05\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 7.51037453301251e-05\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 7.454425795003772e-05\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 7.445492228725925e-05\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 7.459595944965258e-05\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 7.435274892486632e-05\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 7.423841772833839e-05\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 7.472511264495551e-05\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 7.425476796925068e-05\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 7.421926391543821e-05\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 7.411104394122958e-05\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 7.424983894452453e-05\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 7.408963196212426e-05\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 7.403990894090384e-05\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 7.393566134851426e-05\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 7.415861706249416e-05\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 7.386061333818361e-05\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 7.405729236779734e-05\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 7.388612837530673e-05\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 7.396267785225063e-05\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 7.39158017677255e-05\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 7.377874513622373e-05\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 7.397007429972291e-05\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 7.368918129941449e-05\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 7.394048938294873e-05\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 7.372334628598765e-05\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 7.3657984030433e-05\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 7.379220856819302e-05\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 7.359549636021256e-05\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 7.38673922023736e-05\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 7.389730511931702e-05\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 7.364389341091738e-05\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 7.342497672652826e-05\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 7.373270345851779e-05\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 7.352045970037579e-05\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 7.353929220698773e-05\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 7.362013042438775e-05\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 7.3368655575905e-05\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 7.356855901889503e-05\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 7.34545144950971e-05\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 7.324293372221291e-05\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 7.412909297272564e-05\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 7.402344519505277e-05\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 7.542086008470506e-05\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 9.289764071581885e-05\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 8.016763953492046e-05\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 8.641054446343333e-05\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 8.168860425939783e-05\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 7.865161605877802e-05\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 7.438861997798085e-05\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 7.8137336531654e-05\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 7.450400793459267e-05\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 7.360704417806118e-05\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 7.420650275889784e-05\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 7.334939640713855e-05\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 7.323684170842171e-05\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 7.3181587504223e-05\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 7.328149717068299e-05\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 7.32467949273996e-05\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 7.310031971428543e-05\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 7.301776437088847e-05\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 7.29879952268675e-05\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 7.296917436178774e-05\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 7.292192167369649e-05\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 7.301345787709579e-05\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 7.287842541700229e-05\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 7.291262649232522e-05\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 7.282293227035552e-05\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 7.295580435311422e-05\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 7.279202633071691e-05\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 7.296885538380592e-05\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 7.286959327757359e-05\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 7.281420432263985e-05\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 7.29907484492287e-05\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 7.279784185811877e-05\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 7.287519692908973e-05\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 7.26517810835503e-05\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 7.290736888535321e-05\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 7.263693842105568e-05\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 7.304395810933784e-05\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 7.263220846652985e-05\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 7.28186426567845e-05\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 7.255408156197517e-05\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 7.276313554029911e-05\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 7.254822558024898e-05\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 7.300201279576868e-05\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 7.260035286890343e-05\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 7.261496793944388e-05\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 7.27400227333419e-05\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 7.247391593409702e-05\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 7.271996990311891e-05\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 7.266625005286186e-05\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 7.256829412654043e-05\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 7.247443369124085e-05\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 7.259957288624719e-05\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 7.250430062413215e-05\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 7.253014569869265e-05\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 7.283311482751742e-05\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 7.241758925374597e-05\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 7.289459579624236e-05\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 7.24315075785853e-05\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 7.239317928906531e-05\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 7.249122252687812e-05\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 7.254254422150552e-05\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 7.239837723318487e-05\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 7.234877324663102e-05\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 7.260565820615738e-05\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 7.224519358715043e-05\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 7.247590692713857e-05\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 7.258155528688803e-05\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 7.217516569653526e-05\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 7.265724590979516e-05\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 7.217852340545506e-05\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 7.23871003719978e-05\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 7.242024090373888e-05\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 7.216554746264592e-05\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 7.275930343894288e-05\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 7.208871800685302e-05\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 7.247242145240307e-05\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 7.212839758722112e-05\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 7.22898286767304e-05\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 7.229148061014711e-05\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 7.202907087048515e-05\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 7.247979956446216e-05\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 7.209554227301852e-05\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 7.259738689754159e-05\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 7.21561603131704e-05\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 7.199457788374275e-05\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 7.227598165627568e-05\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 7.197753759101033e-05\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 7.306869927560911e-05\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 8.959967701230198e-05\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 8.709412737516687e-05\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 8.477217459585518e-05\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 7.711639540502801e-05\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 7.508401031373069e-05\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 7.356189045822248e-05\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 7.327991246711463e-05\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 7.293406786629931e-05\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 7.22203723853454e-05\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 7.229475944768637e-05\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 7.203919521998614e-05\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 7.208232855191455e-05\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 7.193982164608315e-05\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 7.190261880168691e-05\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 7.195561134722084e-05\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 7.195720332674683e-05\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 7.195125595899299e-05\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 7.191118522314354e-05\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 7.183631270891056e-05\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 7.202654000138863e-05\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 7.181932975072413e-05\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 7.195031124865637e-05\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 7.189428171841428e-05\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 7.19064025906846e-05\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 7.190543867181987e-05\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 7.18010900891386e-05\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 7.192507153376937e-05\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 7.173563702963293e-05\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 7.204489782452583e-05\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 7.178592408308759e-05\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 7.210616982774809e-05\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 7.170561410021037e-05\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 7.179193227784707e-05\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 7.17635650653392e-05\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 7.180577289545908e-05\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 7.184592890553176e-05\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 7.171668083174155e-05\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 7.192030549049377e-05\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 7.167125993873925e-05\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 7.189684780314564e-05\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 7.16170750092715e-05\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 7.207058137282729e-05\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 7.163038797443733e-05\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 7.200970692792907e-05\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 7.169330172473564e-05\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 7.193560333689674e-05\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 7.161721412558109e-05\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 7.17345654265955e-05\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 7.16200127499178e-05\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 7.199343963293359e-05\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 7.15485360706225e-05\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 7.228562957607209e-05\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 7.152387581299991e-05\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 7.17240126687102e-05\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 7.167757110437378e-05\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 7.169137214077637e-05\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 7.1972067526076e-05\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 7.157039595767856e-05\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 7.178833300713449e-05\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 7.149490673327819e-05\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 7.173505146056414e-05\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 7.17076865839772e-05\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 7.155897794291376e-05\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 7.197048602392897e-05\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 7.154971885029227e-05\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 7.178694679168984e-05\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 7.140123605495319e-05\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 7.190803589764982e-05\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 7.159169617807493e-05\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 7.155113416956738e-05\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 7.161277811974287e-05\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 7.157512009143829e-05\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 7.17677516513504e-05\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 7.14872032403946e-05\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 7.21942278323695e-05\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 7.255211850861088e-05\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 8.316176012158393e-05\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 8.713620365597308e-05\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 7.776138838380575e-05\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 7.428980577969924e-05\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 7.251437724335119e-05\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 7.20830651698634e-05\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 7.176474435254931e-05\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 7.233331794850528e-05\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 7.315978815313429e-05\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 8.37349728681147e-05\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 8.265960466815159e-05\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 7.738046639133245e-05\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 7.311936176847667e-05\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 7.200677355285734e-05\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 7.169802382122725e-05\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 7.18667233013548e-05\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 7.15589668834582e-05\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 7.169212418375536e-05\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 7.152444013627245e-05\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 7.144975679693744e-05\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 7.165309420088306e-05\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 7.14357296237722e-05\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 7.149752549594268e-05\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 7.139476714655757e-05\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 7.144426635932177e-05\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 7.138769200537354e-05\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 7.147467171307653e-05\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 7.134619343560189e-05\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 7.149971061153338e-05\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 7.132040802389383e-05\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 7.163433561800047e-05\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 7.141229289118201e-05\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 7.12900041253306e-05\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 7.142741116695106e-05\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 7.136750558856874e-05\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 7.13854978675954e-05\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 7.14979725307785e-05\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 7.129775825887918e-05\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 7.141878450056538e-05\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 7.132213155273347e-05\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 7.137929409509525e-05\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 7.158511085435749e-05\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 7.137028151191772e-05\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 7.141958514694124e-05\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 7.125362724764273e-05\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 7.143883791286498e-05\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 7.131591381039471e-05\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 7.137929671443999e-05\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 7.149012177251279e-05\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 7.121000031474978e-05\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 7.169456803239882e-05\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 7.126183219952509e-05\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 7.143470604205504e-05\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 7.139636290958151e-05\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 7.126082025934011e-05\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 7.145842275349423e-05\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 7.134096842491999e-05\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 7.120242662495002e-05\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 7.148423639591784e-05\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 7.132347644073889e-05\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 7.129902747692541e-05\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 7.147297787014395e-05\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 7.12615906377323e-05\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 7.132150261895731e-05\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 7.144182513002306e-05\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 7.123351329937577e-05\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 7.135014893719927e-05\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 7.13929912308231e-05\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 7.123586867237464e-05\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 7.16361470404081e-05\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 7.134161365684122e-05\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 7.122629496734589e-05\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 7.13307352270931e-05\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 7.146827498218045e-05\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 7.113003637641668e-05\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 7.181274995673447e-05\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 7.126266864361241e-05\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 7.136221131077037e-05\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 7.111422077286989e-05\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 7.163561211200431e-05\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 7.121681293938309e-05\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 7.151819037972018e-05\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 7.139695313526317e-05\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 7.14892556425184e-05\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 7.119024259736761e-05\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 7.132849772460759e-05\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 7.134618324926123e-05\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 7.116638822481036e-05\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 7.143364229705184e-05\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 7.119662885088474e-05\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 7.14442238677293e-05\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 7.125490461476147e-05\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 7.11842262535356e-05\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 7.139472145354376e-05\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 7.133859675377607e-05\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 7.149656681576744e-05\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 7.130609272280708e-05\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 7.119280169717968e-05\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 7.125109987100586e-05\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 7.121918722987175e-05\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 7.126213400624692e-05\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 7.111661339877173e-05\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 7.133423088816925e-05\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 7.163358124671504e-05\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 7.192509045125917e-05\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 7.120397785911336e-05\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 7.144104078179226e-05\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 7.124184659915045e-05\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 7.116817665519193e-05\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 7.131807506084442e-05\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 7.114688836736605e-05\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 7.125146861653774e-05\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 7.133988488931208e-05\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 7.104671414708718e-05\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 7.145889569073915e-05\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 7.103298412403092e-05\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 7.151334953960031e-05\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 7.130227371817455e-05\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 7.149865821702406e-05\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 7.120297843357548e-05\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 7.126137148588896e-05\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 7.122084323782474e-05\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 7.107642682967708e-05\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 7.122445735149086e-05\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 7.112650404451414e-05\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 7.112738385330885e-05\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 7.138167682569475e-05\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 7.109883619705216e-05\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 7.226878136862069e-05\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 7.183303328929469e-05\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 8.51585526834242e-05\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 7.9461848072242e-05\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 7.867421460105107e-05\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 8.062482724199072e-05\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 8.753960020840167e-05\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 7.706142583629117e-05\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 7.283155195182189e-05\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 7.323235477088019e-05\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 7.17744935536757e-05\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 7.164046284742654e-05\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 7.132999307941645e-05\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 7.139614637708292e-05\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 7.123588235117494e-05\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 7.118866400560364e-05\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 7.115567859727889e-05\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 7.113291852874682e-05\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 7.11683026747778e-05\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 7.108409481588751e-05\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 7.105578610207886e-05\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 7.115769345546141e-05\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 7.102497533196583e-05\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 7.112698775017634e-05\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 7.098823261912912e-05\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 7.114219246432185e-05\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 7.098850997863337e-05\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 7.104713789885863e-05\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 7.105272496119142e-05\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 7.098368019796908e-05\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 7.106990233296528e-05\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 7.099360635038466e-05\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 7.102088100509718e-05\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 7.102951902197674e-05\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 7.12224084418267e-05\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 7.096766465110705e-05\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 7.114339852705598e-05\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 7.10791238816455e-05\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 7.11046258220449e-05\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 7.107193203410134e-05\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 7.108246558345854e-05\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 7.100986898876727e-05\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 7.103632000507787e-05\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 7.104245742084458e-05\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 7.109811762347818e-05\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 7.096344401361421e-05\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 7.123605028027668e-05\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 7.099360373103991e-05\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 7.101038296241314e-05\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 7.11047777440399e-05\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 7.099847571225836e-05\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 7.109931815648452e-05\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 7.097975234501064e-05\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 7.10826221620664e-05\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 7.100790622644127e-05\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 7.105669617885724e-05\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 7.109918835340067e-05\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 7.099422509782016e-05\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 7.118132867617533e-05\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 7.092418993124738e-05\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 7.115031272405758e-05\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 7.106938341166824e-05\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 7.109117723302916e-05\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 7.097916590282693e-05\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 7.109157944796606e-05\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 7.108761696144938e-05\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 7.100312155671418e-05\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 7.11805812898092e-05\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 7.097594789229333e-05\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 7.120923546608538e-05\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 7.094433880411088e-05\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 7.150038552936167e-05\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 7.096414949046447e-05\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 7.133874809369445e-05\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 7.092703832313418e-05\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 7.135526248021051e-05\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 7.090529863489791e-05\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 7.127110817236825e-05\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 7.098075322574004e-05\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 7.112042687367647e-05\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 7.108733931090682e-05\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 7.098513364326209e-05\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 7.126737618818879e-05\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 7.096291228663177e-05\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 7.130161131499335e-05\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 7.099155976902693e-05\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 7.121254689991474e-05\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 7.090501399943605e-05\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 7.118336186977103e-05\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 7.084665296133608e-05\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 7.166909810621291e-05\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 7.095991895766929e-05\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 7.12130541796796e-05\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 7.089337101206184e-05\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 7.130401296308264e-05\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 7.112712453817949e-05\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 7.099209149600938e-05\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 7.10675519076176e-05\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 7.111470942618326e-05\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 7.103964686393738e-05\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 7.108474354026839e-05\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 7.087903388310224e-05\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 7.116715831216425e-05\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 7.09552099579014e-05\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 7.114009931683541e-05\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 7.097529887687415e-05\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 7.099063135683537e-05\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 7.086164259817451e-05\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 7.119183545000851e-05\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 7.118031557183713e-05\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 7.285699481144547e-05\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 8.836627501295879e-05\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 7.440923887770623e-05\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 7.468010648153722e-05\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 8.07856532628648e-05\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 8.65991212776862e-05\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 8.324071648530663e-05\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 7.76342861354351e-05\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 7.414753054035828e-05\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 7.192355173174292e-05\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 7.424822280881926e-05\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 7.316665403777733e-05\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 7.191368407802657e-05\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 7.149164972361177e-05\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 7.162064604926855e-05\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 7.11748650064692e-05\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 7.107448385795578e-05\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 7.1331990766339e-05\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 7.104594493284822e-05\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 7.098479371052235e-05\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 7.101199007593095e-05\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 7.097257854184136e-05\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 7.095201493939385e-05\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 7.105415919795632e-05\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 7.0989103987813e-05\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 7.090066763339565e-05\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 7.122508861357347e-05\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 7.09151171031408e-05\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 7.095807406585663e-05\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 7.097971974872053e-05\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 7.089829625329003e-05\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 7.090225786669179e-05\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 7.092473038937897e-05\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 7.087192323524506e-05\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 7.10080424323678e-05\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 7.08199999644421e-05\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 7.10222401539795e-05\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 7.081657298840582e-05\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 7.100860471837223e-05\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 7.084085169481113e-05\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 7.095221488270909e-05\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 7.089815888321027e-05\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 7.08364965976216e-05\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 7.102576113538816e-05\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 7.080193376168608e-05\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 7.102187955752016e-05\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 7.081013260176405e-05\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 7.100833754520863e-05\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 7.084423559717834e-05\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 7.104226417141035e-05\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 7.090941013302653e-05\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 7.090922415954993e-05\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 7.091645791660994e-05\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 7.088592246873304e-05\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 7.098306203261018e-05\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 7.08392122760415e-05\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 7.099399052094668e-05\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 7.080036419210956e-05\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 7.116047490853816e-05\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 7.077938469592482e-05\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 7.114197360351682e-05\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 7.076956680975854e-05\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 7.119959860574454e-05\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 7.078056398313493e-05\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 7.109030499123036e-05\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 7.078772789100185e-05\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 7.136212661862373e-05\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 7.082011870807037e-05\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 7.10212709964253e-05\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 7.07976971170865e-05\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 7.109274505637586e-05\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 7.0896873367019e-05\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 7.0951821107883e-05\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 7.105790864443406e-05\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 7.08568815025501e-05\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 7.104409160092473e-05\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 7.077385002048687e-05\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 7.122307986719534e-05\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 7.080170849803836e-05\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 7.110856764484196e-05\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 7.08409634535201e-05\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 7.099331938661635e-05\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 7.084151351591572e-05\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 7.113150611985475e-05\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 7.098539732396603e-05\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 7.111246755812317e-05\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 7.08846966153942e-05\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 7.095942506566643e-05\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 7.106396253220737e-05\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 7.092864019796252e-05\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 7.115099026123062e-05\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 7.089785329299047e-05\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 7.10265027009882e-05\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 7.082961761625483e-05\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 7.10775816696696e-05\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 7.08431177190505e-05\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 7.102246978320181e-05\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 7.101662049535662e-05\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 7.099663227563724e-05\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 7.085815363097936e-05\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 7.115518761565908e-05\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 7.076604320900515e-05\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 7.126534124836326e-05\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 7.078185997670516e-05\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 7.119977701222524e-05\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 7.090990198776126e-05\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 7.13871800689958e-05\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 7.098496251273901e-05\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 7.093763240845875e-05\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 7.0929384091869e-05\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 7.098175294231623e-05\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 7.080961164319888e-05\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 7.116898166714236e-05\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 7.08247654256411e-05\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 7.105478609446436e-05\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 7.08411741652526e-05\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 7.105907367076725e-05\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 7.084774610120803e-05\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 7.10291211726144e-05\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 7.090408384101465e-05\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 7.102768518961967e-05\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 7.10568725480698e-05\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 7.107855868525804e-05\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 7.087250123731792e-05\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 7.110035483492538e-05\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 7.093205844284967e-05\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 7.101070805219934e-05\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 7.080388924805447e-05\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 7.112399296602234e-05\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 7.073930406477302e-05\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 7.124036725144833e-05\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 7.080583367496729e-05\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 7.116313965525478e-05\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 7.076733018038795e-05\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 7.123911840608344e-05\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 7.082752534188331e-05\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 7.104769290890545e-05\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 7.080381648847833e-05\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 7.135269057471305e-05\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 7.077841321006417e-05\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 7.106531411409378e-05\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 7.073115993989631e-05\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 7.124216557713225e-05\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 7.103515439666807e-05\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 7.084746845066547e-05\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 7.090084080118686e-05\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 7.083756645442918e-05\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 7.097714900737628e-05\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 7.091920066159219e-05\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 7.104757736669853e-05\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 7.076578913256526e-05\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 7.109074824256822e-05\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 7.071964937495067e-05\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 7.126511336537078e-05\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 7.073835091432556e-05\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 7.12051714072004e-05\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 7.082683587213978e-05\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 7.095132285030559e-05\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 7.081239920808002e-05\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 7.133453123969957e-05\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 7.118829467799515e-05\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 7.071341562550515e-05\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 7.112092251190916e-05\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 7.068086561048403e-05\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 7.123003859305755e-05\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 7.075063535012304e-05\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 7.115933898603544e-05\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 7.101272640284151e-05\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 7.071082742186263e-05\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 7.108921883627772e-05\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 7.064762787194922e-05\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 7.124848198145629e-05\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 7.078166847350075e-05\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 7.095300999935716e-05\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 7.082363561494275e-05\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 7.088162004947662e-05\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 7.087142032105476e-05\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 7.075055851601065e-05\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 7.096405286574736e-05\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 7.068154169246554e-05\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 7.171626260969788e-05\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 7.986143638845533e-05\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 8.34249606123194e-05\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 7.747798255877569e-05\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 7.64573339256458e-05\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 7.596320589073002e-05\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 7.251122704474256e-05\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 7.276690448634326e-05\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 7.127792370738462e-05\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 7.101857947418466e-05\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 7.082299736794085e-05\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 7.076012232573703e-05\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 7.084319862769917e-05\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 7.070331455906853e-05\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 7.0738474605605e-05\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 7.064822304528206e-05\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 7.073688379023224e-05\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 7.061547454213723e-05\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 7.069167128065601e-05\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 7.070860592648387e-05\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 7.063020253553986e-05\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 7.06995470682159e-05\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 7.064877834636718e-05\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 7.066451827995479e-05\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 7.067228812957183e-05\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 7.068091887049377e-05\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 7.068333361530677e-05\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 7.072234759107232e-05\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 7.06171986530535e-05\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 7.076245063217357e-05\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 7.062369404593482e-05\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 7.090302620781586e-05\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 7.066704536555335e-05\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 7.068313483614475e-05\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 7.071394997183233e-05\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 7.077021145960317e-05\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 7.067383354296908e-05\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 7.07874252111651e-05\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 7.06675261608325e-05\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 7.078343100147322e-05\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 7.066917110932991e-05\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 7.081738760462031e-05\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 7.062381715513765e-05\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 7.094407774275169e-05\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 7.063771859975531e-05\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 7.091545994626358e-05\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 7.0629000547342e-05\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 7.101720984792337e-05\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 7.063854543957859e-05\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 7.088719634339213e-05\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 7.06794802681543e-05\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 7.085543969878927e-05\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 7.068461301969364e-05\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 7.091437641065568e-05\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 7.076297246385366e-05\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 7.081617281073704e-05\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 7.0809944299981e-05\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 7.092490210197866e-05\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 7.080084062181413e-05\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 7.077627902617679e-05\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 7.071925967466085e-05\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 7.091711362591014e-05\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 7.063733646646142e-05\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 7.10336840711534e-05\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 7.064550358336419e-05\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 7.101896917447448e-05\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 7.064946548780426e-05\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 7.104076852556319e-05\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 7.071771804476157e-05\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 7.084618060616776e-05\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 7.08705111173913e-05\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 7.072509353747591e-05\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 7.087197358487175e-05\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 7.068340404657647e-05\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 7.09268610808067e-05\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 7.068139122566208e-05\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 7.095914101228118e-05\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 7.072949636494741e-05\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 7.086877885740251e-05\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 7.07415261422284e-05\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 7.086559693561867e-05\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 7.082677620928735e-05\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 7.073946064338088e-05\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 7.081229268806055e-05\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 7.073809072608128e-05\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 7.087896578013898e-05\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 7.066420832416043e-05\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 7.088451646268368e-05\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 7.06617982359603e-05\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 7.09087328868918e-05\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 7.067949074553326e-05\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 7.100680726580321e-05\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 7.079382543452084e-05\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 7.090235099894926e-05\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 7.064223376801237e-05\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 7.090683793649077e-05\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 7.065141544444486e-05\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 7.091278355801478e-05\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 7.080849900376052e-05\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 7.147380674723535e-05\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 7.07043195143342e-05\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 7.081931020366029e-05\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 7.05882639158517e-05\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 7.093067513778806e-05\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 7.067272497806698e-05\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 7.118159322999418e-05\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 7.061628246447072e-05\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 7.085805671522394e-05\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 7.060073810862378e-05\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 7.093248306773603e-05\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 7.062124088406562e-05\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 7.093891937984154e-05\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 7.058865274302662e-05\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 7.101556315319612e-05\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 7.060659409034997e-05\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 7.118651061318814e-05\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 7.062589866109192e-05\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 7.08672491600737e-05\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 7.059004477923737e-05\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 7.092567975632846e-05\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 7.05611304147169e-05\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 7.136027124943212e-05\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 7.0739665243309e-05\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 7.068493869155645e-05\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 7.073825196130201e-05\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 7.069477229379117e-05\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 7.074949447996915e-05\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 7.082559546688572e-05\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 7.117005792679265e-05\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 7.060355885187164e-05\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 7.100715796696022e-05\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 7.06007523695007e-05\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 7.087477337336167e-05\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 7.05801579169929e-05\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 7.10039891419001e-05\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 7.060698699206114e-05\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 7.089097867719829e-05\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 7.064306410029531e-05\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 7.086806726874784e-05\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 7.058456831146031e-05\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 7.087666541337967e-05\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 7.057468726998195e-05\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 7.091671519447117e-05\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 7.06010896828957e-05\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 7.105778116965666e-05\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 7.060540840029716e-05\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 7.171347911935299e-05\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 7.144726958358661e-05\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 8.919723331928253e-05\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00012344974995357917\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00010630189935909585\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00010215296904789284\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.00010093816235894338\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00010044495982583612\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00010020477609941736\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00010004576412029564\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 9.993866318836808e-05\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 9.985167125705629e-05\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 9.97252477100119e-05\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 9.970606362912803e-05\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 9.949708968633786e-05\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 9.938020637491718e-05\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 9.943773533450439e-05\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 9.921893884893506e-05\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 9.904995822580531e-05\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 9.909932588925586e-05\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 9.897030569845811e-05\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 9.874291543383152e-05\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 9.864767227554694e-05\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 9.865298809017986e-05\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 9.84408866497688e-05\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 9.851449489360675e-05\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 9.829013724811375e-05\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 9.819322003750131e-05\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 9.814793826080859e-05\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 9.798663901165127e-05\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 9.7962629806716e-05\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 9.787733812117949e-05\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 9.786784066818654e-05\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 9.771070996066556e-05\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 9.764347807504236e-05\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 9.760029730387033e-05\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 9.750230412464589e-05\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 9.74319624947384e-05\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 9.741846035467461e-05\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 9.7330714634154e-05\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 9.726047108415514e-05\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 9.716879925690591e-05\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 9.726721735205502e-05\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 9.705183940241113e-05\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 9.699683723738418e-05\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 9.704427327960729e-05\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 9.688339196145535e-05\n",
      "Autoencoder4 training DONE... TOTAL TIME = 316.4993815422058\n",
      "start evaluation on test data for Autoencoder4\n",
      "MSE is 0.006194987049022919\n",
      "mutls per sample is 5242880\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder5\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.0006417272752150894\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.00037051161401905117\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.00022185061359778047\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.00016086808231193572\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.00013126923411618918\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.00012108081195037812\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00011805327550973744\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00011555551725905388\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.00011420504713896662\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.0001133313708123751\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.00011266468063695357\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00011187743075424805\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.00011121878895210103\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00011083182180300355\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00011019705387298018\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.00010983889311319217\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.00010955225938232616\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.0001094803644809872\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.00010911292309174315\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00010860406240681186\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00010836086759809405\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00010811181127792224\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00010787615872686728\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.00010784778016386553\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.00010731057263910771\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.0001072595696314238\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00010686452180380001\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.00010688607493648305\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00010644793888786808\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00010611761535983533\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.0001060917490394786\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00010582309652818367\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00010578752175206318\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00010559247952187434\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00010552615101914853\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00010546671459451318\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0001051848559291102\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00010502612218260765\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00010522259894059972\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00010491855442523957\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.0001047832693438977\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.00010469374916283414\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00010465595580171793\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.00010453944938490167\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00010469814238604158\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00010453074413817376\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00010456091811647639\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00010445816646097228\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.0001042836342821829\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00010417868441436439\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00010414825781481341\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.0001041017146781087\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00010404952423414216\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.00010398692451417446\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00010393408738309517\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00010389430390205235\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00010384983179392293\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00010377280501415954\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00010367567912908271\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00010360027605202049\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00010356326703913509\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00010363545472500845\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00010350783151807263\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00010344918991904706\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00010341856395825743\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.00010345659393351525\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.0001033416436985135\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.00010335443454096094\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00010341471468564123\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00010333077516406774\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00010328467848012223\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.0001031787320971489\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.0001031331784906797\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.00010313302336726338\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00010312956408597529\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.0001030716291279532\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00010301490605343134\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00010302689159289003\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00010305970761692151\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00010294086067005992\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00010288411809597164\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00010284073854563758\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00010280337097356096\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.0001027576535125263\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00010270557220792397\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.0001027071860153228\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00010279367241309956\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.00010266011609928682\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00010280766029609367\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00010271903098328038\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00010272504528984428\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.00010271384118823334\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00010273458086885511\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00010271162929711863\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00010274184489389882\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00010253316024318338\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00010250033752527087\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00010248639504425228\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00010253196000121533\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00010251761064864694\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00010237516544293612\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00010232625325443223\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00010235709749395027\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00010236624482786283\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00010241917509119958\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00010231819935142994\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00010229144420009106\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00010225816455204039\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00010223392688203603\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00010226735204923898\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00010226138314465061\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00010230669897282497\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00010249060433125124\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.00010221396165434271\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.0001021333719836548\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00010212003398919478\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.00010211800545221195\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00010209809843217955\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00010208508698269725\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.0001022028864827007\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00010205550788668915\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00010202075762208551\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.0001020275946939364\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.0001020140017499216\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00010202702862443402\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00010238471266347915\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00010228305240161717\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00010213428147835657\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.00010200716380495578\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00010200846329098567\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00010204307589447126\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00010202342557022348\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00010194237198447809\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00010193063644692302\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00010195089824264869\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00010192673740675672\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00010185552120674402\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.00010182011552387848\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00010179996548686177\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00010178203985560685\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00010177031275816261\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00010177437972743064\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00010194361151661723\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.00010179477045312524\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00010176985728321597\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00010182653582887724\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.0001018661807756871\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00010193563502980396\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00010185731633100659\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00010180146084167063\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00010179380391491578\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.000101819658302702\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00010186548170167952\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00010193469322985039\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.0001019724682555534\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00010172145703108981\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.00010166913794819266\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00010167045693378895\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00010167079424718395\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00010172461508773268\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00010166628198930994\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.00010162462276639417\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.00010161620535654947\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00010165924555622041\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00010163333470700309\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00010159868077607825\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00010157945682294667\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00010158171702641994\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00010159591882256791\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00010158637072890997\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00010169662040425465\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00010175735369557514\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00010174281051149592\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00010163753613596782\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00010153422219445929\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.00010153032577363775\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00010152755945455283\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.00010156474687391892\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00010157521464861929\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00010154345654882491\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.000101491937530227\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.00010150119778700173\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00010158658784348518\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00010161196725675836\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00010170220251893624\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.00010169636690989137\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.0001016031377366744\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00010158978897379711\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00010165651794523\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.0001016830131993629\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.00010161770012928173\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.00010156878037378193\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.0001015244604786858\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00010144908854272217\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00010141136939637363\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00010139387362869457\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00010140166006749495\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00010144234634935856\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00010142980754608288\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00010137638950254769\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00010138928395463154\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.0001014996279263869\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.0001014011719962582\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00010135514516150579\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00010133257223060354\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00010133000207133591\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00010144244501134381\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00010151440306799486\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00010144314961507917\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.0001013888165471144\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00010138337907847017\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00010134113312233239\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00010130640614079311\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00010128941823495552\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.0001012858989997767\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00010130644222954288\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00010131821822142229\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00010144556348677725\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.00010150002606678754\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00010144966450752691\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00010136014956515283\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00010138056590221823\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.0001013714008149691\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00010136562923435122\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00010137862205738202\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00010140100959688425\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00010140630416572094\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00010136289813090116\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00010132488736417145\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00010131267597898841\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.0001013122548465617\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00010127648973139003\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.0001012820127652958\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00010132838651770726\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00010132757219253108\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00010128927009645849\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.00010127984511200339\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00010135974356671795\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00010138401907170192\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00010132309136679396\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.0001012176158837974\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00010120143910171465\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00010119351296452805\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.0001011795224621892\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00010120391001692042\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00010122928419150413\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.00010129526519449428\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00010155045049032197\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.0001017648380366154\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.0001016931320191361\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.000101348333992064\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00010131655435543507\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00010136707860510796\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00010128730325959624\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00010122511768713594\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00010119806916918606\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00010118060308741405\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00010117247671587393\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00010116287128766999\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00010115233628312126\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.0001011398056289181\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00010112424119142816\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.0001011100978939794\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.00010109623399330303\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00010108442307682708\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00010107849549967796\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00010108191752806307\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00010108180082170293\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00010110826988238842\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00010114140808582305\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00010110688424902036\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00010114574688486755\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.0001012081844964996\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00010116786754224449\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00010109318827744573\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00010109699243912473\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00010109156952239573\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.0001010897514061071\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.00010110442177392542\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00010115136741660535\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.0001013805071124807\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.0001014165484230034\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.0001014429988572374\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00010144657135242596\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00010115148965269328\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00010113773721968755\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00010112189163919539\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00010111019044416025\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.00010110367904417217\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00010112454387126491\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00010123454121639952\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00010164894047193229\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00010127837624168023\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00010124761523911729\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00010129292320925742\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.00010116385790752247\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00010129192523891106\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00010146399639779702\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00010125547647476197\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00010109849274158477\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00010109141148859636\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00010106855654157699\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00010105037450557575\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00010103322740178555\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00010102090745931492\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.0001010053267236799\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.0001009957303176634\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00010099207429448143\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00010098414437379687\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00010098070866661146\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.0001009775942657143\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00010097415390191599\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.00010096461366629228\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00010096251615323127\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.00010095549834659322\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00010094948462210595\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00010094324446981773\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00010093519755173474\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00010092707903822883\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.0001009223744040355\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.00010091674572322518\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.00010091326374094933\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00010091156233102083\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.00010091499600093811\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00010091425676364452\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.0001009191627963446\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.00010092538257595152\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.0001009402287309058\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00010096800950122998\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00010104283457621932\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.0001010898340609856\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.00010108945745741949\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.00010095608013216406\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00010095877049025148\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00010093688470078633\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00010094811761518941\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00010097578313434497\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00010101856605615467\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00010118870268343017\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00010129202564712614\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00010100306593813002\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00010101975465659052\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00010101192281581462\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00010100891988258809\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.0001010300513007678\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.00010106378671480343\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00010104330576723441\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00010102976317284629\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00010134962940355763\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00010113892989465967\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00010098718892550096\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00010097891645273193\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.0001009810928371735\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00010099767736392096\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.000101012903614901\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00010101614287123085\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.0001009398530004546\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00010090313851833343\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.00010088805487612262\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00010088087350595742\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.00010087327071232721\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00010086513735586777\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00010085527261253446\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00010084815177833661\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00010084437119076028\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00010083937813760712\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00010083235072670504\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00010083516681334004\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00010083981062052771\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.0001008459966396913\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.0001008842428564094\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00010088842507684604\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00010081285465275869\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.00010081258165882901\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.0001008214199100621\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00010083921428304166\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.00010089957766467705\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00010088569251820445\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.0001008671458112076\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00010089651041198522\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.0001010295032756403\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00010099852865096182\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.0001009113303734921\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00010089059913298115\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00010086218710057437\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.00010084389272378758\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00010083787550684065\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00010084702196763829\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00010086396330734715\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00010085690388223157\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00010086094902362674\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00010082904016599059\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.0001008367066970095\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00010088175593409688\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.00010092536045704037\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00010096793266711757\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.00010105243301950396\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00010155712225241586\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00010155769705306738\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00010105886409291997\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.0001009887587861158\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.00010098153637954965\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00010093793505802751\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00010089129296829924\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00010085724585223943\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.0001008384011220187\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00010082834080094472\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.00010081812361022458\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00010081180196721107\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00010080581647343934\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00010080120613565668\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00010079832747578621\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00010079684405354783\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00010079988685902209\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00010079585248604417\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.0001007903937716037\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.00010078603721922264\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00010078163351863623\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00010078042047098279\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00010077956336317584\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00010077656683279202\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.00010077688988531008\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00010077398386783898\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00010077530925627797\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00010078175750095397\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00010078864317620174\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00010080470819957554\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00010084992827614769\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00010084105830173939\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.00010080432839458809\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00010079613042762503\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00010079520579893142\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00010079519677674397\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00010079930332722142\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.00010080301610287278\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00010083966539241374\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00010086232621688396\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00010086251044413075\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00010092414595419541\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00010104706132551655\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00010104643559316173\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00010098703089170158\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00010104028595378623\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00010092254407936707\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.0001008998349425383\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.00010090639989357442\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00010093641525600105\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00010097531165229157\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.0001009460975183174\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00010092527983943\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00010092486918438226\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.00010092010401422158\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00010094509867485613\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.00010091730364365503\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.00010082058608531952\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00010077966871904209\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00010077287355670705\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00010076582751935349\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.0001007590550580062\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00010075088212033734\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00010074319900013506\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00010073687648400665\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00010072855744510889\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.00010072412173030899\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.00010072149598272517\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.00010071720374980941\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00010071330529171973\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.00010070607415400445\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.00010070207645185292\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00010070183285279199\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00010071291791973635\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00010073430399643257\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00010076174599817023\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.00010077840357553214\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.0001007760813809\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.00010076436214148999\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.00010074265883304178\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.00010072715114802122\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00010071856901049614\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.00010071155615150929\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00010071545868413523\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00010072467004647479\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00010073680023197085\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00010075621481519193\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00010077806829940528\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00010073904995806515\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.0001007410604506731\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00010079300875077025\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00010088154522236437\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00010075361089548096\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.000100724320509471\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00010071261611301452\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.00010070125164929778\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.0001007026425213553\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00010073824960272759\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00010075889527797698\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.0001007761969231069\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00010075375234009698\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00010079259547637775\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00010084804729558528\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.00010077176179038361\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00010074178513605147\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00010074049088871107\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00010075250087538734\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00010078192834043875\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00010086313501233235\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00010090742085594684\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00010081787244416774\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.00010077380982693285\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.00010077926825033501\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.0001007720796042122\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.00010077729413751513\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00010079568659421056\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00010081773129059002\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00010080105887027457\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.0001007974284584634\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00010080321080749855\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.0001008316595107317\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00010088073060614988\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.00010098042665049434\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.00010099919250933454\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00010083978704642505\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.0001008632715092972\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00010088374314364046\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00010085297282785177\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.00010081396001623944\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00010083045403007418\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00010087022121297196\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00010096184530993923\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00010085945134051144\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00010082647611852735\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00010077388986246661\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00010075114201754331\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.0001007371986634098\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00010073090699734166\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.0001007255693548359\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00010071881610201672\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.00010071289318148047\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00010070927994092926\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00010071081516798585\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00010071181721286848\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.00010071749304188415\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00010072996810777113\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00010075473925098777\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00010080518783070147\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00010086416557896882\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00010091205942444503\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00010097230406245217\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.00010087871720315889\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.00010071713390061632\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.0001007289849803783\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00010069632175145671\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00010069350624689832\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00010069336160086095\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00010068818257423117\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.0001006834305007942\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00010067966853966936\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.0001006860678899102\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00010069632873637601\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00010070726886624471\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00010071763274027034\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.00010071774013340474\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.00010071403696201742\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00010070455115055665\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00010069511801702902\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00010068773495731875\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00010068907082313671\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00010068814794067293\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00010067134106066078\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00010067418799735605\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00010068358125863596\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00010070447868201882\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00010070684482343494\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00010076873935759068\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00010073054465465248\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.00010070425225421786\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00010070551245007664\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.0001007150937221013\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00010073074867250398\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.0001007333435700275\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00010069556854432449\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00010067783121485263\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.00010067421651910991\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00010067885101307183\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00010068811650853604\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00010070370160974562\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.00010071348893688991\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00010069979383843019\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.0001006820346810855\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00010066962597193196\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.00010066858114441856\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00010065136128105223\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.0001006627720198594\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.00010068809759104624\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00010074426769278944\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.00010069176933029667\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00010073351673781871\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00010074244113638997\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.0001006575147039257\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.0001006723364116624\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00010069478856166824\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00010077332350192591\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00010090701543958857\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.000100735402374994\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.00010062206420116126\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00010063454101327807\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.00010061443113954737\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00010061048262286931\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00010060806380352005\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00010059908381663263\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.00010059333581011743\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.0001005893643014133\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00010058337647933513\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00010058195242891088\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.00010057941311970353\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00010057679755846039\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00010057276056613774\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.00010057186329504475\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00010057006787974388\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00010057157254777849\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00010056909814011305\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00010056813101982698\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00010056213039206342\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00010056000261101871\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.00010055782506242395\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00010055655468022451\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.0001005536099546589\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00010055235616164282\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00010055434657260776\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00010056368540972471\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.0001005550828995183\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00010055430640932173\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00010055667720735073\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00010056640428956598\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00010058104409836232\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00010057798441266641\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00010058479383587838\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00010068505711387843\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00010069065203424543\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00010061041539302095\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00010063871828606352\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00010070261807413771\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00010082383290864527\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00010069473239127547\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00010063839814392849\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00010064226546091959\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00010066845483379439\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00010066291695693508\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00010064699221402406\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00010060867440188304\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.00010059224296128377\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00010058615385787562\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00010058228275738656\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00010058421961730347\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00010058872401714326\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00010059472551802174\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00010060740954941139\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.0001006247365148738\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00010068175644846633\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00010081448446726427\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00010082324297400191\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00010065655718790368\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00010063505906146019\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00010061679815407842\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00010060118976980448\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.0001005918363807723\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.0001005841774167493\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00010057998297270387\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.00010057579085696488\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00010057286621304229\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00010057068633614109\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.0001005670681479387\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.00010056938335765154\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.00010056305909529328\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00010056066064862535\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.00010056225873995572\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.00010056735016405582\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.0001005808066111058\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00010060252796392888\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00010059711901703849\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.0001005718859960325\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.00010056536237243563\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00010056681261630729\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00010056621074909344\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00010057208652142436\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.0001005746828741394\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00010058923915494233\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.00010058405867312103\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00010063921072287486\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00010066688992083073\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00010062046960229053\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.00010060021391836926\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00010059154214104637\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.00010058742016553879\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00010058845713501796\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00010059276828542352\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00010059640830149874\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00010062314628157765\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00010062167362775653\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.0001006426865933463\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.0001007592017413117\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00010081797692691907\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00010096741345478222\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00010078505816636607\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.00010086422902531922\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00010075243044411763\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00010081333544803784\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00010078326071379707\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00010070317337522283\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00010069552285131067\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00010071718657854945\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00010074480436742306\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.00010078315361170098\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00010080174979520962\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00010073831799672917\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00010070597578305751\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00010068690928164869\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00010068886011140421\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00010069250391097739\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00010069717362057418\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.0001006957798381336\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00010068586736451835\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00010067618859466165\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00010067248076666147\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.00010066915827337652\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00010066716349683702\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.00010066285380162298\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.00010065790265798569\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.00010065199428936466\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.0001006503906683065\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00010064137662993744\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00010063165362225846\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00010062347137136385\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00010062507208203897\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00010062372661195695\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00010062323621241375\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.0001006235505337827\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.0001006237481487915\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00010062352841487154\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00010062315850518644\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00010061973822303116\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00010061733395559713\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00010062521963845939\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00010064363945275545\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00010066092683700844\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00010067273688036948\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00010068129340652376\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00010070915363030508\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00010070694377645851\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.0001006076263729483\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00010058357584057376\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.00010058573709102347\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00010057302803033963\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00010056133673060686\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00010054795391624794\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00010053715144749731\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00010052635887404904\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.00010051675111753866\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00010051135875983164\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.00010050790064269677\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00010050402313936502\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00010050111508462578\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00010049665434053168\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00010049235017504543\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.00010049036762211472\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00010048861149698496\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00010048512514913454\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00010048308206023649\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00010048014228232205\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.00010047757183201611\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.0001004748820560053\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00010047260671854019\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00010047030809801072\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.00010046899493318051\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00010046738258097321\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.0001004653243580833\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.0001004641319741495\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00010046220559161157\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00010045990318758414\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00010046020150184632\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00010046237235656008\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00010046886251075194\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00010047785763163119\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00010047338728327304\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00010046990733826533\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00010047720425063745\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00010047496587503702\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00010048099240520969\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00010048851021565497\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00010049121978227049\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.00010050046839751303\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00010052400029962883\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00010050658776890487\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00010047168878372758\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00010046162846265361\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00010045799892395735\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00010045751405414194\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00010046065232018009\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00010046492010587827\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00010046861745649949\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00010047286254120991\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.00010047640185803175\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00010048429656308145\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00010048875003121794\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.0001004833186743781\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00010047456831671297\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00010049681324744597\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.00010049059346783906\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00010050709301140159\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00010054655489511787\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.00010065803391626105\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00010091240168549121\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00010065393289551138\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00010062068904517218\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00010062823130283505\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00010065168840810657\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00010066153452498839\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.00010069689713418484\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.00010065384674817323\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00010062864574138075\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00010061965906061233\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.0001006295409752056\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.00010065082809887827\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00010070370772155001\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.00010076942795421929\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00010076650127302856\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00010069159674458205\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00010067539173178375\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00010061335953650996\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00010055691294837744\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00010055278369691222\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00010055118938907981\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00010054516897071153\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00010053530189907178\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.000100524871959351\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.00010051586519693956\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00010050849232356995\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00010050271055661141\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.0001004979299614206\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00010049464443000033\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00010049256001366302\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00010048942698631435\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00010048735421150923\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.0001004845934221521\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00010048172815004364\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00010048106603790075\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00010047859366750344\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00010047469200799241\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.0001004719571210444\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.0001004678849130869\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00010046492709079757\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.00010046163137303666\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00010045898117823526\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00010045669303508475\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00010045476723462343\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.00010045227769296616\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.00010045039234682918\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00010044813272543252\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.00010044389200629667\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00010044145776191726\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00010043767921160906\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00010043612594017758\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00010043411544756964\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00010043101501651108\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00010042897774837911\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00010043150366982445\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00010043048387160524\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.00010042833164334298\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00010042656649602577\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00010042976209660992\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.00010043684567790479\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00010046129929833114\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00010048898024251684\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00010048324038507417\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00010047965362900868\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.00010049859643913805\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00010050347773358226\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00010046779090771451\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.00010043820249848067\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00010043386660981923\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00010043256625067443\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00010043212299933658\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00010043540998594836\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00010044074297184124\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00010044376365840435\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00010045803355751559\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00010046269831946119\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00010046185605460777\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00010046311683254317\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.0001004576261038892\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.00010046979645267129\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00010048764612292871\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00010049133794382215\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00010048245312646031\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00010047929012216628\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.00010048079799162224\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.0001005046020145528\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00010054801910882815\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00010045489179901778\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00010044464084785432\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00010044102557003498\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00010043521499028429\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00010043087182566523\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00010043039510492236\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.00010043174348538742\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.00010043410788057372\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00010043367015896365\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.00010043580463388935\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.0001004409440793097\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.00010044421826023609\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00010045155009720474\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.00010046595125459134\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.00010047383897472172\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00010048631520476192\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00010049022850580513\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00010049926175270229\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00010052315337816253\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00010053499892819672\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00010052192548755557\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00010051717603346333\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00010052298312075436\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00010053295962279662\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.0001005689988960512\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00010064676171168685\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00010060234926640987\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.0001005388848716393\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00010052915400592611\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00010052741592517123\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.00010052871861262248\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00010053843230707571\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00010055029444629327\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.0001005428351345472\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.00010051773395389319\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.0001004907488822937\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.00010047507355920971\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00010046086827060207\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00010045674891443923\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.00010044846771052107\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00010044596623629332\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00010044027760159225\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.0001004404405830428\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00010043828457128257\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.00010044028866104782\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00010044065391412004\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00010044116788776591\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00010044160095276311\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00010044559778179973\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.00010045592382084578\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00010045061295386403\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.0001004506356548518\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.0001004519101115875\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00010045536328107119\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00010046842915471643\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00010049781820271165\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.00010049281787360088\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00010048101132269948\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00010051036486402154\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00010059518535854295\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00010061999288154766\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.00010051551391370594\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00010047358053270727\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00010046185867395252\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00010045822942629456\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00010046586045064033\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00010048128926428035\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.0001004884362919256\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.00010047472373116762\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.00010044424212537706\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.0001004379292135127\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00010043796384707094\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00010043724527349696\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.0001004354321048595\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00010043327900348231\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.000100434132036753\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00010043768037576228\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00010044152877526357\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00010044483613455668\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00010044803144410253\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.00010045385162811726\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00010046171344583854\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00010047138581285253\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.00010048095311503857\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.00010049019241705537\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.0001005039844312705\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.0001005193017772399\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.00010053867154056207\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.00010053482285002246\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00010052778350654989\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00010051860037492588\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00010050338896689936\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00010048831318272277\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.00010047780786408111\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00010047234944067896\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00010046930721728131\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00010046837967820466\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00010047359392046928\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00010047626303276047\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.0001004824464325793\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.00010048555210232734\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00010048555443063378\n",
      "Autoencoder5 training DONE... TOTAL TIME = 216.9295301437378\n",
      "start evaluation on test data for Autoencoder5\n",
      "MSE is 0.002706441188166045\n",
      "mutls per sample is 2232384\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder6\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.0004779094643890858\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.00025996080890763553\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.0001357482705498114\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0001185135217383504\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.00011307812586892396\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.00011105150682851673\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00010956730809994042\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00010887488635489717\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.00010862526774872094\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.0001080914176418446\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.0001076119119534269\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.0001069197477772832\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.00010673053504433482\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00010611225559841841\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00010569435777142644\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.00010470285022165626\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.00010501258191652596\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00010394256678409874\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.00010367781476816162\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00010331345110898837\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00010309132660040631\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00010317842388758436\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00010262796422466636\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.0001023949365480803\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.00010213290341198444\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00010227318038232625\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00010196056304266676\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.00010203504731180146\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00010184393759118393\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00010193597583565861\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00010171831643674523\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00010177941236179322\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00010162730613956228\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00010173690418014303\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.000101532100525219\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00010131825838470832\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.00010157559387153015\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00010119358310475946\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00010118139063706621\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00010116903751622886\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00010094586090417579\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.00010103028529556468\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00010084467037813739\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.00010095214587636291\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00010069651441881434\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00010067922208691016\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00010059724736493081\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00010051472287159413\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00010053080593934282\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00010042496171081439\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00010086468275403604\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00010040350956842303\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00010033879254478962\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.00010051544406451285\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00010035931773018092\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00010036529274657369\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00010033682105131447\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00010050622775452211\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00010030178527813405\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00010033725033281371\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00010045114089734852\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00010078487888677045\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00010034033592091873\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00010025992465671151\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00010023924376582727\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.00010029241704614833\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00010025620169471949\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.000100310422712937\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00010029039374785498\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00010027263488154858\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00010024198243627325\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.00010042094567324966\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00010026728850789368\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.00010029771365225316\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00010041041416116059\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.00010021292953751981\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00010021776193752885\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00010035899846116081\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00010020252957474441\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00010037660918897017\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00010035259270807728\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00010018464497989043\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00010018985951319337\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00010020337183959783\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00010015770967584104\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.00010045325994724408\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00010026008443674073\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.00010023509414168074\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00010031531244749204\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00010014969011535868\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00010012264276156202\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.00010021807189332321\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00010012332582846283\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00010030977457063272\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00010042586014606058\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00010032522812252864\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00010012536338763312\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00010019374778494239\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00010012812999775633\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00010010010213591158\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00010008506447775289\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00010020846064435319\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00010016641434049234\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00010011726815719158\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00010027089156210423\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00010008812765590846\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00010024390969192609\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00010026919277152047\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00010017998691182584\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00010010239435359836\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00010017892171163112\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00010011310107074678\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00010007237870013341\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.00010013023391366005\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00010022531321737915\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00010008102544816211\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.00010021381167462096\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00010011891048634424\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00010007302509620786\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00010011179809225723\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00010006257769418881\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00010008095297962427\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.00010014344938099385\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00010046889074146747\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00010007535543991252\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00010007411066908389\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00010005137970438227\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00010010592930484563\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.00010010708880145102\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00010010422876803204\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.0001001392284524627\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00010003335017245264\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00010008056211518124\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00010005542630096898\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00010012259561335668\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00010006682277889923\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00010007167322328314\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.0001000438944902271\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00010054485173895955\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00010022656701039523\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00010004139534430578\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.0001000700198346749\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00010002824536059051\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 9.998949652072042e-05\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00010000163631048054\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.0001000003787339665\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00010007685574237257\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00010006193857407197\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 9.99890104867518e-05\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 9.999070141930133e-05\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.0001001575234113261\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00010002342605730519\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00010006911819800735\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00010003028117353097\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00010000930196838454\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00010004607203882187\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 9.999699948821217e-05\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 9.99880800372921e-05\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00010017240361776203\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 9.998536756029352e-05\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00010003638337366283\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 9.999470290495083e-05\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 9.996155567932874e-05\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00010008037905208767\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00010013419261667878\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 9.999075875384733e-05\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00010001000569900497\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00010000739421229809\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00010016022046329454\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00010002032009651884\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00010004068055422976\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00010009345947764814\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00010003605129895731\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00010001069429563359\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00010007759876316413\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.000100045086001046\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00010002673225244507\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 9.99886792851612e-05\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00010007781500462443\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.0001000342718907632\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 9.996792243327946e-05\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 9.997080895118415e-05\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00010003210802096874\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00010007100208895281\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 9.999184112530202e-05\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 9.992832754505798e-05\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 9.991684404667467e-05\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00010009390825871378\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00010002832656027749\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 9.993895597290247e-05\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 9.992519509978593e-05\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 9.99135460006073e-05\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 9.998851630371063e-05\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 9.998293448006735e-05\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 9.996955806855112e-05\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 9.996968292398378e-05\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 9.994143591029569e-05\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 9.991725208237768e-05\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00010024469200288877\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00010018906177720056\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 9.993152430979534e-05\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 9.99330377089791e-05\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 9.994364896556362e-05\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 9.994206426199526e-05\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 9.991916827857494e-05\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 9.993273997679352e-05\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 9.992045874241739e-05\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 9.989676473196596e-05\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 9.989631013013423e-05\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 9.997939603636042e-05\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00010002644703490659\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 9.991003753384575e-05\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 9.992649574996904e-05\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 9.98959012213163e-05\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 9.99356503598392e-05\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 9.988105623051524e-05\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00010000499285524711\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 9.994526160880923e-05\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 9.986262884922325e-05\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 9.99772100476548e-05\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 9.987272234866395e-05\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 9.987417899537832e-05\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 9.990996797569096e-05\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00010002708208048717\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 9.989248850615695e-05\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 9.988429461373016e-05\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 9.995112079195678e-05\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 9.990353137254716e-05\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 9.990657243179157e-05\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 9.98810277087614e-05\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 9.99190885340795e-05\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 9.992214210797102e-05\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 9.991503815399482e-05\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 9.9930124997627e-05\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 9.993527230108157e-05\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 9.98572472599335e-05\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 9.990112099330872e-05\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 9.99210905865766e-05\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 9.986768593080342e-05\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 9.999077912652864e-05\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 9.992832056013868e-05\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 9.987077326513827e-05\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 9.985795768443494e-05\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 9.985781594878063e-05\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 9.983300551539288e-05\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.0001001689795521088\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 9.987853321945295e-05\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 9.98521241126582e-05\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 9.985099546611309e-05\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 9.985679876990616e-05\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 9.989167447201907e-05\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 9.988905541831628e-05\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 9.987527126213536e-05\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 9.984435135265812e-05\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 9.981744718970731e-05\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 9.986650489736348e-05\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00010010557540226727\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 9.994845400797203e-05\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 9.984869655454531e-05\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 9.984417236410082e-05\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 9.994969761464745e-05\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 9.985158016206696e-05\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 9.98196069849655e-05\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 9.984399483073503e-05\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 9.98289292328991e-05\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 9.987581695895642e-05\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 9.983789699617773e-05\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 9.984583331970498e-05\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 9.982939751353115e-05\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 9.982348186895252e-05\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 9.981319570215419e-05\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 9.998262888984755e-05\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 9.992419742047786e-05\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 9.980527072912083e-05\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 9.982006828067825e-05\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 9.981345006963238e-05\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 9.981071489164605e-05\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 9.980587696190923e-05\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 9.985852346289904e-05\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 9.991514671128243e-05\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 9.98441397678107e-05\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00010004390001995489\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 9.985328710172325e-05\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 9.982263029087335e-05\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 9.982512157876045e-05\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 9.982803574530408e-05\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 9.984698641346768e-05\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 9.981753857573494e-05\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 9.982649353332818e-05\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 9.98266928945668e-05\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 9.980810224078595e-05\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 9.980480041122064e-05\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 9.978570014936849e-05\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 9.977945854188874e-05\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 9.981184470234439e-05\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 9.981115232221782e-05\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 9.980753209674731e-05\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 9.981648152461275e-05\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 9.98226783121936e-05\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 9.981069742934779e-05\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 9.977228939533233e-05\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 9.977311710827053e-05\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 9.982943622162566e-05\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 9.981395793147385e-05\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 9.980494593037293e-05\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 9.979172144085169e-05\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 9.986831777496263e-05\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 9.981020732084289e-05\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 9.981636161683127e-05\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 9.984742006054148e-05\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 9.986318473238498e-05\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 9.9842463969253e-05\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 9.985724173020572e-05\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 9.985445794882252e-05\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 9.986401797505095e-05\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 9.977711160900072e-05\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 9.978849702747538e-05\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00010000241512898355\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 9.983256371924654e-05\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 9.984895674278959e-05\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 9.982025483623147e-05\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 9.978757676435634e-05\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 9.977164823794737e-05\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 9.976373636163771e-05\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 9.975637862225994e-05\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 9.976841043680906e-05\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 9.979489928809926e-05\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 9.98179437010549e-05\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 9.978703659726307e-05\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 9.975508757634088e-05\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 9.975862718420103e-05\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 9.976087283575908e-05\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 9.974274318665266e-05\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 9.97394000296481e-05\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 9.987672179704531e-05\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 9.976544795790687e-05\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 9.974977147066965e-05\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 9.973868873203173e-05\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 9.973455627914517e-05\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 9.97217110125348e-05\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 9.978350659366698e-05\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 9.972536703571677e-05\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 9.971347142709419e-05\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 9.977773297578096e-05\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 9.973634732887148e-05\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 9.975027205655351e-05\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 9.974379820050672e-05\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 9.973665350116789e-05\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 9.977591573260724e-05\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 9.975699940696358e-05\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 9.975369903258979e-05\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 9.988653531763703e-05\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 9.97603015275672e-05\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 9.974467917345464e-05\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 9.974471555324272e-05\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 9.9740362202283e-05\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 9.972680971259251e-05\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 9.972354542696849e-05\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 9.987850149627775e-05\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 9.975338703952729e-05\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 9.977011010050773e-05\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 9.975994675187394e-05\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 9.975939348805696e-05\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 9.97402219218202e-05\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 9.9725546897389e-05\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 9.971214487450197e-05\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 9.970329236239195e-05\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 9.969483769964427e-05\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 9.972990810638293e-05\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 9.973224659916013e-05\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 9.97283379547298e-05\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 9.978461312130094e-05\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 9.972473868401722e-05\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 9.970464772777631e-05\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 9.973127045668661e-05\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 9.973115986213088e-05\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 9.973102074582129e-05\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 9.96911761467345e-05\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 9.970157116185873e-05\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 9.969303297111764e-05\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 9.973645297577605e-05\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 9.970260609406978e-05\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 9.971537976525724e-05\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 9.973559092031792e-05\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 9.969067439669744e-05\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 9.967637946829199e-05\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 9.96724289143458e-05\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 9.969216946046799e-05\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 9.96575885801576e-05\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 9.977640729630366e-05\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 9.968236117856577e-05\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 9.967734717065468e-05\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 9.96780491550453e-05\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 9.972654457669705e-05\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 9.971271530957893e-05\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 9.9757848365698e-05\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 9.972918662242591e-05\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 9.97236868715845e-05\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 9.9790517706424e-05\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 9.969782666303218e-05\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 9.967271849745884e-05\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 9.968165599275381e-05\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 9.967409248929471e-05\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 9.965777600882575e-05\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 9.967749065253884e-05\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 9.964457334717736e-05\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 9.966904297471047e-05\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 9.96581037179567e-05\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 9.964410914108157e-05\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 9.966525656636805e-05\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 9.963955526473e-05\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 9.964941826183349e-05\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 9.963373362552375e-05\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 9.963937045540661e-05\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 9.965773613657803e-05\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 9.97114714118652e-05\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 9.963301359675824e-05\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 9.964429395040498e-05\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 9.963840566342696e-05\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 9.964590019080788e-05\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 9.96526621747762e-05\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 9.962474839994684e-05\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 9.960687049897387e-05\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 9.964724915334955e-05\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 9.964292054064572e-05\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 9.965507197193802e-05\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 9.96461565955542e-05\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 9.960907365893945e-05\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 9.9602882983163e-05\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 9.962382959201933e-05\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 9.962323441868648e-05\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 9.971146821044385e-05\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 9.962375479517504e-05\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 9.959231392713264e-05\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 9.957809146726504e-05\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 9.957485046470538e-05\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 9.960392373614013e-05\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 9.965887293219566e-05\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 9.962998825358227e-05\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 9.958543494576589e-05\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 9.967338206479326e-05\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 9.96177495107986e-05\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 9.959617047570645e-05\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 9.957753878552467e-05\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 9.958602138794959e-05\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 9.957545960787684e-05\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 9.959981864085421e-05\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 9.96462645707652e-05\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 9.964041790226474e-05\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 9.959520190022886e-05\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 9.960027731722221e-05\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 9.959353774320334e-05\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 9.956901631085202e-05\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 9.967190562747419e-05\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 9.960745694115758e-05\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 9.959660936146974e-05\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 9.959668037481606e-05\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 9.960576222511009e-05\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 9.959767339751124e-05\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 9.959487739251927e-05\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 9.957058617146686e-05\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 9.95930636418052e-05\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 9.959180664736777e-05\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 9.967566438717768e-05\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 9.969866194296628e-05\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 9.957081900211051e-05\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 9.958664450095966e-05\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 9.957519563613459e-05\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 9.966036566765978e-05\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 9.961146541172639e-05\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 9.960108785890042e-05\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 9.959391463780776e-05\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 9.960556635633112e-05\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 9.963104530470445e-05\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 9.963555115973577e-05\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 9.960819850675762e-05\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 9.960297174984589e-05\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 9.963235614122823e-05\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 9.961910167476162e-05\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 9.957150934496894e-05\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 9.956982656149193e-05\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 9.957414702512324e-05\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 9.956516703823581e-05\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 9.95579591835849e-05\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 9.954802080756054e-05\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 9.95426214649342e-05\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 9.954036737326533e-05\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 9.953469038009643e-05\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 9.953297179890797e-05\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 9.952165826689452e-05\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 9.955469780834392e-05\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 9.953758912160993e-05\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 9.952037857146933e-05\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 9.952694235835225e-05\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 9.952537744538859e-05\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 9.953882574336603e-05\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 9.952542488463223e-05\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 9.953087515896186e-05\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 9.953570232028141e-05\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 9.955891058780253e-05\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 9.954449167707935e-05\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 9.954208304407076e-05\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 9.951713465852663e-05\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 9.950671199476346e-05\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 9.95298897032626e-05\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 9.950917738024145e-05\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 9.948530903784558e-05\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 9.947229205863551e-05\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 9.949037485057488e-05\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 9.948448336217552e-05\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 9.950645588105545e-05\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 9.955590008758009e-05\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 9.950500301783905e-05\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 9.950464183930308e-05\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 9.948687424184755e-05\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 9.948220133082942e-05\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 9.948372287908569e-05\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 9.951182961231098e-05\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 9.951177868060768e-05\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 9.962532669305802e-05\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 9.953512315405533e-05\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 9.9570098973345e-05\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 9.95374855119735e-05\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 9.956723486538977e-05\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 9.951858402928337e-05\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 9.95206626248546e-05\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 9.952443127986043e-05\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 9.951893182005733e-05\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 9.950796695193276e-05\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 9.950778854545206e-05\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 9.950749576091766e-05\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 9.950809355359525e-05\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 9.950445237336681e-05\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 9.948786144377664e-05\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 9.947062644641847e-05\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 9.946307371137664e-05\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 9.945288562448695e-05\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 9.94439100031741e-05\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 9.943927434505895e-05\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 9.9468577245716e-05\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 9.944760153302922e-05\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 9.943950339220464e-05\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 9.942873264662922e-05\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 9.948324790457264e-05\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 9.949838044121861e-05\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 9.945197554770857e-05\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 9.944727207766846e-05\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 9.944236517185345e-05\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 9.947776794433594e-05\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 9.944101853761822e-05\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 9.945752419298514e-05\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 9.94950724998489e-05\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 9.947875456418842e-05\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 9.947761718649417e-05\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 9.950436913641169e-05\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 9.948711580364034e-05\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 9.95229190448299e-05\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 9.95700279599987e-05\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 9.94966080179438e-05\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 9.950166073394939e-05\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 9.953644359484315e-05\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 9.95217775925994e-05\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 9.949586150469258e-05\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 9.948446560883894e-05\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 9.946019214112312e-05\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 9.944671968696639e-05\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 9.945976693416014e-05\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 9.94933128822595e-05\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 9.94481160887517e-05\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 9.945361263817176e-05\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 9.945842757588252e-05\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 9.948070568498224e-05\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 9.949781757313758e-05\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 9.945515310391784e-05\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 9.944260324118659e-05\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 9.944669407559559e-05\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 9.945848607458174e-05\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 9.945308149326592e-05\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 9.944999386789277e-05\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 9.944217075826601e-05\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 9.943647659383714e-05\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 9.943632670911029e-05\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 9.943346929503605e-05\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 9.94237489067018e-05\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 9.940246120095253e-05\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 9.941208292730153e-05\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 9.941935481037945e-05\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 9.943258482962847e-05\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 9.938987670466304e-05\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 9.941497351974249e-05\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 9.942436561686918e-05\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 9.940519143128768e-05\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 9.938267525285483e-05\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 9.938131464878098e-05\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 9.937191265635192e-05\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 9.942107310052961e-05\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 9.954317356459796e-05\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 9.94084111880511e-05\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 9.939617040799931e-05\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 9.937593480572105e-05\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 9.936928283423186e-05\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 9.93679478415288e-05\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 9.936610062140972e-05\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 9.936686750734224e-05\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 9.936670307070017e-05\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 9.936141985235736e-05\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 9.9403377098497e-05\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 9.940698451828211e-05\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 9.937078575603664e-05\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 9.935374284395948e-05\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 9.934700094163418e-05\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 9.934028086718172e-05\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 9.937926894053817e-05\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 9.936444577760994e-05\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 9.935942653100937e-05\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 9.93493729038164e-05\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 9.939124982338398e-05\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 9.936255723005161e-05\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 9.934868081472815e-05\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 9.933766297763213e-05\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 9.934325818903744e-05\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 9.936212911270558e-05\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 9.940873191226274e-05\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 9.937824710505083e-05\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 9.93709941394627e-05\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 9.946456120815128e-05\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 9.937292255926878e-05\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 9.936891990946605e-05\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 9.937709139194339e-05\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 9.936826420016587e-05\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 9.934777801390738e-05\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 9.935080975992605e-05\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 9.933638822985813e-05\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 9.934104920830578e-05\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 9.9341414461378e-05\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 9.935037174727767e-05\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 9.933350374922157e-05\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 9.932614979334176e-05\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 9.932647255482152e-05\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 9.932041488355026e-05\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 9.933747176546604e-05\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 9.930128784617409e-05\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 9.930283180437982e-05\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 9.929370105965063e-05\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 9.929270105203614e-05\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 9.938069386407733e-05\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 9.928914514603093e-05\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 9.928008046699687e-05\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 9.927109145792201e-05\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 9.926672326400876e-05\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 9.925866703270003e-05\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 9.925121645210311e-05\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 9.926345926942304e-05\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 9.926450031343847e-05\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 9.927063947543502e-05\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 9.932192886481062e-05\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 9.925801394274459e-05\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 9.926744911354035e-05\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 9.925374994054436e-05\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 9.926536149578169e-05\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 9.93312563514337e-05\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 9.928508923621848e-05\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 9.925925260176883e-05\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 9.926649712724611e-05\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 9.929419902618975e-05\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 9.928243176545947e-05\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 9.926886355970055e-05\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 9.92505915928632e-05\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 9.92429195321165e-05\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 9.924424288328738e-05\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 9.924154728651047e-05\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 9.927191567840054e-05\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 9.928182873409242e-05\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 9.92641827906482e-05\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 9.92960756411776e-05\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 9.928328887326643e-05\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 9.935774811310694e-05\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 9.930767264449969e-05\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 9.92945150937885e-05\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 9.927850303938613e-05\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 9.928084065904841e-05\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 9.929002553690224e-05\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 9.929776279022917e-05\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 9.929954161634669e-05\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 9.932159387972206e-05\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 9.930482628988102e-05\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 9.927557199262083e-05\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 9.925842314260081e-05\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 9.926790691679344e-05\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 9.929850493790582e-05\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 9.93135609314777e-05\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 9.930352709488943e-05\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 9.927938779583201e-05\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 9.927655570209026e-05\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 9.930716652888804e-05\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 9.927789127686993e-05\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 9.924359270371496e-05\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 9.925025049597025e-05\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 9.925441059749574e-05\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 9.925680264132097e-05\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 9.926173952408135e-05\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 9.924922458594665e-05\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 9.924456040607765e-05\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 9.926344093400985e-05\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 9.92737163323909e-05\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 9.925947204465047e-05\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 9.925440972438082e-05\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 9.926023543812335e-05\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 9.920054755639284e-05\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 9.919136355165392e-05\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 9.919633332174272e-05\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 9.918256371747702e-05\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 9.917141695041209e-05\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 9.917022893205285e-05\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 9.917199378833174e-05\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 9.917245362885297e-05\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 9.916936425724999e-05\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 9.916293289279566e-05\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 9.915417467709631e-05\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 9.914282476529479e-05\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 9.913028974551707e-05\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 9.911989443935454e-05\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 9.911129367537797e-05\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 9.910314111039042e-05\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 9.909647051244974e-05\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 9.909075568430126e-05\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 9.908568928949535e-05\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 9.909199987305328e-05\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 9.909598738886417e-05\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 9.909108688589185e-05\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 9.907099680276587e-05\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 9.906669467454776e-05\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 9.930104308295995e-05\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 9.910670516546816e-05\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 9.908268519211561e-05\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 9.906714898534119e-05\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 9.905749262543395e-05\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 9.911098662996664e-05\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 9.908173320582137e-05\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 9.906819788739085e-05\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 9.905699029332027e-05\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 9.910032735206186e-05\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 9.907992120133713e-05\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 9.904955251840874e-05\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 9.91432907176204e-05\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 9.907593397656455e-05\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 9.90681251278147e-05\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 9.905804297886789e-05\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 9.905471524689347e-05\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 9.911335015203803e-05\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 9.906640800181776e-05\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 9.914323658449575e-05\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 9.910020715324208e-05\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 9.905071725370362e-05\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 9.903951868182049e-05\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 9.90496136364527e-05\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 9.906036691972986e-05\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 9.905515151331201e-05\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 9.90706990705803e-05\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 9.903606318403036e-05\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 9.904341626679525e-05\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 9.90585345425643e-05\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 9.90458411979489e-05\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 9.903963597025722e-05\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 9.903060243232175e-05\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 9.906554478220642e-05\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 9.906756633426995e-05\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 9.90576233016327e-05\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 9.90539844497107e-05\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 9.903392172418535e-05\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 9.903065045364202e-05\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 9.902447869535536e-05\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 9.910175984259695e-05\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 9.902841615257785e-05\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 9.90122731309384e-05\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 9.901100362185389e-05\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 9.902220510412008e-05\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 9.900575299980119e-05\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 9.90062509663403e-05\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 9.902416029945017e-05\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 9.901507786707952e-05\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 9.901194862322882e-05\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 9.899877157295123e-05\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 9.903101075906307e-05\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 9.901420708047226e-05\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 9.899890050292015e-05\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 9.905442479066551e-05\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 9.90318032563664e-05\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 9.902866586344316e-05\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 9.903959900839254e-05\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 9.909585904097185e-05\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 9.909445274388418e-05\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 9.902590041747316e-05\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 9.902136895107106e-05\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 9.902175719616934e-05\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 9.901075449306518e-05\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 9.900733188260347e-05\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 9.902963531203568e-05\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 9.901935904053971e-05\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 9.900079981889576e-05\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 9.89946213667281e-05\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 9.898715215967968e-05\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 9.898706601234153e-05\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 9.898406045977026e-05\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 9.898698248434811e-05\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 9.899214375764132e-05\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 9.897780662868172e-05\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 9.897635347442701e-05\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 9.898840187815949e-05\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 9.896863659378141e-05\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 9.894677757984028e-05\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 9.894850547425448e-05\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 9.896620118524879e-05\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 9.897877316689119e-05\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 9.896535222651437e-05\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 9.897348907543346e-05\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 9.89942837622948e-05\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 9.901597804855556e-05\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 9.900346834911034e-05\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 9.901181125314907e-05\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 9.896791190840304e-05\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 9.897630981868133e-05\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 9.895192430121824e-05\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 9.89596953149885e-05\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 9.89658830803819e-05\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 9.895907336613164e-05\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 9.895668976241723e-05\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 9.89678577752784e-05\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 9.897602372802793e-05\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 9.892675996525213e-05\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 9.889360662782565e-05\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 9.88901118398644e-05\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 9.889630222460255e-05\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 9.889709268463775e-05\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 9.888822765788063e-05\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 9.888191591016948e-05\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 9.887723863357678e-05\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 9.88717653672211e-05\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 9.886354004265741e-05\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 9.88517072983086e-05\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 9.884139406494796e-05\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 9.883337916107848e-05\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 9.882855403702705e-05\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 9.882572805508972e-05\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 9.882329788524658e-05\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 9.884502622298897e-05\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 9.883886232273654e-05\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 9.885369683615864e-05\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 9.882677375571802e-05\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 9.880653378786519e-05\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 9.88015829352662e-05\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 9.879623423330485e-05\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 9.879015880869701e-05\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 9.878409968223422e-05\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 9.878361888695508e-05\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 9.879956836812198e-05\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 9.877618838800118e-05\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 9.877879434498026e-05\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 9.876531810732559e-05\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 9.879672783426941e-05\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 9.878738695988432e-05\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 9.87662115949206e-05\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 9.876820724457502e-05\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 9.878035489236936e-05\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 9.880472032818943e-05\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 9.876969357719645e-05\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 9.878518176265061e-05\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 9.877850476186723e-05\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 9.878405428025871e-05\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 9.877977601718157e-05\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 9.877825243165717e-05\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 9.878060751361773e-05\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 9.878684999421238e-05\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 9.878081851638854e-05\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 9.879053628537804e-05\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 9.879436402115971e-05\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 9.877382894046604e-05\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 9.8761425470002e-05\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 9.874123759800568e-05\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 9.873449511360377e-05\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 9.873620816506445e-05\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 9.872715163510293e-05\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 9.872092661680654e-05\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 9.872474503936246e-05\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 9.876482363324612e-05\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 9.873416711343452e-05\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 9.874940413283184e-05\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 9.874517476418987e-05\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 9.87692127819173e-05\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 9.873086091829463e-05\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 9.873159462586045e-05\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 9.879474557237699e-05\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 9.881180856609717e-05\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 9.879969293251634e-05\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 9.878388518700377e-05\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 9.877736389171332e-05\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 9.876179188722745e-05\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 9.877182863419875e-05\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 9.879907418508082e-05\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 9.880339930532501e-05\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 9.882865037070587e-05\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 9.878625831333921e-05\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 9.881257254164666e-05\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 9.878143289824948e-05\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 9.872999333310873e-05\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 9.874883340671659e-05\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 9.877690754365177e-05\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 9.875261428533123e-05\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 9.868904278846457e-05\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 9.868489549262449e-05\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 9.868953173281626e-05\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 9.868599998299033e-05\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 9.866873238934204e-05\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 9.866208798484876e-05\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 9.867031301837415e-05\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 9.867791173746809e-05\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 9.866923530353233e-05\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 9.867892833426595e-05\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 9.867360291536898e-05\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 9.870164358289912e-05\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 9.86813084455207e-05\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 9.868350898614153e-05\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 9.868082794127986e-05\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 9.866808308288455e-05\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 9.865344967693091e-05\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 9.864495572401211e-05\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 9.864952036878094e-05\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 9.864342398941518e-05\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 9.862187289400026e-05\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 9.860167658189312e-05\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 9.859086654614657e-05\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 9.858609933871776e-05\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 9.858165693003684e-05\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 9.857761964667589e-05\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 9.857586555881426e-05\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 9.856983437202871e-05\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 9.855607524514198e-05\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 9.854738047579303e-05\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 9.85474357730709e-05\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 9.85622260486707e-05\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 9.854414209257811e-05\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 9.853631927398964e-05\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 9.852846182184294e-05\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 9.852302493527531e-05\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 9.851466136751697e-05\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 9.856174321612343e-05\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 9.856181568466127e-05\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 9.853939758613705e-05\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 9.854309144429862e-05\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 9.852242073975504e-05\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 9.850469010416418e-05\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 9.850068483501672e-05\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 9.849863505223766e-05\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 9.849385329289361e-05\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 9.84915450681001e-05\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 9.84949289704673e-05\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 9.84994851751253e-05\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 9.850634669419378e-05\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 9.853364201262594e-05\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 9.85172763466835e-05\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 9.852185408817604e-05\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 9.85202071024105e-05\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 9.848021960351616e-05\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 9.848911024164409e-05\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 9.848113782936707e-05\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 9.845681430306285e-05\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 9.84437926672399e-05\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 9.843840205576271e-05\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 9.845459135249257e-05\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 9.845856664469465e-05\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 9.843603882472962e-05\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 9.845379943726584e-05\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 9.845664957538248e-05\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 9.846814587945118e-05\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 9.848043962847442e-05\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 9.849090332863852e-05\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 9.844832209637388e-05\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 9.845114516792818e-05\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 9.846653760178015e-05\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 9.848621761193499e-05\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 9.84512030845508e-05\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 9.845239692367614e-05\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 9.844819694990292e-05\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 9.844221145613119e-05\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 9.843637730227783e-05\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 9.843823441769928e-05\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 9.844062355114147e-05\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 9.843235660810024e-05\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 9.8435616819188e-05\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 9.84600160154514e-05\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 9.851113019976765e-05\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 9.854861680651083e-05\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 9.851566166616976e-05\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 9.850821050349623e-05\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 9.850995964370668e-05\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 9.85580071574077e-05\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 9.849776222836226e-05\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 9.84391791280359e-05\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 9.844637155765667e-05\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 9.843312349403277e-05\n",
      "Autoencoder6 training DONE... TOTAL TIME = 343.93302273750305\n",
      "start evaluation on test data for Autoencoder6\n",
      "MSE is 0.003281272717728379\n",
      "mutls per sample is 5570560\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder7\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.00048657362698577343\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.0004223278479184955\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.00042175174574367704\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.00042172087472863495\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.00042173006804659965\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.0004217510379385203\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00042175287730060516\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00042160032317042353\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.0003827172180172056\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.0003021731658373028\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.0002533217979362234\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00018346158671192825\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.00013848126283846795\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00012403920263750478\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00011826874135294928\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.0001158106976072304\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.00011443707131547854\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00011347481195116416\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.00011255112593062223\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00011219758162042126\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00011195833387318999\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00011148700519697741\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.0001112332651973702\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.00011087265593232588\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.00011145638301968575\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00011195193335879594\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.0001101266939076595\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.00010985613917000592\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00010965209396090358\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00010953525197692216\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00010943153261905536\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00010934533580439165\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00010930657386779785\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00010936213977402075\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00010959935054415836\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00010917418403550982\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.00010924234229605644\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00010914135520579294\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00010922795336227864\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.0001094906617072411\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00010969106515403836\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.00010920451022684574\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00010910057579167188\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.00010900001449044794\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00010896459716605022\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00010895739949773998\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.000108989012951497\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00010896799329202623\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00010893423954257742\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00010889549652347341\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00010898060485487803\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00010930395859759301\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00010907973104622215\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.0001094152420409955\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00010886192525504157\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00010882302070967853\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00010878487315494567\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00010874044441152365\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00010873145831283182\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00010875887324800715\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00010875591135118157\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00010870756959775462\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00010865170683246107\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00010860245703952386\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00010859777918085455\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.000109097691019997\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00010877463239012287\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.0001086804602527991\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00010862767056096345\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00010858383029699326\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00010859858186449855\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.00010861796094104648\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.0001085604855325073\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.00010852376988623291\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00010853346553631127\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.00010855636472115293\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00010867792851058766\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00010889370780205354\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00010908502765232696\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00010876297979848459\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00010847073630429805\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00010845264740055427\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00010844382457435131\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00010843641794053838\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00010843203926924616\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.00010843030584510416\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00010841968294698745\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.00010841616312973201\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00010840155475307256\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00010836061934242024\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00010835127293830738\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.00010839917231351138\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00010846295452211053\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00010844849952263757\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00010836785193532705\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00010842185351066291\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00010851153871044517\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00010858199006179348\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00010833402309799567\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00010827960126334801\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00010827657097252086\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00010827084945049137\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00010825808480149136\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00010824059165315703\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00010822406824445352\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00010821779113030062\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00010833486681804061\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00010825309087522328\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00010828203754499555\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00010836605302756652\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00010865730670047924\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00010865925636608154\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00010840461909538135\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.00010813896515173837\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00010813930770382285\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00010816754278494046\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.00010766730818431825\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00010737025993876159\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00010582759539829568\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.0001045965138473548\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00010394879034720362\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00010387358983280138\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.00010331912490073592\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00010307554301107302\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.0001028764879447408\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00010271799576003104\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00010261964838718996\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00010243208089377731\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.00010246972524328158\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00010225169593468309\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00010225464124232531\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00010221667878795415\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00010212799767032265\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00010199720622040331\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00010197709198109806\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00010242502234177665\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00010206831706454977\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.00010192762303631753\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00010186611238168553\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00010182324214838445\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00010176811512792483\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00010175277071539313\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00010175656789215281\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.00010174424649449065\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.0001020310417516157\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00010184623330133036\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00010173520218813792\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00010165282787056639\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00010162897524423897\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00010161276441067457\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00010158717719605193\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00010160055157030002\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00010156844829907641\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00010159067431231961\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00010159263358218596\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.0001016537329996936\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.00010151657392270863\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00010144536761799827\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00010141738806851208\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00010156521806493401\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00010168466658797115\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.00010151363996556028\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.00010154235176742077\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00010147961642360314\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00010137461067643017\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00010137837176444009\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00010136307508219034\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00010133701667655259\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00010132460854947567\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00010132930212421343\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00010136984376003966\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00010146516840904951\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00010136051336303354\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00010133289033547044\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00010135017771972343\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.00010130159498658031\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.000101295119675342\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.00010135512595297768\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00010151319613214582\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00010139117104699835\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00010132188501302153\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.00010120426013600081\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00010119065555045382\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00010118377540493383\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00010115455486811698\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.0001011219693464227\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.00010110222559887916\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00010110033879755065\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00010108259797561914\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00010108003509230912\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.0001010969141498208\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.00010118191421497613\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00010117388534126803\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00010109044145792723\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00010104767716256902\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00010101939202286303\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00010099844017531723\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00010104897140990943\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00010111573559697718\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00010103781329235062\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00010101357009261846\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00010105132387252524\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.0001011901893070899\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00010118069214513525\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00010143782274099066\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00010136583674466237\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00010118351347045972\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00010110517730936408\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00010110281786182896\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.00010110568633535877\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.0001010175672126934\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00010096431447891519\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00010095321253174917\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00010093781660543755\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.0001009183406131342\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00010090064810356126\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00010089917865116149\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00010099610779434442\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.00010086912516271695\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00010088323440868408\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00010089947958476842\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00010098051483510062\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00010101435356773435\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.0001010595032130368\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00010106278205057606\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00010094742145156488\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00010093279357533902\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00010090814001159743\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00010093133547343313\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00010097166523337364\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00010091378004290164\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00010081214742967858\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.0001007885322906077\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00010077598970383406\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00010076256119646133\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00010074906691443175\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.00010073604702483863\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00010072310047689825\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00010081800690386444\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00010077801125589758\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.00010081358137540519\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00010073114593978971\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00010071267402963712\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.00010069401527289301\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00010068012139527127\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00010083105356898159\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.00010071742202853784\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00010082512861117721\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00010095885139890016\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00010080723324790597\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.0001009775442071259\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00010120020393515005\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00010103964246809483\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00010099998617079109\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.0001007947835023515\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00010078885534312577\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00010077818325953557\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00010075464320834727\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00010071761906147003\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00010070064454339444\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00010069072508485988\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00010067707597045229\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00010066627175547182\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.00010065459471661598\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00010065044451039284\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00010063862544484436\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00010063194873509929\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00010062188608571888\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00010061223729280755\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00010060456901555881\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00010059650609036907\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00010058960178866982\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00010058906365884468\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00010058742976980284\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00010059350199298934\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00010059581254608929\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00010067634662846104\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00010070720803923904\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.00010068346949992701\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00010063738300232216\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.00010065513284644111\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00010068455274449662\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.00010083991073770449\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00010096699872519821\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00010071657743537799\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00010071036289446056\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00010071961354697123\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00010075398022308945\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.0001007096585817635\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00010064326284918935\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00010064289614092559\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00010066151502542197\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00010067332186736166\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00010068500763736665\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00010070898628327995\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.00010071119439089671\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.0001006826781667769\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00010065674141515046\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00010069687908980995\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.0001007954275701195\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00010083440516609698\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.0001007919036783278\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00010079594241688028\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00010091135249240324\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00010118636419065297\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.00010129966074600815\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00010130480804946274\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00010124892491148784\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00010078903345856815\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00010067998344311491\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00010065820388263091\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.0001006585918366909\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.00010063952737255022\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00010062666609883308\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.00010061851819045842\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00010060791973955929\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00010060391068691387\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.0001005932703264989\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00010059117426862941\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.0001005798825644888\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.00010057155130198225\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.00010056823637569323\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00010056305181933567\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.0001005581408389844\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00010055522347101941\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.00010055065533379093\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.00010054406186100096\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00010053871315903961\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00010053115081973374\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00010052416619146242\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00010051520715933294\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.00010050458338810131\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.0001004982736776583\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.0001004921004641801\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00010048500989796594\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00010048268362879754\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00010048610565718263\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00010050418641185387\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00010054137266706675\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.0001006118263467215\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00010075646976474672\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00010068212606711312\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00010060691827675328\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00010062412387924268\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00010066809481941163\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.00010066158400150016\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00010062135057523846\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00010062778630526737\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00010063719091704115\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00010060136439278722\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00010058975633000955\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00010058884246973321\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.0001005952138802968\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00010061430308269336\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.00010067177005112172\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00010080816631671042\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.00010106439585797489\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00010082839609822259\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.0001007834027404897\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00010068451811093838\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.0001005800033453852\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00010057095787487924\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00010056604602141305\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00010055795166408644\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00010054420185042546\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00010053686943138018\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00010053180099930614\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.0001005356211680919\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.0001005350291961804\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00010054262587800621\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00010054053651401772\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00010054217389551923\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00010053148551378399\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.0001005314980284311\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.00010052742785774172\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.0001005269555025734\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.00010050963901448995\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00010050033597508445\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.0001004955591633916\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.000100560957507696\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00010057641862658784\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.0001005710891331546\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.00010066846589324996\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00010054369049612432\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00010053300822619348\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.000100552189105656\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.0001005624170647934\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00010057589795906097\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00010056730679934845\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00010055443650344386\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00010054408572614193\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00010052996047306806\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.00010051434044726193\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00010050937125924975\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.0001004948970512487\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00010051101446151734\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.00010058263171231375\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00010046027426142246\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00010045271599665284\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00010053277161205187\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00010052639030618594\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.00010044816823210567\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00010045852046459913\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00010047328018117696\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00010052380501292646\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00010049284639535472\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00010049755510408432\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.0001005358248949051\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00010059847816592083\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00010052728437585756\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00010050073324237019\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00010050942888483406\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00010051288292743265\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00010051705350633711\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00010053714475361631\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.0001005673041800037\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.00010059086751425639\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00010058753774501383\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00010050967510323972\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.0001004651715629734\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.0001004374140757136\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.00010042517533292994\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00010041091532912105\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00010040160763310269\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00010039227578090504\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.0001003777579171583\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00010044682363513857\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00010042534791864455\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00010044249007478357\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.00010036837280495092\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00010038032807642594\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00010035550280008465\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00010035230545327067\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00010048945056041703\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.000100448036682792\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00010038618551334366\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00010038980137323961\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00010039714252343401\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00010041088564321398\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00010041578556410969\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00010039550455985591\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00010038711508968844\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00010038479027571156\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00010040033579571173\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.00010044656490208581\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.00010055560560431331\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00010076622653286905\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.0001007132389349863\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.00010074374731630086\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00010092452983371913\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.0001005721092224121\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.0001005439049913548\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00010048185824416577\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.00010043259942904114\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.00010045078000985086\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00010047633084468543\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00010050624317955226\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.000100541700667236\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00010062672605272382\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00010056155122583732\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00010044271417427808\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00010042937501566484\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00010042245092336089\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.00010041114175692201\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.00010040022025350481\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.00010040032007964328\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00010040267894510179\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.0001004095547250472\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.00010042579378932714\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00010044654860394075\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00010047449846751988\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.0001005103564239107\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00010052494617411867\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.00010052297118818388\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00010053475940367206\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.00010056006140075624\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.0001005404387251474\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.00010049412405351177\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00010050606448203325\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.0001006015040911734\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00010064613161375746\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00010053592413896694\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00010052362107671797\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00010049470467492937\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00010048341151559725\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00010047101008240134\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.0001004485454177484\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00010041288187494502\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00010039321728982032\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00010037990345153958\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.00010037408297648653\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00010036610969109461\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.0001003668352495879\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.00010035957820946351\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00010035805375082419\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00010035814106231556\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00010035579005489126\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.0001003566209692508\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00010035854036686942\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.000100359596835915\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.0001003595581278205\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00010035603365395218\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00010035643092123791\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00010035974933998659\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00010036292747827247\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00010036877007223666\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00010037362284492701\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00010037590254796669\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.00010037905216449872\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.0001003768341615796\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00010035446583060548\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.00010034413455286994\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00010033798549557105\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00010032754711573944\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00010031943267676979\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00010031503799837083\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00010042160371085628\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00010038576001534239\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00010032654477981851\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.00010031323850853368\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.00010030669829575345\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00010031382349552587\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00010035576560767368\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00010035500716185197\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00010031505633378401\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.00010032402176875621\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00010032521735411138\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00010033898201072588\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00010037535801529884\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00010046542913187295\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.0001003659566049464\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00010034137812908739\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00010033035301603377\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.0001003353192936629\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00010033715981990099\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00010034545906819404\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00010036121122539043\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.00010037449392257259\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00010037664265837521\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00010036800958914682\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00010035570448962972\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.00010037881089374424\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00010045814764453098\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00010064476751722395\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00010063172754598782\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00010042302717920393\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.0001003836834570393\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00010040680877864361\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.00010043198213679716\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.00010044964292319492\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00010041872010333464\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00010047270596260205\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00010053561243694275\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.000100425205018837\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00010042824607808142\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.0001004078786354512\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00010041169269243256\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.00010042465233709663\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00010044518625363707\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00010039480606792495\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00010040606226539239\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.00010046691575553268\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.00010049051721580326\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.0001003598008537665\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00010031097248429433\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00010031376732513308\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00010031557088950649\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00010031281592091545\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00010030709207057953\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00010029972589109093\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00010029794386355206\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00010029737488366664\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00010029311291873455\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00010029157390818\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00010028761898865924\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.0001002842903835699\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00010028084856458009\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.0001002836920088157\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00010028298303950578\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.0001002838759450242\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00010029251308878883\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00010031011974206195\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.00010034920473117381\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00010046431736554951\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00010037315107183531\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00010036920808488503\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.00010037339990958571\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00010032363148638979\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.00010029651602962985\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.0001002863486064598\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.00010028034128481522\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.0001002769052865915\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00010027400188846514\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.0001002662890823558\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00010026942531112582\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.00010026701958850026\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00010026291391113774\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00010026287316577509\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00010026676638517528\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00010027396667283028\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00010030838486272841\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00010034629027359187\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00010033441823907196\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.00010033330181613564\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.0001003201919957064\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00010033719649072736\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.00010033877420937642\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00010033596452558414\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00010036234452854842\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00010040491702966393\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.00010040463617770001\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.00010032866353867575\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00010030797449871897\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00010030189267126843\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.00010029681609012186\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00010029271361418068\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00010029164288425818\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.00010029192053480074\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00010028985881945118\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00010029665019828827\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00010029921657405793\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00010030149365775287\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00010030527540948242\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00010031299141701311\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.0001003776435391046\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.0001005084827193059\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.00010066318878671155\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00010037098254542798\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00010036752239102498\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.0001003968226723373\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00010049193224404007\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00010050387616502121\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00010040431836387142\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00010041522735264152\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00010043840884463862\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00010045688191894442\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00010047194373328239\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.0001004639221355319\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00010041914851171896\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.0001003177321399562\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00010029316908912733\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00010030026838649064\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00010030248435214162\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00010030154691776261\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00010030392557382584\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00010030533449025824\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00010030786244897172\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00010031550424173474\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00010032540332758799\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00010034115257440135\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.00010035995685029775\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00010038100561359897\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00010040683351689949\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00010045978327980265\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00010054866725113243\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00010056130297016353\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00010041697503766046\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00010034829087089747\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00010034834733232856\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00010034057486336678\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00010033538419520482\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00010033464262960478\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00010033297119662166\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00010033164668129757\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00010032792954007164\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00010031864221673458\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.000100309556000866\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00010030052886577324\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.0001002987613901496\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00010030155361164362\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00010030525852926075\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.00010031231300672516\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.00010032987484009936\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.00010036244115326554\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00010042554087704048\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.00010049036791315302\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.00010035327286459506\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00010036569292424247\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00010043467569630594\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00010059960855869576\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.00010056583734694868\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.00010036673047579825\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00010035178362159058\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00010034330363851041\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00010032596095697954\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00010030894918600097\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00010029697907157242\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.0001002863681060262\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00010028032615082339\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00010027234588051214\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00010026615957031026\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.00010026049247244373\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00010025565745308996\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.00010025049734394997\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00010024569201050326\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00010024001967394725\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00010023719194578007\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00010023223760072142\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00010022913804277778\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.0001002231115126051\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00010021653957664967\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00010020693734986707\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00010020013141911476\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00010019222128903493\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.0001001886755693704\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00010018663218943403\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00010018264350946992\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00010017839085776358\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00010017353692092001\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00010016466374509037\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00010016784653998911\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00010018302971730009\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.00010020564164733514\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00010024638409959152\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00010016807966167107\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00010016411717515438\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00010016429296229034\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00010016359185101465\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00010016098734922707\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00010016297630500049\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.00010016923828516155\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00010017758293543011\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.0001003508799476549\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00010017246851930395\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.00010015841253334657\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.0001001564908074215\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.00010016020911280066\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.0001001607850776054\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.00010015768493758515\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00010014952335041016\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00010014325234806166\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00010013684863224625\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00010014041763497516\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00010020390123827383\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.0001001500865095295\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00010017907596193254\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.0001001589244697243\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.00010017276596045121\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.0001001836682553403\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00010023470764281228\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00010028200136730448\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00010025747731560841\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00010028957593021914\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00010024310642620549\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00010016407090006396\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00010017337539466099\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00010016841610195115\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00010017023130785674\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00010017092135967687\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00010017941269325093\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.0001001824700506404\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.00010018942790338769\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00010020598187111318\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00010023596085375175\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00010027713055023924\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00010030783974798396\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00010024113318650052\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.00010020867106504739\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00010019863140769302\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.00010019178123911843\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00010018977802246809\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.0001001946409814991\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00010020264133345336\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00010020936926594004\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.0001002221368253231\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00010025789408246055\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00010025990050053225\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00010026393632870168\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00010026813892181963\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.00010027382435509935\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00010028048796812073\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00010027732991147786\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00010027770040323957\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.00010028276941739023\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00010029376047896222\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.00010032546939328313\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00010036466730525717\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00010040691529866308\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00010037713538622483\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.0001003357776789926\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00010032122110715136\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00010043825197499246\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00010062513989396393\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00010052658268250525\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00010067389055620879\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.0001008066619397141\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00010059944062959403\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00010037952859420329\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00010042782174423337\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00010056181548861787\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.0001004464563447982\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00010027997283032164\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00010028389631770551\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00010027729935245588\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00010025936469901353\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00010025347990449518\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00010024727089330554\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00010024170856922865\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00010023379232734441\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00010022523085353896\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.0001002173314918764\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.00010021020891144872\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00010020502959378064\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00010019588225986808\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.00010018935252446681\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.0001001843748963438\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00010017689113738015\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.00010017129417974502\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00010017001652158797\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.0001001680947956629\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.00010016525076935068\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.0001001653732964769\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00010016225423896686\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00010016236454248429\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00010015822655986994\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00010015783918788656\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00010015660838689655\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.00010015490755904466\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.00010015158040914685\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.0001001539544085972\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00010015239560743793\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00010015145264333114\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.0001001530172652565\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00010015430161729455\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.00010015914333052933\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00010016634216299281\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00010017770022386685\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00010018045810284093\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00010018702771048993\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00010018679575296118\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00010018667031545191\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00010018661821959541\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00010018732136813923\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.0001001847002771683\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00010018253000453115\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.00010018600703915582\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00010018360917456448\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00010017501976108179\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.0001001497995457612\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00010012740036472678\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00010011127043981105\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00010010132624302059\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00010009408841142431\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00010008739162003621\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00010008265060605481\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00010007908334955573\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00010007426608353853\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.0001000700265285559\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.0001000728813232854\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00010009618796175346\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00010010787198552862\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.0001001138411811553\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00010006376658566296\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00010006385244196281\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00010005763149820268\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.00010010225581936538\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.0001000765772187151\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00010011546284658834\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.00010010008583776652\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00010007996461354196\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00010007572273025289\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00010007415927248076\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00010007723205490037\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00010008043143898249\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00010012613201979548\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00010014322528149933\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00010019132314482704\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.0001001511182403192\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00010015459003625437\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00010017920692916959\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.00010016027343226597\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00010011019709054381\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00010007726261392236\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00010007168020820245\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00010007040837081149\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.00010007224365836009\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00010007486329413951\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00010007534292526543\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.00010009104327764362\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00010009676887420938\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00010010593803599476\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00010012493177782744\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00010008907469455153\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00010009933030232787\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00010010896221501753\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00010012868093326687\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00010018128319643438\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00010019239387474954\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00010009693971369416\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00010009949241066352\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.0001000990645843558\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.0001000959679367952\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00010009655088651925\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00010010002966737375\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00010009878315031528\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.00010010319325374439\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.0001001101967995055\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00010011296428274363\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00010012152168201283\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00010013225924922153\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00010013632039772347\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.0001001394135528244\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00010015153879066929\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.0001001611034735106\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.00010017471155151724\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.00010018680128268897\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00010019945009844378\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.00010023750917753204\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00010030277480836958\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.000100298035540618\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00010035451588919387\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.00010029605124145746\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.00010037620115326717\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00010044151043985039\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00010027004347648471\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00010026492062024772\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00010024274233728647\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00010024691553553566\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00010024436138337477\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00010024446761235595\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.0001002615224570036\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00010029708908405156\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.0001003572964691557\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00010030841687694192\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00010023369191912935\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00010025996627518907\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00010031504411017522\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00010034989099949598\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.00010033069469500333\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00010028142743976786\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00010025346098700538\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00010022878443123773\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.0001002006561611779\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00010018440429121256\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.00010017891123425216\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00010017654654802755\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00010018153872806579\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.0001001913147047162\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00010021103633334861\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00010024762857938185\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.00010029190132627264\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00010027562209870667\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.00010020683810580522\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00010016035317676141\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00010014884901465848\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00010014628467615694\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00010014752566348761\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.000100151909282431\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00010015587555244565\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00010016514512244612\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00010017626511398702\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00010019178705988451\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00010020979767432437\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00010023664770415053\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.00010027577984146774\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00010033925966126844\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00010037115658633411\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00010032862774096429\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00010024160117609426\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.0001002271968172863\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00010022858710726722\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00010020481742685661\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00010019288485636935\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00010017308028182015\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00010017513966886326\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00010017966123996303\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.0001001800675294362\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.00010016188636654988\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.0001001343221287243\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00010009924881160259\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00010007757577113807\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.00010006987111410126\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00010006561555201188\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00010006423544837162\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.000100060481345281\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00010005833290051669\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00010005513235228136\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00010005322605138644\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.00010005201940657571\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00010004956944612786\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00010004839510656894\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.00010004669893532991\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.0001000450470019132\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.00010004325886256993\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00010004420561017469\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.0001000411919085309\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.0001000659546116367\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00010008629004005343\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00010007010365370661\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00010004762647440657\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00010004516807384788\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.00010004944488173351\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00010005460324464366\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00010006532480474561\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00010006174095906318\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00010014286468503996\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.0001001180792809464\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.00010005083051510155\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.00010005315387388691\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00010004871030105277\n",
      "Autoencoder7 training DONE... TOTAL TIME = 358.4121401309967\n",
      "start evaluation on test data for Autoencoder7\n",
      "MSE is 0.0023279181052703983\n",
      "mutls per sample is 5392448\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder8\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.000483118153642863\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.00042231318540871145\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.0004217622254509479\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0004217028489802033\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.0003984941472299397\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.0003156652103643864\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00027182848192751406\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00019206483790185303\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.00013615075149573386\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.0001225677944603376\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.00011690606421325355\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00011265378881944343\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.00011061479861382395\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00010936166363535449\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00010807796963490545\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.00010758775722933933\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.00010667638649465516\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00010638565901899711\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.00010604888870147988\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00010582751943729818\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00010555453627603129\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00010469257773365826\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00010593852231977508\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.00010435636533657089\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.00010372499993536621\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00010328356700483709\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00010306527168722823\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.0001029368510353379\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00010263576958095655\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00010264928307151421\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00010232862405246124\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.0001023624453227967\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.0001021790731465444\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00010234549437882379\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00010198130970820785\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00010201997123658657\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.00010198084666626528\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00010194681730354205\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.000101889401266817\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00010184669139562174\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00010174676979659126\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.00010162851423956454\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00010167086205910892\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.00010153002833249047\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00010151555819902569\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00010188991000177338\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00010147091583348811\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00010151813417905941\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00010150407935725525\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00010146171174710617\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00010143445979338139\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00010151625523576514\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.0001014405209571123\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.00010145845997612924\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00010128273861482739\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00010123126325197518\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00010150263813557103\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.0001013094451627694\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00010136338212760165\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00010144925094209612\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00010156511678360402\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00010123775864485651\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.0001012156912474893\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00010131413175258786\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.0001013076605158858\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.00010118798847543076\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00010110195726156234\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.00010108268004842102\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00010105747351190076\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.0001010841823881492\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00010108268179465085\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.00010110674047609791\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00010124962806003168\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.00010100286803208292\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00010097196704009548\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.00010102853470016271\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00010097672726260499\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00010083712870255112\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00010092753451317549\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00010084857844049111\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00010083479224704206\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00010081022512167692\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00010077255748910829\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00010090428550029174\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.0001008960697799921\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.00010079404339194298\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00010073463898152113\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.00010066474991617725\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00010059578315122053\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00010061232896987348\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00010073608224047348\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.0001007094356464222\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00010066543705761433\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00010057108069304377\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00010059110267320648\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.0001004782784730196\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00010050953569589183\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00010045820265077055\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00010040975641459226\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.0001004146144259721\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00010038628446636722\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.0001003399965702556\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.0001003141823457554\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00010047471383586526\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00010040983412181958\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00010044203751021996\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00010039530781796201\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00010040225461125374\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00010038520704256371\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00010033848171588033\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00010031616897322237\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.0001002964231884107\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00010027146490756422\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.00010036939085694029\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00010032673046225682\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00010038283362518996\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.0001005024925689213\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00010053918638732285\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00010042319598142058\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00010034993611043319\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00010044969792943447\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00010058755666250363\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.0001006926313857548\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00010038283915491774\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00010032240999862551\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.000100283098872751\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00010024967574281618\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00010028461198089644\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.0001002565841190517\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00010022717993706465\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00010022507543908432\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00010024021728895605\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00010024559684097767\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00010024976974818855\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00010024773626355455\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00010023740120232105\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00010022014117566869\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.00010020110581535845\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00010018409899203107\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00010016155545599758\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00010013866616645828\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00010014470637543127\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00010010558267822489\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.00010010567260906101\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.0001001271017594263\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00010012312908656895\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00010029029537690804\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00010028943652287125\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00010014626255724579\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00010013311897637323\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00010050807584775612\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00010021061782026663\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00010014233965193852\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.0001001318241469562\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00010013528284616769\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00010014414903707802\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.00010015483072493225\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00010028322169091552\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00010021218069596216\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00010022955189924688\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00010028914461145178\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.00010038739070296287\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.00010034504026407376\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00010020226967753843\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00010017119551775977\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00010019266017479822\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00010025672760093585\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00010034147155238315\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.000100192544341553\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00010017804801464081\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00010026540141552687\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00010038365551736206\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00010029557772213593\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00010032815131125971\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00010033931292127818\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.0001001668136450462\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00010014800034696236\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.00010012975661084057\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00010010836296714843\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00010010769678046927\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.0001001776588964276\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.0001002003304893151\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00010013612481998279\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00010013263730797917\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.0001001113245729357\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.0001001066411845386\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.00010008572717197239\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00010054126352770253\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00010033054160885513\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00010029053402831777\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.00010031673475168645\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.0001005247759167105\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00010017127671744674\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00010012519109295681\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00010014100582338869\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00010011319449404254\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00010011123376898467\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00010010504280216992\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00010009194142185151\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00010007824865169823\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00010006596217863261\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00010005496500525623\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.00010004751587985084\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00010004485317040235\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00010004802054027096\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00010004170530010014\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.0001000380385085009\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00010004452167777344\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00010008846438722686\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.0001000826057861559\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00010010719939600677\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00010004802898038179\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00010004127165302634\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00010024845920270309\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00010016413754783571\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00010007137374486775\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00010006139491451904\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00010011159087298437\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.0001001617408473976\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00010018314293120057\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00010016442160122096\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00010007637029048055\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.0001003963380935602\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00010022185189882293\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00010013354331022129\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00010014114814111963\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00010013657651143148\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00010011547798058018\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.0001000706825288944\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00010002167371567339\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00010002347960835322\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00010003152419812977\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.00010003139643231406\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00010003352159401402\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00010003395203966647\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00010041232162620873\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.00010010508471168578\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00010004543844843283\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00010001769958762453\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.0001000040234066546\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.00010011481557739899\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00010007241740822792\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00010008340497734025\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.0001000651333015412\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00010002063965657725\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00010010938247432932\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.00010006956523284316\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00010019732580985874\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00010022028378443792\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00010017736058216542\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00010012628481490538\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00010010768921347334\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00010035730025265366\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00010013835737481713\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00010009824763983488\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00010005278076278046\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00010008741402998566\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00010005857999203726\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00010004889511037618\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00010009532823460177\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00010013786377385258\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00010007846023654565\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00010008222772739828\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.00010012632177677006\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00010018822678830474\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00010013232153141871\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00010010431957198307\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00010010710800997913\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00010010064666857942\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00010007121716625988\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00010004427400417626\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00010003754228819161\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00010003416886320338\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00010002733499277383\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00010002202936448156\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00010004023002693429\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00010013341961894185\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00010007680946728215\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.00010014734871219844\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00010020062007242813\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.0001001577713759616\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00010014496452640741\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.0001002009745570831\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00010019853012636303\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00010038667183835059\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00010020748624810948\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00010015676089096815\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.0001003580738324672\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.00010027668642578646\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00010008741868659854\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00010010056081227958\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00010011472972109914\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00010012723709223792\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00010015032283263281\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00010013820661697537\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.00010007353354012594\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00010004988260334358\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.0001000445956015028\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00010003442090237513\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00010002775292377919\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00010002480004914105\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00010002420662203803\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00010001961898524314\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00010001482354709878\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00010001761605963111\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.00010014368308475241\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00010004300769651308\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00010004772368120029\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00010003055649576709\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00010003591800341383\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00010003722418332473\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00010006420518038795\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.00010018789704190568\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00010004196898080408\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 9.999696776503697e-05\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 9.998886787798255e-05\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 9.99868562212214e-05\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 9.99761643470265e-05\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 9.99633633182384e-05\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00010002024500863627\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.0001000501576345414\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 9.998324268963188e-05\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 9.998600842664018e-05\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 9.997415036195889e-05\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 9.995398111641407e-05\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.00010013662511482835\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 9.998523397371173e-05\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00010007565550040453\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00010006295255152509\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00010001888818806038\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00010001249407650903\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.0001000482341623865\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.00010010526311816647\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00010004645038861781\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00010002022929256782\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00010003035044064745\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00010005884600104764\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00010009551711846142\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00010008959186961874\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00010004898736951872\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00010001299961004406\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00010000636364566163\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00010000137874158099\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 9.99964849324897e-05\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 9.999509202316404e-05\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 9.99888646765612e-05\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 9.998684894526378e-05\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 9.999045461881905e-05\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00010000656882766634\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00010002938855905086\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00010002752213040367\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00010005136631662026\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.00010003961477195845\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00010001955641200766\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.00010005527088651434\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.0001000675733666867\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.0001000555546488613\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00010007342789322137\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.00010003474104451015\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00010012462502345443\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.00010023428272688761\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00010021069872891531\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00010025922616478055\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00010012924816692248\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00010021389229223132\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00010053801641333848\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00010015451349318027\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00010009293269831687\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00010008867771830409\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.0001000912781455554\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.0001000958142685704\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00010009520745370537\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00010001415736041963\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 9.999360074289143e-05\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.00010000031004892663\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00010000235371990129\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.0001000077198841609\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00010001577873481438\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.00010002340422943234\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00010002904222346843\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00010002591705415397\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00010001029120758176\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 9.999217640142888e-05\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 9.998595109209419e-05\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 9.998608846217393e-05\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 9.998224297305569e-05\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 9.997839253628627e-05\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 9.997266053687781e-05\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 9.996904846047983e-05\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 9.996504551963881e-05\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 9.996451117331163e-05\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 9.996190172387287e-05\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 9.996042790589854e-05\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 9.99576979666017e-05\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 9.995465428801253e-05\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 9.9950892617926e-05\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 9.994705469580367e-05\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 9.994213382015004e-05\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 9.993840823881328e-05\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 9.993422019761056e-05\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 9.993112063966692e-05\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 9.992881765356287e-05\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 9.992418636102229e-05\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 9.99211531598121e-05\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 9.993514308007434e-05\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 9.996691951528192e-05\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 9.998433786677196e-05\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 9.997953718993813e-05\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00010001168411690741\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 9.99492488335818e-05\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 9.994343447033315e-05\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 9.994146006647497e-05\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 9.99640952795744e-05\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00010016600601375102\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.0001000317744910717\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 9.997761488193646e-05\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 9.9983167310711e-05\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 9.999381174566224e-05\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 9.99747883179225e-05\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 9.994691121391953e-05\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00010001427610404789\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 9.997605986427515e-05\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.0001000450603896752\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00010001875809393823\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 9.999682981288061e-05\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 9.997873712563887e-05\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00010000530281104148\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00010003926494391636\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00010005895601352676\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 9.9999142694287e-05\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00010000930749811232\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00010000767011661082\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 9.999813482863829e-05\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00010000553651480004\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.00010003267001593486\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00010006563243223355\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00010015795705839992\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.0001003454314195551\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00010015949985245243\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00010007567616412416\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00010003950970713049\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00010006136755691842\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00010011180653236806\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00010020297253504395\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.0001001351754530333\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.00010008456069044768\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00010007231263443828\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00010004607785958797\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.00010001204063883051\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 9.999626199714839e-05\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 9.999486734159291e-05\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 9.999707603128627e-05\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00010000006586778909\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.00010000665410188959\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.0001000239365384914\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00010005996358813718\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00010009279649239033\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00010003466333728283\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 9.99978554318659e-05\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 9.999389556469395e-05\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 9.998825058573857e-05\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 9.998566412832587e-05\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 9.998574532801285e-05\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 9.998847905080765e-05\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 9.999134112149477e-05\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 9.998115187045187e-05\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 9.996498323744163e-05\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 9.995473141316324e-05\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 9.994819789426402e-05\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 9.99413916724734e-05\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 9.993430285248905e-05\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 9.992870909627526e-05\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 9.992348961532116e-05\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 9.992117760702968e-05\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 9.991894097765907e-05\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 9.991730534238741e-05\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 9.991773375077173e-05\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 9.99248685548082e-05\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 9.993196174036712e-05\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 9.998736320994795e-05\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 9.992408187827096e-05\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 9.991290280595422e-05\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 9.9966561247129e-05\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 9.995668195188045e-05\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 9.992207225877791e-05\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 9.991545171942561e-05\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 9.991053520934657e-05\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00010005096526583657\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 9.995402884669601e-05\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 9.99215102638118e-05\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 9.990792197640985e-05\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 9.992792445700615e-05\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 9.996662702178582e-05\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 9.993443585699424e-05\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 9.994062798796222e-05\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 9.995684900786728e-05\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 9.99983210931532e-05\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00010001658927649259\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 9.99883865006268e-05\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 9.998566820286214e-05\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 9.99833771493286e-05\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00010003475181292742\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00010004428622778505\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00010001090326113627\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 9.997577493777499e-05\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 9.995897620683536e-05\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 9.994613414164633e-05\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 9.994470194214955e-05\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 9.995920700021089e-05\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.00010002444323617966\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00010002708993852139\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 9.998917172197252e-05\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00010000336857046932\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00010002812865423039\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00010005578806158156\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00010008613928221166\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00010007050033891573\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00010006866359617561\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 9.998735855333507e-05\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 9.999043919378892e-05\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 9.998420428019017e-05\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 9.998928435379639e-05\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00010002552560763434\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00010017230437370017\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00010004541953094303\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 9.999913309002295e-05\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 9.999004745623097e-05\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00010000354988733307\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.0001000356805161573\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.0001000999310053885\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00010004217736423016\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.0001000308242510073\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.0001000452067819424\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00010003140516346321\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00010000386828323826\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00010002716007875278\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00010003223665989935\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 9.998883499065414e-05\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 9.99660647357814e-05\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 9.996334119932726e-05\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 9.996595588745549e-05\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 9.997135523008182e-05\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 9.998269466450438e-05\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00010000315232900902\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00010003489267546684\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.000100054778449703\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 9.99994741869159e-05\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 9.996902517741545e-05\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 9.997033572290093e-05\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 9.997894027037546e-05\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 9.998764609917998e-05\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 9.997821558499709e-05\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 9.99779423000291e-05\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00010003236617194489\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00010003609844716266\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 9.9968530703336e-05\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 9.995859523769469e-05\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 9.993608458898962e-05\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 9.992964420234785e-05\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 9.992652776418254e-05\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 9.992257371777669e-05\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 9.991886094212532e-05\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 9.991501545300707e-05\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 9.991138998884708e-05\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 9.990818856749684e-05\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 9.990703372750431e-05\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 9.990824066335335e-05\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 9.991389670176432e-05\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 9.991791914217174e-05\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 9.991983650252223e-05\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00010001016402384266\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 9.992589708417655e-05\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 9.99171199509874e-05\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 9.990632330300286e-05\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 9.989935730118304e-05\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 9.989236103137955e-05\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 9.988990495912731e-05\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 9.995715488912538e-05\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 9.994477120926604e-05\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 9.989286394556984e-05\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 9.988762729335577e-05\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 9.988402394810692e-05\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 9.996372449677437e-05\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 9.989193291403354e-05\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 9.988284204155206e-05\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 9.989801474148407e-05\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 9.989756566938013e-05\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 9.9928748968523e-05\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 9.993176994612441e-05\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 9.989324957132339e-05\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 9.989083715481683e-05\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 9.989143960410729e-05\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 9.989315381972119e-05\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 9.990089165512472e-05\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 9.99175282777287e-05\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 9.996534761739895e-05\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 9.993297600885854e-05\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 9.988397912820802e-05\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 9.987925936002285e-05\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 9.987587574869394e-05\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 9.98736679321155e-05\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 9.987116180127487e-05\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 9.987590950913728e-05\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 9.987738536437974e-05\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 9.988498291932046e-05\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 9.986107383156196e-05\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 9.993974381359295e-05\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 9.987894532969222e-05\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 9.987069381168113e-05\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 9.98809322481975e-05\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 9.991612139856443e-05\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 9.992332925321535e-05\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 9.99308901373297e-05\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 9.991504135541618e-05\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.00010000073059927672\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 9.998418157920241e-05\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 9.98953144880943e-05\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 9.990222664782778e-05\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 9.991937113227323e-05\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 9.989595040678979e-05\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 9.988470614189282e-05\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 9.989008802222088e-05\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 9.991495520807803e-05\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 9.991376427933573e-05\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 9.991836763219908e-05\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 9.997354354709386e-05\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.00010008970682974905\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00010008033248595893\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00010001687536714598\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00010002578201238067\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00010001857532188297\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00010000655223848299\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00010001519491197542\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00010004155919887125\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00010003069561207666\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 9.992726088967174e-05\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 9.991265396820381e-05\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 9.992676466936246e-05\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 9.993502171710134e-05\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 9.993951680371538e-05\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 9.994965890655294e-05\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 9.99902913463302e-05\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00010007340082665906\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 9.995806467486546e-05\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 9.9954066099599e-05\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 9.9962878448423e-05\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 9.998110093874857e-05\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.0001000462711090222\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00010008588113123551\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 9.997477493016049e-05\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 9.994768275646493e-05\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 9.996827429858968e-05\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 9.9947611161042e-05\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 9.993237967137247e-05\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 9.992380772018806e-05\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 9.991792816435919e-05\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 9.991744678700343e-05\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 9.992097999202087e-05\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 9.993195388233289e-05\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 9.99556461465545e-05\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 9.999352972954512e-05\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 9.995276253903285e-05\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 9.993798739742488e-05\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 9.995952626923099e-05\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 9.99687920557335e-05\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 9.997364046284929e-05\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 9.997389715863392e-05\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 9.995253378292546e-05\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 9.994303312851117e-05\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 9.995250031352043e-05\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 9.995290572987869e-05\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 9.993655141443013e-05\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 9.99408881762065e-05\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 9.995348256779834e-05\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 9.99393657548353e-05\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 9.991109662223608e-05\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 9.990051737986505e-05\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 9.989908779971302e-05\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 9.989565965952352e-05\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 9.989071812015027e-05\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 9.988460136810318e-05\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 9.988051693653688e-05\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 9.987851284677162e-05\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 9.987995668780059e-05\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 9.988509031245484e-05\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 9.989503945689648e-05\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 9.990213700802997e-05\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 9.988958336180077e-05\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 9.987622441258281e-05\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 9.988545149099082e-05\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00010000175039749592\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 9.987839235691354e-05\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 9.987552475649863e-05\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 9.987849596654997e-05\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 9.987686877138913e-05\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 9.987413242924959e-05\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 9.987169556552544e-05\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 9.987166471546516e-05\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 9.986763470806181e-05\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 9.989364276407287e-05\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 9.989512123866007e-05\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 9.993051498895511e-05\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 9.991972154239193e-05\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 9.989222249714657e-05\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 9.9862695496995e-05\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 9.986125631257892e-05\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 9.986387362005189e-05\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 9.986168151954189e-05\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 9.985958313336596e-05\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 9.985754499211908e-05\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 9.985538723412901e-05\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 9.985227632569149e-05\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 9.985178534407169e-05\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 9.985140524804593e-05\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 9.984897711547092e-05\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 9.984810749301687e-05\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 9.984962904127314e-05\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 9.985526674427092e-05\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 9.987939760321752e-05\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 9.993521554861217e-05\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 9.992207167670131e-05\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 9.98630587127991e-05\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 9.987544384784997e-05\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 9.987836965592578e-05\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 9.989108832087368e-05\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 9.990611259127036e-05\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 9.988804958993569e-05\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 9.985821030568331e-05\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 9.98478225665167e-05\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 9.984852629713715e-05\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 9.985024778870866e-05\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 9.985158918425441e-05\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 9.985469019738957e-05\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 9.986880148062483e-05\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 9.988347359467298e-05\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 9.987186989746987e-05\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 9.985456097638234e-05\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 9.98564506880939e-05\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 9.986471966840327e-05\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 9.988368139602244e-05\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 9.991398226702586e-05\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 9.991941129555926e-05\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 9.987459023250267e-05\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 9.99004123150371e-05\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 9.986913180910052e-05\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 9.98472742503509e-05\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 9.984702046494931e-05\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 9.986333578126505e-05\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 9.985145967220887e-05\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 9.984594769775868e-05\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 9.990112215746194e-05\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 9.987146651837975e-05\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 9.989619749831036e-05\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 9.989890066208317e-05\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 9.987429744796827e-05\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 9.98508426710032e-05\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 9.986419754568488e-05\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 9.98677356983535e-05\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 9.985690616304055e-05\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 9.986015240428969e-05\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 9.987143857870251e-05\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 9.989325830247254e-05\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 9.991535684093833e-05\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 9.994372812798247e-05\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 9.991540107876063e-05\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 9.993903106078505e-05\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 9.986384073272347e-05\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 9.98790017911233e-05\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 9.988923120545224e-05\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 9.99273857451044e-05\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 9.999382746173069e-05\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00010004219599068165\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 9.997864370234311e-05\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.0001000009648851119\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00010006390511989594\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 9.999881003750489e-05\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 9.996873355703428e-05\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 9.999388101277872e-05\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 9.999636997235939e-05\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 9.995085478294641e-05\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 9.995526110287756e-05\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 9.998063906095922e-05\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 9.997976419981569e-05\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 9.994314197683707e-05\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 9.990488382754847e-05\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 9.989359125029296e-05\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 9.988792153308168e-05\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 9.988069417886436e-05\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 9.987657278543339e-05\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 9.987562836613506e-05\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 9.987538767745718e-05\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 9.987555269617587e-05\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 9.98747450648807e-05\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 9.987458790419623e-05\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 9.987385332351551e-05\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 9.987203637138009e-05\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 9.986896329792217e-05\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 9.986620949348435e-05\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 9.986221703002229e-05\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 9.985786338802427e-05\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 9.985347365727648e-05\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 9.984880831325427e-05\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 9.984430187614634e-05\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 9.983993833884597e-05\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 9.983583702705801e-05\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 9.983218187699094e-05\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 9.982888761442155e-05\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 9.98257304308936e-05\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 9.982284915167838e-05\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 9.982026589568705e-05\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 9.98176442226395e-05\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 9.981528128264472e-05\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 9.981124225305393e-05\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 9.980794129660353e-05\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 9.980420669307933e-05\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 9.980130009353161e-05\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 9.979900612961502e-05\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 9.979720402043312e-05\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 9.979578288039193e-05\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 9.979481343179942e-05\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 9.97932575410232e-05\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 9.980438539059832e-05\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 9.980163449654355e-05\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 9.990899008698761e-05\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 9.982089191908017e-05\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 9.981535229599104e-05\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 9.980946459108963e-05\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 9.980276721762494e-05\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 9.979750961065292e-05\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 9.979559836210683e-05\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 9.980222297599539e-05\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 9.979679831303656e-05\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 9.993846295401454e-05\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 9.982026473153383e-05\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 9.979870490496979e-05\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 9.979342896258458e-05\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 9.97895488399081e-05\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 9.978708112612367e-05\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 9.979164227843285e-05\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 9.97877964982763e-05\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 9.979654365452006e-05\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 9.99799428973347e-05\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 9.98321277438663e-05\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 9.980810922570527e-05\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 9.980107948649675e-05\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 9.979186550481245e-05\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 9.978916263207793e-05\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 9.978883725125342e-05\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 9.979122638469562e-05\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 9.979732538340614e-05\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 9.98173645348288e-05\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 9.983388939872384e-05\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00010003022413002327\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 9.984669886762277e-05\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 9.98534809332341e-05\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 9.987654280848801e-05\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 9.989728103391826e-05\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 9.984951379010454e-05\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 9.981313574826345e-05\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 9.981665352825076e-05\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 9.982391900848598e-05\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 9.981040348066017e-05\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 9.981242619687692e-05\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 9.981015289667994e-05\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 9.981038776459173e-05\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 9.980718808947131e-05\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 9.980436909245326e-05\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 9.980275295674801e-05\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 9.980038506910205e-05\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 9.979713533539325e-05\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 9.97947744326666e-05\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 9.9795647256542e-05\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 9.980198345147073e-05\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 9.980537608498707e-05\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 9.990764025133103e-05\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 9.982329385820776e-05\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 9.9819868337363e-05\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 9.987665747758e-05\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 9.983797353925183e-05\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 9.981931158108637e-05\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 9.981099661672488e-05\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 9.980761416954919e-05\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 9.979319496778771e-05\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 9.979820169974119e-05\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 9.98586366767995e-05\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 9.980541071854531e-05\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 9.980254515539854e-05\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 9.98006496229209e-05\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 9.97985282447189e-05\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 9.979643480619416e-05\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 9.979223541449756e-05\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 9.987172961700708e-05\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 9.985935961594806e-05\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 9.985651529859752e-05\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 9.98124189209193e-05\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 9.982184215914458e-05\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 9.984492877265438e-05\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 9.983662574086338e-05\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 9.983749070670456e-05\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 9.984398202504963e-05\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 9.98695447924547e-05\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 9.992570237955079e-05\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 9.996686450904235e-05\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 9.998663765145466e-05\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 9.990802209358662e-05\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 9.992565057473257e-05\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 9.987442084820942e-05\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 9.988327976316213e-05\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 9.988695121137425e-05\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 9.988309437176213e-05\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 9.986292396206409e-05\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 9.986074815969915e-05\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 9.985877113649622e-05\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 9.985036886064335e-05\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 9.984210628317669e-05\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 9.983593539800495e-05\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 9.983136696973815e-05\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 9.98281768988818e-05\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 9.982581192161887e-05\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 9.982397285057231e-05\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 9.982164483517409e-05\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 9.981969807995483e-05\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 9.98190077370964e-05\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 9.981862123822793e-05\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 9.981818351661786e-05\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 9.981856390368194e-05\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 9.981934999814257e-05\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 9.982132323784753e-05\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 9.982580289943144e-05\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 9.983704396290704e-05\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 9.987212746636942e-05\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00010000175330787897\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 9.991917380830273e-05\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00010001975751947612\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 9.989606915041804e-05\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 9.991056780563668e-05\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 9.995836298912764e-05\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 9.998607594752685e-05\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 9.998096240451559e-05\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00010000176436733454\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.0001000071706948802\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 9.990392019972205e-05\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 9.990060178097338e-05\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 9.990606136852875e-05\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 9.992593375500291e-05\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 9.99687414150685e-05\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 9.997658664360642e-05\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 9.987575263949111e-05\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 9.983348252717405e-05\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 9.984207892557606e-05\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 9.984395437641069e-05\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 9.983898664359003e-05\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 9.98350651934743e-05\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 9.983198542613536e-05\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 9.982984163798392e-05\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 9.982840419979766e-05\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 9.982722636777907e-05\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 9.982549148844555e-05\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 9.982440562453121e-05\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 9.98234623693861e-05\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 9.982363408198581e-05\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 9.982384130125865e-05\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 9.982607152778656e-05\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 9.983047057176009e-05\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 9.984200674807653e-05\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 9.987512195948512e-05\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 9.994362917495892e-05\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 9.995436354074627e-05\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 9.989624202717096e-05\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 9.985452838009224e-05\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 9.981242212234065e-05\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 9.980623755836859e-05\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 9.980609465856105e-05\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 9.980385773815214e-05\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 9.979974129237235e-05\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 9.979674476198852e-05\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 9.979398251743987e-05\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 9.979317605029791e-05\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 9.979734983062372e-05\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 9.978964284528047e-05\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 9.977963345590979e-05\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 9.97779268072918e-05\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 9.977209498174489e-05\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 9.97816154267639e-05\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 9.977573179639876e-05\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 9.97886384720914e-05\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 9.978981775930151e-05\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 9.97820810880512e-05\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 9.977112582419067e-05\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 9.976559987990186e-05\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 9.976319503039121e-05\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 9.97612503124401e-05\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 9.975889843190089e-05\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 9.975654713343829e-05\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 9.975340770324693e-05\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 9.975250955903902e-05\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 9.974903921829537e-05\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 9.974802378565073e-05\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 9.974767919629813e-05\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 9.974346961826086e-05\n",
      "Autoencoder8 training DONE... TOTAL TIME = 376.1660785675049\n",
      "start evaluation on test data for Autoencoder8\n",
      "MSE is 0.0013204038459268908\n",
      "mutls per sample is 5591040\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder9\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.0004791299451608211\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.0004223553277552128\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.0004217485606204718\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.00042171436245553196\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.0004217263113241643\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.0004217462649103254\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00042175683309324084\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.0004217591567430645\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.0004217686632182449\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.00042178898816928266\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.0004218018450774252\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00042180653777904807\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.0004218089545611292\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00042180947377346456\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.0004218111024238169\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.00042181653319858017\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.0004218191129621118\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00042181757860817017\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.0004218170139938593\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00042181947152130305\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00042182588949799535\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00042182450531981887\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00042182126082479954\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.00042182155419141055\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.0004218217520974576\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00042182207223959265\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00042182041448540986\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.0004218196915462613\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00042181965662166474\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.0004218189197126776\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.0004218181851319969\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00042181797209195793\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00042181634227745235\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00042181457625702025\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.0004218138544820249\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00042181237949989736\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0004218114376999438\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00042181297787465157\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.0004218131769448519\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.0004218119045253843\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00042180939577519895\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.00042180813616141675\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00042180653777904807\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.0004218050010968\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.0004218030069023371\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00042180200456641613\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.0004218007915187627\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00042179949465207754\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00042179849115200343\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.0004217996436636895\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.0004217982175759971\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.0004217951570171863\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.0004217942082323134\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.000421793187269941\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.0004217931453604251\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.0004217939870432019\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00042179128737188874\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00042179146548733116\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00042178988456726074\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00042178706498816607\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00042178755393251777\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00042178607545793054\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.0004217861162032932\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00042178800911642613\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00042178874486126005\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.0004217896389309317\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00042179097305051984\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.0004217879520729184\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.0004217829997651279\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00042177891242317856\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.0004217768635135144\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.0004217747400980443\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00042177451658062634\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.0004217729705851525\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.0004217736062128097\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.00042177289840765296\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.0004217711288947612\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.0004217714408878237\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.0004217706515919417\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.0004217681707814336\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.0004217678483109921\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00042176789487712084\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00042176616378128526\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00042176487972028553\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.0004217634629458189\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.0004217640741262585\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00042176396353170274\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.00042176315211690964\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.0004217621905263513\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00042176214745268223\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00042176047689281406\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.0004217605770099908\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.0004217606014572084\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00042176074348390105\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00042176154674962164\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00042176369577646256\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00042176407645456494\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.0004217648576013744\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.0004217688320204616\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00042177392868325114\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00042178161325864496\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.0004217801569029689\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.0004217629274353385\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.0004217520507518202\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.0004217508132569492\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.0004217514849733561\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.0004217547446023673\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00042175644426606596\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00042175630806013943\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00042175188777036964\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.0004217479727230966\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.0004217452090233564\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.0004217435314785689\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.0004217413568403572\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00042174160364083946\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00042174219968728723\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.0004217413160949945\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.0004217409947887063\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.0004217408341355622\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00042173945461399854\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.0004217394674196839\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.0004217391717247665\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.00042173938476480543\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.0004217393638100475\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00042173802852630616\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.0004217371391132474\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.0004217365593649447\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00042173615307547154\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.0004217366536613554\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.0004217415302991867\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.0004217369039542973\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.0004217359132599086\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00042173465713858606\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.0004217345698270947\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.0004217326093930751\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.000421731291571632\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.0004217315011192113\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.0004217312950640917\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.0004217315942514688\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00042173033114522695\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.0004217278386931866\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00042172753252089023\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.0004217264172621071\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.0004217259201686829\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00042172540444880725\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00042172607965767383\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.000421725797932595\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00042172388988547026\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00042172451154328885\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.0004217238537967205\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.0004217233171220869\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.0004217230319045484\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.0004217230447102338\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00042172177112661304\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.0004217227327171713\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.0004217210260685533\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.00042172046029008924\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00042172069079242646\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.0004217216873075813\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.0004217223369050771\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00042172240559011696\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.00042172238579951226\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.0004217210726346821\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.0004217195883393288\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00042171971173956993\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.0004217190761119127\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.0004217170749325305\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.0004217154998332262\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00042171626235358416\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00042171640787273644\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.0004217163589783013\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00042171691195107996\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.0004217170597985387\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.0004217162786517292\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00042172462563030424\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.00042171981069259344\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.0004217178025282919\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.0004217155941296369\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.0004217147862073034\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.0004217140947002918\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00042171385022811593\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.0004217136709485203\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00042171365465037524\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00042171314940787853\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00042171316570602355\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.0004217120388057083\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.0004217117337975651\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00042171129025518893\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.0004217118781525642\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00042171137523837386\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.00042171146487817167\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.000421711941016838\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.0004217118700034916\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00042171190725639465\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.0004217115696519613\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00042171142296865583\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.0004217114835046232\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.0004217126325238496\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.0004217126197181642\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.0004217118304222822\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.000421711839735508\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00042171215056441725\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.0004217126686125994\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00042171359062194826\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.000421714810654521\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.0004217158409301192\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.000421716736163944\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00042171508190222087\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00042171512730419634\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.0004217159270774573\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00042171917157247664\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00042174165486358106\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.0004217319458257407\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.000421718010911718\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00042171479784883556\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00042171793640591207\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.0004217194824013859\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00042171974782831965\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.00042172419955022634\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00042172318440862\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00042172311805188656\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00042172242188826204\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00042172931251116097\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.0004217294789850712\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.000421740987803787\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00042176622664555905\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00042176348622888324\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00042179791606031356\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.0004217994562350214\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00042172649293206635\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00042172166635282335\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00042171899462118746\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.0004217176581732929\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.0004217166139278561\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.0004217158444225788\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00042171497363597155\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.0004217143100686371\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.0004217137233354151\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.0004217133822385222\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00042171285254880784\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.0004217123205307871\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00042171178618445994\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00042171281995251775\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.000421712986426428\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00042171161738224327\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.0004217110481113195\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.0004217104380950332\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00042171004111878573\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.0004217096418142319\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00042171080713160336\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00042171059991233053\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.0004217095405329019\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00042170905973762276\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00042171125416643916\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00042171054054051637\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00042170890141278504\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.0004217083228286356\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.0004217076068744063\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.0004217072739265859\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00042170749395154417\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.0004217097966466099\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00042170939152128993\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.0004217073088511825\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.0004217063647229224\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.0004217058559879661\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00042170565808191896\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00042170541128143667\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00042170584201812745\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00042171008768491445\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00042170676635578275\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00042170383385382594\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00042170306551270187\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00042170312022790315\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00042170263244770465\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.0004217023006640375\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00042170207016170027\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00042170187924057246\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.0004217018058989197\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.0004217014508321881\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.0004217012319713831\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.0004217010922729969\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00042170059052295985\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.00042170027503743765\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00042170001310296354\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.0004216996661853045\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00042169945547357204\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.0004216991877183318\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00042169881286099555\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.0004216983960941434\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00042169781518168746\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.0004216975509189069\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00042169728665612636\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.0004216969909612089\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00042169669992290435\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00042169641703367235\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.0004216961155179888\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00042169583379290997\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00042169560212641956\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952726710588\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00042169498046860097\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469757936896\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946486849338\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461958110335\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00042169455788098275\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00042169448337517676\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.0004216943518258631\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00042169435415416955\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00042169422027654946\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.0004216942668426782\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00042169425869360567\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00042169423890300097\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00042169411899521946\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.0004216942109633237\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.0004216942726634443\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.0004216942982748151\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00042169432272203266\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.0004216943448409438\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00042169436579570174\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.0004216943948995322\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.000421694420510903\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.00042169444728642704\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.0004216944694053382\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00042169448686763645\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.0004216945159714669\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00042169453809037805\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.0004216945555526763\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.0004216945788357407\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.0004216945986263454\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946219094098\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463471509517\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946579981595\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467778876426\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469874352215\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947162058204\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947359964252\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00042169475462287663\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947720851749\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00042169479303993284\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948105022311\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.000421694825636223\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948465909809\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.000421694852411747\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00042169487569481133\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00042169489432126284\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00042169490945525467\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.00042169492226094005\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00042169493972323837\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00042169495369307695\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00042169495951384307\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949793044478\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00042169499094597997\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00042169500375166535\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.00042169501655735074\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00042169502703472974\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950375121087\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00042169504682533444\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950561385602\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950654517859\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950759291649\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950887348503\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.0004216951003763825\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00042169510503299536\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00042169511783868074\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00042169512598775325\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.000421695135300979\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.0004216951399575919\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00042169515043497083\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.0004216951539274305\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00042169516440480946\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00042169516906142234\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00042169518419541416\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.0004216951900161803\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.0004216951981652528\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952016577125\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952051501721\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00042169521912001074\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952249407768\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00042169522726908325\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952377464622\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952493879944\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952552087605\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.00042169526568613946\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952691785991\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952726710588\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527034275234\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528198428454\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952912975103\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952912975103\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952947899699\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952994465828\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529828242955\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952994465828\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952994465828\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530293904243\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531341642143\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.000421695311088115\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953122522682\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953122522682\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530875980855\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.000421695311088115\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531574472787\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953180730343\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532738626\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532738626\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532738626\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532738626\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532855041326\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532855041326\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953308787197\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953308787197\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533320702613\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953308787197\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953308787197\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533320702613\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533669948576\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533669948576\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533669948576\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953448485583\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534135609864\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534135609864\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953460127115\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534834101796\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953495051712\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535649009047\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535649009047\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535998255015\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953576542437\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953588183969\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536114670334\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953588183969\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536114670334\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953658033162\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536813162266\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536813162266\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953704599291\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536813162266\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953658033162\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537278823554\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536696746947\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953692957759\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536696746947\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537278823554\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953704599291\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953704599291\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953704599291\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953692957759\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536813162266\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536463916303\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953576542437\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535649009047\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953588183969\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953576542437\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535299763083\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.000421695354161784\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953506693244\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534834101796\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953460127115\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953448485583\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953436844051\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534019194545\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533669948576\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533320702613\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531574472787\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531574472787\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531574472787\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.000421695311088115\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531341642143\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531341642143\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.000421695311088115\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953180730343\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531341642143\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531574472787\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531341642143\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531574472787\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953180730343\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953180730343\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953308787197\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953425202519\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534834101796\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535299763083\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953553259373\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953576542437\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535649009047\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953634750098\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535998255015\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537278823554\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537278823554\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537162408234\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953739523888\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953762806952\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953774448484\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953774448484\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.000421695375116542\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953809373081\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953832656145\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537977315485\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537278823554\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00042169537162408234\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536813162266\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953739523888\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953704599291\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536813162266\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953658033162\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00042169536114670334\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535998255015\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953634750098\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953634750098\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535998255015\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535998255015\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953634750098\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535998255015\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535998255015\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.000421695354161784\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953576542437\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953576542437\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535649009047\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953553259373\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.000421695354161784\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535183347764\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00042169535183347764\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953495051712\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953506693244\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953495051712\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534834101796\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953495051712\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953460127115\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534717686476\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534834101796\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953460127115\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953436844051\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953448485583\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953436844051\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953460127115\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953460127115\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953448485583\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953448485583\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534135609864\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953436844051\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953448485583\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534834101796\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953425202519\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953436844051\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534019194545\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534135609864\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534019194545\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534019194545\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534019194545\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533669948576\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533669948576\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.000421695337863639\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534019194545\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.00042169534019194545\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953390277922\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533669948576\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953308787197\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533553533257\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953343711793\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953308787197\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533320702613\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532855041326\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532855041326\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532738626\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953262221068\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00042169533320702613\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953320428729\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532971456645\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532855041326\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532738626\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532505795357\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953238938004\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532156549394\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953180730343\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953227296472\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00042169532040134075\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953192371875\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953180730343\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953180730343\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531690888106\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953122522682\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530992396175\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953122522682\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953145805746\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530992396175\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530875980855\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530992396175\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530992396175\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530992396175\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530992396175\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00042169531341642143\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.000421695311088115\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.000421695311088115\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530875980855\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953075956553\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.000421695311088115\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530875980855\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530875980855\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953064315021\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953075956553\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953064315021\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530875980855\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530875980855\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953064315021\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530526734887\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530293904243\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530526734887\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953064315021\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530526734887\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.0004216953041031957\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530293904243\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530293904243\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530293904243\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530293904243\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529828242955\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00042169530177488924\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529828242955\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529711827636\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529828242955\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952924616635\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529711827636\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952947899699\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529711827636\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.000421695300610736\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952994465828\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529711827636\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952947899699\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529711827636\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529362581673\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952924616635\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529711827636\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529362581673\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952959541231\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529362581673\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529362581673\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952924616635\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952912975103\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529013335704\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529362581673\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528896920385\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529013335704\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952912975103\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528896920385\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529013335704\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528896920385\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529013335704\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.00042169529013335704\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952912975103\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528896920385\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528896920385\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528314843773\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.000421695284312591\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528896920385\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952878050506\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952866408974\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.000421695284312591\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528547674417\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.000421695284312591\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952808201313\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528314843773\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528314843773\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528198428454\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528198428454\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528314843773\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528198428454\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952796559781\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952796559781\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952796559781\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952796559781\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952808201313\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952796559781\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952808201313\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00042169528198428454\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952726710588\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952808201313\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952796559781\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952808201313\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952796559781\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527849182485\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527383521197\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952761635184\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952726710588\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952726710588\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527383521197\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952726710588\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952749993652\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527383521197\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527383521197\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527034275234\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527034275234\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527150690553\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527732767166\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527383521197\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527383521197\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952691785991\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00042169527150690553\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.00042169526685029266\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952691785991\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00042169526452198627\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.000421695263357833\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.00042169526219367983\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952610295266\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952610295266\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00042169526219367983\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952610295266\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952610295266\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00042169526452198627\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525870122015\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952563729137\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525870122015\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952552087605\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525404460727\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952563729137\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525753706696\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525404460727\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952552087605\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952528804541\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952528804541\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952493879944\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952493879944\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952528804541\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00042169525171630083\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952493879944\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524705968795\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524705968795\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952447313815\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952493879944\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952447313815\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524705968795\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524705968795\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524705968795\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524589553476\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952482238412\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952447313815\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524007476864\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952435672283\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952447313815\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952412389219\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952435672283\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952424030751\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524007476864\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952424030751\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952435672283\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952412389219\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952447313815\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952424030751\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952435672283\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952424030751\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00042169523891061545\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.000421695236582309\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952377464622\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00042169523891061545\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.00042169524007476864\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.0004216952377464622\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00042169523891061545\n",
      "Autoencoder9 training DONE... TOTAL TIME = 379.7125587463379\n",
      "start evaluation on test data for Autoencoder9\n",
      "MSE is 0.6575605083718555\n",
      "mutls per sample is 5395744\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder10\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.00047675117501057686\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.0004223329818341881\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.00042177829775027933\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0004217111587058753\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.0004217112495098263\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.00042171833338215946\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.0004217285488266498\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00042173899244517086\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.0004217540379613638\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.0004217697086278349\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.0004217825876548886\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00042179328273050487\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.0004218002315610647\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.0004218025109730661\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.0004218026634771377\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.0004218044446315616\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.0004218087752815336\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.0004218131781090051\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.0004218164214398712\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.0004218199232127517\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.0004218231956474483\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00042182465083897116\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00042182508506812157\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.000421825828962028\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.0004218302993103862\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00042183401063084603\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00042183520854450763\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.0004218345554545522\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.000421833231812343\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00042183244368061424\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00042183255776762963\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.0004218321235384792\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00042183218523859976\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.0004218319698702544\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00042183140525594356\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.0004218312038574368\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0004218313028104603\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00042182992561720314\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.0004218281898647547\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.0004218265053350478\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.0004218258871696889\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.0004218251816928387\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00042182438541203736\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.0004218240245245397\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.0004218224994838238\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.0004218204168137163\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.0004218185809440911\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00042181676253676413\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.0004218156135175377\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00042181479162536564\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00042181434575468303\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00042181349243037403\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00042181201511994006\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.0004218107310589403\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00042180998250842096\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00042180768912658095\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00042180485790595414\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00042180240387097\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00042180114658549425\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.0004218004853464663\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.0004218003246933222\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00042179891490377484\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.0004217962885741144\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00042179451207630337\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.0004217931628227234\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.0004217920161318034\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.000421790461987257\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.0004217913083266467\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00042179340380243957\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00042179011390544477\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00042178557254374027\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.0004217829089611769\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.000421781976474449\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.00042178147356025875\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.0004217805131338537\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.00042177915573120116\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00042177810915745796\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00042177619063295426\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00042177458526566626\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.0004217733652330935\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00042177228489890693\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00042177137918770314\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00042176991002634167\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00042176927556283775\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.0004217681591399014\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.0004217679123394191\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.0004217674268875271\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.0004217660834547132\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.0004217652033548802\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.0004217634291853756\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00042176207061856986\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.0004217624105513096\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00042176277260296045\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00042176424991339443\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.0004217669926583767\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00042177532566711306\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00042177499737590553\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.000421760322060436\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00042175407987087965\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.0004217522859107703\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00042175165261141954\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00042175218812189996\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.0004217534861527383\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00042175099719315767\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.0004217488737776876\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00042174683418124917\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00042174584465101364\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00042174505768343806\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.0004217447410337627\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00042174422065727413\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00042174297384917737\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.0004217420844361186\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00042174113448709247\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.0004217406152747571\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00042174001806415617\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.0004217393568251282\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.0004217399575281888\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.0004217407095711678\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.0004217370960395783\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00042173614958301186\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.0004217367572709918\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00042173593887127935\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.0004217375465668738\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00042173851164989173\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.0004217364138457924\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00042173395049758253\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00042173203430138527\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.0004217318573500961\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.00042173203080892564\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00042173250927589833\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.0004217319737654179\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.0004217306582722813\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.0004217296408023685\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.0004217294440604746\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00042172931716777384\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.0004217302554752678\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00042172801331616936\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.0004217275837436318\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.0004217269876971841\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00042172645451501014\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00042172701680101457\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.0004217268712818623\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00042172648129053415\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.00042172724148258566\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.0004217300971504301\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00042172712041065096\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.0004217260400764644\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.000421730101807043\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.0004217240365687758\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.0004217215464450419\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00042172292713075875\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.0004217245371546596\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.0004217294417321682\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00042173149646259847\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.0004217300552409142\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.0004217245860490948\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.0004217217769473791\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.0004217219876591116\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00042172214249148966\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00042172315763309596\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00042172405403107405\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.0004217202262952924\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.0004217181680724025\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00042171780834905805\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00042171773500740526\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.0004217203555162996\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.0004217197140678763\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.0004217171133495867\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.0004217193159274757\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00042171996203251185\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.0004217190050985664\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00042171639623120425\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00042171549517661333\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.0004217195219825953\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.0004217201960273087\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.00042171320412307976\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00042170184897258876\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.0004001500050071627\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00032158374204300345\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00027781615615822373\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00018738383834715933\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.0001447531388839707\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.0001290869014337659\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00011989562190137804\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00011571203387575224\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.0001132572756614536\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.00011157742206705734\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00011202109511941671\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00011053045600419864\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00011001468024915084\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.0001096305571263656\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.00010925748967565595\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00010895521991187707\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00010866465396247804\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00010851065278984607\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00010827859514392913\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00010815700923558325\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00010807021666551009\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00010778095165733247\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00010759069409687072\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00010761120356619358\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00010765282553620636\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.00010749167850008235\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00010747269814601168\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00010742381709860638\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00010735715011833235\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00010729050496593118\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00010748279426479712\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00010735102667240426\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.00010720639635110274\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00010714668285800145\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.0001071095117367804\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00010707137698773295\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00010703046107664705\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00010705700027756393\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00010704423999413848\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00010711625654948875\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00010699815728003159\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.0001070395766873844\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00010701170191168785\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00010696214972995222\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00010694816301111132\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00010701018560212105\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00010696011187974363\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00010697100893594324\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.0001069066792842932\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00010691501724068075\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00010688344336813315\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00010686115332646295\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00010684966546250507\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00010683477099519222\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00010697021207306534\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.00010709977679653093\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00010705486318329349\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00010678407474188134\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00010684517998015509\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.00010681178740924224\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00010677890910301357\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00010678035585442557\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00010678922611987219\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.00010678530117729678\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00010676994599634782\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00010675586672732606\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.00010675306606572121\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00010674386576283723\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00010671994707081467\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.00010669067385606468\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00010667433380149305\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00010708689835155383\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00010703103529522196\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00010673664539353922\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00010671358875697479\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00010669528681319207\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00010668071103282272\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00010667525813914835\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00010667601163731888\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.0001066759237437509\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00010666770016541705\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00010664698929758742\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00010663593478966504\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00010671478637959808\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00010678217164240777\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00010667409020243213\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.00010669274546671658\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00010665322159184143\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00010663626278983429\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.0001066167393582873\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00010694591823266819\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00010715505748521537\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00010689146583899855\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.0001068584094173275\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00010667790862498804\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00010664122382877394\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00010667454014765099\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00010668338567484171\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.0001066778678796254\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00010666784248314798\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00010665572190191597\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.00010664269473636522\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00010663737368304282\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.0001066309554153122\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00010662345885066315\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.00010661040840204806\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00010661170672392473\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00010658918297849595\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00010657455830369145\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.0001065781104261987\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00010661765991244465\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.0001066389912739396\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00010664418252417818\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00010659361607395112\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00010673972719814629\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00010660737782018259\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00010658396407961846\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00010655771329766139\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.00010654352779965848\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00010655183374183252\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00010680004517780616\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00010680118168238551\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00010663250897778198\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00010657674924004824\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00010657613427611067\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00010658417042577639\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00010658505460014566\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00010658149200025946\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.00010657569306204096\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00010655093006789685\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00010653335979441181\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00010652000550180674\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.0001064997079083696\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00010668019356671721\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00010655862890416756\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.00010655569174559787\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00010654321202309803\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.0001065232208929956\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.0001065278664464131\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00010653303295839578\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00010671690804883838\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00010665125766536221\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00010670943447621539\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.00010683124943170696\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.00010692860785638914\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.0001066977615118958\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.00010659744875738398\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00010667916445527226\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.00010670516843674704\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.00010666142537957057\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00010657165315933526\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00010654071433236822\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.0001065489262691699\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00010655116551788524\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.00010653639561496676\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.0001065332890721038\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00010652017634129152\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00010650936455931514\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00010651482793036849\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00010651152959326282\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.0001065055376966484\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00010649441683199256\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00010648862837115303\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00010647511517163366\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.0001064645720180124\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00010646231123246253\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00010645916481735184\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00010644241032423452\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.0001064399306778796\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00010645549686159939\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00010642683046171441\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.0001066662376979366\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00010648905736161396\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00010651523771230132\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00010649450880009681\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.0001064892642898485\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00010648099880199879\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.00010646489739883691\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00010647085902746766\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.00010645910864695907\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00010644824447808787\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.00010643099842127412\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00010641843517078086\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.00010645488044247031\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00010647452436387538\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.0001074139674892649\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00010690356575651094\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00010667032503988594\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00010663647000910714\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00010669376817531884\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00010672388365492225\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00010653034958522767\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00010647567396517842\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00010652095050318167\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00010648299852618948\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00010645442001987249\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.0001064737100386992\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.00010648292285623029\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00010645210568327457\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.0001064293150557205\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00010642403241945431\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.00010640600201440975\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00010640521941240877\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00010639139509294182\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00010638186475262046\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.00010636720660841093\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00010634354053763673\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00010633575031533837\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.00010630447650328279\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00010630124801537022\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00010628071380779147\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00010628881049342454\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00010628739575622603\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00010643601563060656\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00010634227219270542\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.00010632181103574112\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00010640053951647133\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.00010642232198733837\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00010659517050953582\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.00010641302942531184\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00010641604574630038\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00010644026682712138\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00010653321660356597\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00010667147696949542\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.00010643940302543342\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00010647144925314933\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00010638868290698156\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00010635043843649328\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00010631556622684002\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00010628741350956262\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.00010628500051097945\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00010628321906551719\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00010627020965330303\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00010623937676427886\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00010621245513902977\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00010619622567901387\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00010619707871228456\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00010619327455060556\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.00010621930239722133\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.00010619570413837209\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00010620874352753163\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00010638585692504421\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00010630707402015105\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00010620682296575978\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.0001062078948598355\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00010619215696351603\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00010620238434057683\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.0001062457007355988\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00010627277137245983\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00010615115257678554\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.0001062230535899289\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00010624520480632782\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.00010615821665851399\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00010616993444273248\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00010615695238811895\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00010616570332786068\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00010625213413732126\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.00010620585177093744\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00010618487693136558\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00010611186939058825\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00010612870304612443\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00010618827713187784\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00010613997437758371\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.000106222448812332\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00010626727977069095\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00010634932230459527\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00010642594686942175\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.00010671135765733198\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.0001062828244175762\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00010620352491969243\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00010621950350468979\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.0001061293896054849\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00010615203354973346\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00010614901635563002\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.00010612633283017203\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.0001061097247293219\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.00010610977653414011\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.000106105130398646\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00010610238387016579\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00010608003271045163\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00010610983707010747\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00010607626958517358\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00010606360039673746\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00010605723189655692\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00010605192481307313\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00010604466020595282\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.00010602482507238165\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.0001060246117413044\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.00010601098969345913\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00010601156303891912\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.00010602820053463802\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.0001060553005663678\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00010603922186419368\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00010605632793158293\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00010606000112602488\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00010602605325402693\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.00010611143632559106\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00010607531381538137\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.00010611709789372981\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.00010621844121487811\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.00010629331081872806\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00010613849910441786\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.00010618061583954841\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00010620308690704405\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00010613678547088057\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00010606308409478515\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00010596854437608272\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00010590985330054537\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00010589854093268514\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00010592644714051857\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00010600311012240126\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00010604620212689041\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00010602913709590212\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.00010610473371343688\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00010602524678688497\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.00010597010725177825\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.00010591871076030657\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00010592404374619946\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00010597303917165845\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00010600500390864909\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00010610048862872645\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00010608323966152966\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00010648566152667627\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.00010616666375426576\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00010599549917969853\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00010598704684525729\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00010589741752482951\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00010584401636151597\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.0001058951680897735\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00010593160783173517\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00010590755933662876\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.00010590689606033266\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.00010600389883620664\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00010588474717224017\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.0001058078557252884\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00010575985303148627\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00010574722749879583\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00010574017302133143\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00010575204069027678\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00010577030101558194\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00010580631264019758\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00010583165480056778\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.00010605203890008852\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.0001059290172997862\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00010595337051199749\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00010600197711028159\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00010604670416796579\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.0001060991088161245\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.0001059834606712684\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00010598537803161889\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00010603495349641889\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00010612343001412228\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00010603858536342158\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00010606061667203903\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00010630798293277621\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00010637816711096094\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00010614379949402065\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00010606593161355705\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00010611943725962192\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00010619118489557877\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.000106148186896462\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00010601062444038689\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.0001060503296321258\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.0001059533268562518\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.00010595091938739643\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00010603037371765822\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.0001062940075644292\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00010618248343234882\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00010634498175932094\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.0001065286376979202\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00010608450334984809\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.00010625916678691283\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.00010603310889564454\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00010600040870485827\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00010592763836029917\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00010602496098726988\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00010574461688520387\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00010569476406089961\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.00010564998752670363\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00010546215547947213\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.00010580906295217574\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00010509843297768385\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00010493704845430329\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.0001048113263095729\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.000104749949532561\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.00010466293693752959\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00010465872444910929\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00010462633188581095\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.0001045882111066021\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00010455605690367519\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00010460416378919036\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00010457713477080689\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00010452645045006648\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.0001045087695820257\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00010448746266774833\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00010450500471051783\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00010450616216985509\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00010446752567077056\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.00010449522902490571\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00010450963745824992\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.00010455794865265488\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00010454737319378183\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.0001045665453420952\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00010452123678987846\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00010449237452121451\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.00010449156339745968\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00010457670723553746\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00010454953997395932\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00010447678214404732\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.00010442997911013662\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00010443415667396038\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.00010448617424117401\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00010453336290083826\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.0001045736824744381\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00010464998456882312\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00010464664228493348\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.00010449191293446347\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.0001044227002421394\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.00010440315934829415\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00010445115010952577\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00010447882697917521\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00010449702735058963\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00010444527113577351\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00010455503244884312\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.0001045531901763752\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00010455132432980463\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.0001045787808834575\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.0001046001593931578\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00010454193339683115\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.00010441088146762922\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00010445669438922778\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00010439798701554537\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00010445991007145495\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.00010441091959364712\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.00010441362421261147\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00010442787199281156\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.0001044833502965048\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.0001044626469956711\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00010447354870848357\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00010453545342897997\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.00010451994225149975\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00010457172465976328\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00010448902874486521\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.0001044907359755598\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00010443841223604977\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00010439840494655072\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00010438713623443618\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.00010436125041451305\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00010439483710797504\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.00010440731595735997\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00010439030331326648\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00010434562398586422\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00010440239741001278\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.0001044315833132714\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00010433882474899293\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00010443472245242447\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00010437312157591805\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00010439335397677496\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00010439442819915713\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00010439132020110265\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00010443956183735281\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00010434196359710767\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00010443377832416444\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00010441253893077374\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00010441402904689313\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.000104402398574166\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00010457729105837643\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00010443055245559662\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00010441411228384823\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00010442590253660455\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00010447300999658182\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00010444102168548853\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00010440316051244735\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.00010439972771564499\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00010446467116707936\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00010457784519530832\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.0001046528245205991\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00010450542031321674\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00010451864422066137\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00010449298308230937\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00010447684093378484\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00010448708257172256\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00010454337345436215\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00010458939796080813\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00010477684467332438\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00010471216344740242\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00010442412458360195\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00010451265552546829\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.0001044783610268496\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.00010445202147820964\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00010453905735630542\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.0001044945520698093\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00010443908715387806\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00010444480227306485\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.00010440913843922318\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.0001043918143841438\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.00010446044005220756\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00010449053399497644\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.00010452610935317352\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.00010466606443515047\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00010474098526174203\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00010451860027387738\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00010445715975947679\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.00010441503167385235\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.0001044111471856013\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00010436150769237429\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00010430379013996571\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.0001042903948109597\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00010430905385874211\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.0001043551228940487\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.00010435513948323205\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00010432382754515856\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00010429473040858283\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00010428958805277944\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.00010431395319756121\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00010431318369228393\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.00010439984762342647\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00010435377946123481\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00010435511328978464\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00010440049867611378\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00010439557197969407\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00010432674869662151\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.000104313557385467\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.0001042886238428764\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00010425046348245814\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00010427499713841826\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.0001043480570660904\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.00010431050090119243\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00010426387452753261\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00010434074909426271\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00010432542738271878\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00010430808149976656\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00010438449186040089\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00010447206470416859\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00010446872183820233\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.00010431145055918023\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00010432475741254166\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00010429123154608533\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00010428460751427338\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00010426728927996009\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00010424684616737067\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00010426766966702417\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00010425709682749584\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.0001042714825598523\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00010429872112581506\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00010448465560330078\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00010430112481117248\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.00010435710195451975\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00010443472565384582\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.0001045111971325241\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.0001043223132728599\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.00010428975307149812\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00010430453898152336\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00010433380986796692\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00010434617928694933\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00010436214419314638\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.0001043980530812405\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00010437047661980614\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00010437224700581283\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.00010443689418025314\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.00010444318351801485\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00010436796263093129\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00010453409311594441\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00010448700457345695\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00010440391051815822\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00010435910779051483\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00010434079798869789\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00010439348698128015\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00010437357472255827\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00010435170464916155\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00010451992420712486\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00010456602642079815\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00010443902545375749\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00010436985146952793\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.00010450237197801471\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.0001045332770445384\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00010463560902280733\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00010478353797225282\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00010498857620405033\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00010441600607009604\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.00010445893392898143\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00010436078096972778\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.00010437516059027985\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00010450058209244162\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00010449340887134895\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00010454788483912126\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.0001043942358228378\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.00010444050538353622\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00010445682099089027\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00010446907777804882\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.0001044013918726705\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00010427648085169494\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.00010429269401356578\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00010428566834889352\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00010425425192806869\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00010426880500745028\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.00010424259293358774\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00010420938022434712\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.0001041776459896937\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00010418549762107432\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00010421787679661065\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00010420856589917094\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00010418601363198831\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.0001041889050975442\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00010420389531645924\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00010424328793305904\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00010428101581055671\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00010443263105116784\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.0001044080633437261\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00010424898733617738\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00010420689766760916\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00010417966172099113\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00010417977493489161\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.00010419379221275449\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00010423703497508541\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00010429535526782275\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.0001042921154294163\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00010423987143440172\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.000104217923944816\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00010422077379189432\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00010427118162624538\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00010424272011732682\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00010420393053209409\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00010425522137666121\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.00010431465256260708\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00010442795697599649\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00010442756465636193\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.00010434650583192706\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00010436389333335682\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00010453582974150776\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.00010448429733514786\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.0001043578697135672\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00010440720769111067\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.00010455161944264546\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00010482262528967112\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00010433251183712855\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00010434725787490606\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00010430462833028287\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00010430830938275904\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00010426935565192252\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.00010426665976410731\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.0001043637070688419\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00010439365374622866\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.0001043096202192828\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00010429479618323967\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.0001042984495870769\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00010433703835587948\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.00010433692979859188\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00010424725245684385\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00010423206753330305\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.0001042352625518106\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00010427198198158294\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00010424717445857823\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00010435842850711196\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.0001045133444131352\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00010452899266965687\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00010434659256134183\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00010455568437464535\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.00010450824192957953\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00010444573883432894\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00010436681943247094\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.00010424041596706956\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00010419319325592368\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00010418039048090577\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00010414805932668969\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00010412774980068207\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00010413234704174102\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00010411508090328425\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00010415027732960879\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00010421513841720298\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00010429107613163068\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.00010448306158650667\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00010444862098665908\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00010425982211017981\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.00010420279460959136\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00010421273385873064\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00010418613121146336\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00010419844678835944\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.00010423532366985456\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.00010427723405882716\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00010418451885925606\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.00010420982289360836\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00010418989812023938\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00010421352315461263\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.0001041812592302449\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00010428659530589357\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00010444947693031282\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00010446157306432724\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00010450693021994084\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00010444104060297832\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.00010447576991282404\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00010438559751491993\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00010438836587127298\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.0001043986395234242\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00010440021520480514\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00010437926481245086\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00010437836608616635\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00010432229493744671\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.00010428092966321856\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00010430178343085573\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.0001043992378981784\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.00010434419295052066\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00010434452415211126\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00010435432166559622\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00010437608114443719\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00010448040178744122\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00010446699103340506\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00010454686329467222\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00010438458120916038\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00010428397567011416\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00010429848334752023\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00010425189917441457\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00010424156644148753\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.00010420654114568607\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00010418805322842672\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.0001042349755880423\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00010419628728413954\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00010417230922030285\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.00010421882499940694\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.0001041459830594249\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00010415687604108825\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00010418048274004832\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00010421200946439057\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00010427779227029533\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00010439188190503046\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00010415635275421664\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00010421306127682328\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.0001042938293539919\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.00010439041361678391\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00010455513052875177\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.00010456355987116695\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00010433331888634712\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.00010426682682009414\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00010509879677556456\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.00010493987589143217\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.00010452706483192742\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00010443841310916468\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00010438593628350645\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.0001043684093747288\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00010436429060064257\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00010436597658554092\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00010435711999889463\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00010442199534736574\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00010439329576911405\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00010433448100229726\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.00010433129355078564\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00010439391626277938\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00010448870016261935\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00010443082894198596\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00010434282914502546\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.0001043911708984524\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.00010448556597111747\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00010436119890073315\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00010436564625706523\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.0001043818355537951\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.00010449318389873952\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00010440018406370654\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.00010434078722028061\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00010434764029923827\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00010436572018079459\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.00010441843391163274\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00010447039414430038\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00010441543359775096\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.00010445950378198177\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00010445021034684032\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.00010442898783367128\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00010439048666739836\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00010437795135658235\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00010437946970341727\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.0001043617384857498\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.000104335249052383\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00010434881172841415\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00010435315227368846\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00010431365633849054\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00010433621937409044\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00010432054084958509\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00010445793363032863\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.0001043679311987944\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00010429372516227886\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00010427391185658053\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00010430634603835642\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00010428659530589357\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.00010428043053252623\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00010431312228320166\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00010431522561702877\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00010434248222736642\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00010430651920614764\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00010443366394611076\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00010451857029693201\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.0001046481795492582\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.00010432568844407796\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.00010439421253977344\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00010439856036100537\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.0001042682002298534\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.00010426846420159563\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00010427599074319004\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00010426405147882178\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00010425826243590564\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00010425836517242714\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00010424922278616577\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00010424181469716132\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.00010424045525724068\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00010422548541100696\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00010425793938338757\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.00010426627995911986\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.00010426138702314347\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.00010421500832308084\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00010420863662147895\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.00010422545485198497\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.0001042908022645861\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00010429895191919059\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00010426548775285482\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00010436121723614633\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00010428066278109327\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.00010420209378935397\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00010422312101582065\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00010423037718283013\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00010421208338811993\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00010432738374220207\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00010423495812574402\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.00010421778773888945\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.0001042357727419585\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00010432078415760771\n",
      "Autoencoder10 training DONE... TOTAL TIME = 354.52567958831787\n",
      "start evaluation on test data for Autoencoder10\n",
      "MSE is 0.009567680268353775\n",
      "mutls per sample is 5592320\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder11\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.00048201030818745494\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.0004223710810765624\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.0004217732441611588\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0004217310110107064\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.00042172483052127063\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.00042173646623268725\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.0004217498772777617\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.0004217630170751363\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.0004217742360197008\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.0004217856226023287\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.00042179773910902443\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.0004218093899544328\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.00042181963101029396\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.0004218277311883867\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00042183386045508087\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.0004218373552430421\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.00042183751706033947\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.0004218381515238434\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.00042184139485470953\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00042184331337921324\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.0004218434484209865\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00042184342397376894\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00042184427613392475\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.0004218461271375418\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.000421847429824993\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00042184719000943005\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00042184752644971015\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.0004218482389114797\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.0004218475869856775\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00042184648104012014\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00042184511432424187\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.0004218443762511015\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.000421845082892105\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.0004218450305052102\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00042184360325336456\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00042184256599284706\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.00042184200254268944\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00042184208403341474\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00042184102348983286\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00042183895013295117\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.000421836368041113\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.0004218350152950734\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00042183307465165854\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.00042183139477856455\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00042182900593616066\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.0004218265600502491\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.0004218250245321542\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.0004218240350019187\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00042182254255749283\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00042182093020528557\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00042181971250101927\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00042181822122074666\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00042181731783784925\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.00042181616998277605\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.0004218145704362541\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.000421813924331218\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00042181203491054476\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00042180938879027963\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.0004218070418573916\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.0004218053689692169\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00042180437594652176\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00042180528049357236\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00042180255288258193\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00042179898824542763\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00042179798241704703\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.0004217983491253108\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00042179432814009487\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.00042179092182777824\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.0004217902827076614\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.0004217904119286686\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00042178903822787106\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.0004217879252973944\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.000421788664534688\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.0004217866261024028\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.000421783464262262\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.0004217805399093777\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00042177888215519485\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00042177860625088216\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.0004217780870385468\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.0004217771568801254\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.0004217763582710177\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00042177507537417114\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00042177374009042977\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00042177227209322154\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00042177149676717816\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.0004217706061899662\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.0004217710567172617\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.0004217700322624296\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00042176751885563134\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.000421765863429755\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.0004217642894946039\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.0004217642813455313\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.000421764743514359\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00042176310904324055\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.0004217634745873511\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.0004217631975188851\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.0004217617749236524\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00042176417424343525\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00042176625342108307\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.0004217668902128935\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.0004217684315517545\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00042177133611403405\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00042176832212135194\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.0004217576968949288\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00042175087379291653\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.0004217488272115588\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00042174882139079273\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00042174951056949794\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.000421750150853768\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00042175070382654666\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00042175100999884306\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.0004217499657534063\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.0004217488446738571\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.0004217478062491864\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00042174618574790655\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.0004217448434792459\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.00042174445115961134\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.0004217473580501974\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.0004217431030701846\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.0004217427736148238\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.0004217432567384094\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00042174001573584973\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.0004217384418006986\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.0004217380157206208\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00042173886206001046\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.0004217386292293668\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.0004217366559896618\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00042173637659288944\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.0004217361367773265\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.0004217357479501516\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00042173533234745263\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.0004217352147679776\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00042173498310148715\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00042173610650934277\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00042173681780695917\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00042173421825282277\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.0004217326396610588\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.0004217331949621439\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00042173440335318446\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.0004217325267381966\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00042173063266091047\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00042173034278675914\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00042173042078502476\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.0004217297770082951\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00042172900051809845\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.0004217285895720124\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.0004217283404432237\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00042172973277047275\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00042173304478637874\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.0004217333090491593\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.0004217391321435571\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00042173821944743397\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00042173691443167624\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.0004217331181280315\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00042173115303739906\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00042172676068730653\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.0004217240598518401\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.000421722901519388\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.0004217216593679041\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.0004217212158255279\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.0004217209224589169\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.0004217233636882156\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.0004217245348263532\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00042172245332039893\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00042172023677267134\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00042171845445409417\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.0004217177128884941\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.0004217174812220037\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00042171728797256946\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.0004217177291866392\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00042171695618890224\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00042171552311629054\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00042171485372819004\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.0004217142786365002\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.0004217138991225511\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.0004217141156550497\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.0004217131342738867\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.0004217128921300173\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00042171347769908606\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00042171536944806575\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.0004217141692060977\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.0004217122751288116\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.000421713727992028\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.0004217129747848958\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00042171233566477893\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.00042171184089966117\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.00042171162203885615\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.0004217116197105497\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00042171120294369756\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.0004217108490411192\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.0004217141540721059\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.000421717461431399\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00042171435430645945\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00042171109700575474\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.0004217106068972498\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00042171024018898605\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.0004217102658003569\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.0004217094939667732\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.0004217087896540761\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00042170939035713674\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00042170892469584943\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.0004217097093351185\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.00042171164066530766\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00042170793749392034\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.0004217078536748886\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00042170774890109896\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.0004217075917404145\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.0004217073426116258\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.000421708031790331\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.0004217071377206594\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00042170676751993594\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00042170710396021604\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00042170637869276106\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.0004217070376034826\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00042170593165792526\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00042170523083768786\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.0004217051086015999\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.0004217048769351095\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.00042170532047748567\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.000421705060871318\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00042170716682448983\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.0004217058373615146\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00042170612956397233\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.0004217057547066361\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.0004217072587925941\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00042170980479568245\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00042170792701654134\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00042170476168394087\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00042170455562882123\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00042170420638285575\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.0004217037302441895\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.0004217037570197135\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.0004217044659890234\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.0004217041947413236\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.0004217036277987063\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00042170396191067996\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.000421703914180398\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00042170396191067996\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00042170382919721306\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.0004217072098981589\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.0004217074310872704\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00042170524364337325\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.0004217044764664024\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.0004217039328068495\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.0004217036836780608\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00042170349741354584\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.0004217053495813161\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00042171049863100053\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00042170540895313023\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00042170332395471635\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00042170336819253863\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.0004217032645829022\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.000421703236643225\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00042170330532826484\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00042170333326794205\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.0004217031272128224\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00042170473840087654\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00042170748580247166\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.0004217055905610323\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00042170778615400193\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00042170654982328417\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.0004217050108127296\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00042170604225248096\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.0004217036836780608\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00042170443688519297\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00042170288506895306\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.000421702821040526\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.0004217029025312513\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00042170268716290593\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00042170267552137374\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00042170270229689776\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.0004217035893816501\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.0004217036522459239\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.000421703279716894\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.000421703722095117\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.0004217040038201958\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.0004217044299002737\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00042170491418801247\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.00042170509463176133\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.0004217050818260759\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.0004217059491202235\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00042170607252046467\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.0004217061109375209\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.0004217066091950983\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00042171029024757446\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.0004217154800426215\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00042171364650130273\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.0004217153484933078\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.00042174023459665475\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00042174642090685665\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00042171275359578433\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00042170846951194107\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.0004217069956939667\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.000421706463675946\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.0004217059270013124\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.0004217052843887359\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.0004217053821776062\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00042170745437033477\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00042170481639914215\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.0004217046278063208\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00042170374654233457\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.0004217033379245549\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.0004217031807638705\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00042170303058810534\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.0004217027500271797\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.0004217025043908507\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.0004217019525822252\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00042170307133346794\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.0004217043798416853\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00042170207947492597\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.0004217017989140004\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00042170091299340126\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.0004217005579266697\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.000421700346050784\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.0004217001423239708\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.0004216999909840524\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00042169981636106966\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00042169959284365176\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00042169942054897546\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.0004216992622241378\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.0004216991178691387\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.0004216989839915186\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00042169877095147966\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.0004216985055245459\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.0004216981783974916\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.00042169794789515436\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.0004216977418400347\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00042169755208306016\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00042169731110334394\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.0004216969572007656\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00042169650783762335\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.0004216959734912962\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.0004216954635921866\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00042169506312347946\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00042169497697614134\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947953682393\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469874352215\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.0004216943378560245\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.0004216943494975567\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00042169431573711334\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00042169441585429015\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00042169441119767727\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00042169435299001636\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.0004216943995561451\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00042169448221102357\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.0004216945159714669\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00042169454623945055\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.0004216945718508214\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946032829583\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946242377162\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465101324024\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467080384494\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946999076754\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947162058204\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947348322719\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00042169475695118307\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.00042169477441348133\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947965323925\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948058456182\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948268003762\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.00042169483844190837\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948605608195\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00042169487103819846\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948838438839\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00042169489548541604\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949117835611\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.000421694916440174\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00042169493273831905\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949478723109\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00042169495252892376\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949676629156\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949793044478\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00042169499211013316\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.00042169499676674604\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949979308993\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00042169500491581855\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.000421695017721504\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950293630362\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.00042169503052718937\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00042169503634795544\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.000421695044497028\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950503177941\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950561385602\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00042169506195932627\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00042169506661593914\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.00042169506778009234\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950724367052\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950805857778\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508174993097\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950840782374\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950887348503\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508640654385\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508524239065\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508989900353\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950887348503\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950968839228\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950957197696\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950968839228\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00042169509339146316\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00042169509455561635\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950910631567\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00042169509339146316\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950968839228\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00042169509455561635\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950922273099\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00042169509455561635\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950887348503\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950922273099\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508989900353\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950875706971\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508524239065\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950875706971\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950887348503\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950829140842\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507825747134\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950805857778\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507476501165\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508174993097\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950759291649\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507476501165\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507476501165\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950759291649\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950724367052\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507825747134\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950805857778\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950829140842\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950829140842\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508640654385\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508524239065\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950840782374\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508640654385\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00042169508640654385\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950829140842\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507942162453\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950805857778\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950829140842\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950829140842\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507825747134\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950770933181\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950805857778\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950759291649\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950759291649\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00042169507360085846\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950724367052\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950724367052\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.000421695071272552\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00042169506661593914\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.00042169506778009234\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950654517859\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00042169506195932627\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950573027134\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00042169505381025376\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00042169505381025376\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.00042169504682533444\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.000421695044497028\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.000421695044497028\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.00042169504798948763\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.00042169504798948763\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00042169504333287476\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.000421695044497028\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950410045683\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.000421695034019649\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00042169503518380225\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.000421695034019649\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00042169503518380225\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00042169503518380225\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00042169503052718937\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950293630362\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00042169502703472974\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950247064233\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950188856572\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950212139636\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.000421695017721504\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950188856572\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00042169501655735074\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950130648911\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950142290443\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00042169501190073786\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.000421695007244125\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00042169501539319755\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.00042169501073658467\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00042169500840827823\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00042169500840827823\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00042169500375166535\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.0004216950002592057\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949979308993\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949944384396\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00042169499211013316\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.00042169499211013316\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949897818267\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00042169498396106065\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949862893671\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00042169498512521384\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00042169498396106065\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00042169498396106065\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949816327542\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00042169497814029453\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949746478349\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949734836817\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949734836817\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949734836817\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00042169497231952846\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00042169496533460914\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949618421495\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00042169496533460914\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.00042169496417045595\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949618421495\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949583496898\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949618421495\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949583496898\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949583496898\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949548572302\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00042169495369307695\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949548572302\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949502006173\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949513647705\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949502006173\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.00042169494670815763\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00042169494670815763\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00042169494670815763\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00042169494554400444\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949420515448\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00042169494437985125\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00042169494088739156\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00042169493739493193\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00042169493273831905\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00042169493390247225\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949315741658\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949280817062\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949304100126\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00042169492575339974\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00042169492575339974\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00042169492226094005\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00042169492109678686\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00042169492226094005\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949199326336\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949187684804\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949176043272\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00042169491294771435\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949117835611\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949106194079\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.00042169490712694823\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949047986418\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949036344886\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.0004216949036344886\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948966495693\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948966495693\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00042169489432126284\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948966495693\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948919929564\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00042169489432126284\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00042169489432126284\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00042169488966464997\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00042169488966464997\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948850080371\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948873363435\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00042169488267973065\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.00042169488267973065\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00042169488267973065\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.00042169487802311777\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00042169487569481133\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948722023517\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948733665049\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00042169487103819846\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948722023517\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948652174324\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948663815856\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00042169486405327914\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948652174324\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00042169486288912595\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948605608195\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00042169485823251307\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00042169485823251307\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948570683599\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948535759002\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948489192873\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948489192873\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948454268277\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948465909809\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00042169484309852125\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00042169484426267444\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00042169483844190837\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948337852955\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.00042169483611360193\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948337852955\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948349494487\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948337852955\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948302928358\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948291286826\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948302928358\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948279645294\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948268003762\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948209796101\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00042169481981545686\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.00042169482330791654\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00042169481632299723\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00042169481865130367\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948174871504\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948209796101\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00042169481283053754\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00042169481283053754\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948105022311\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00042169480817392467\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00042169480817392467\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948035173118\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00042169480002485216\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948035173118\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.0004216948023531586\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00042169480002485216\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00042169480002485216\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947965323925\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947953682393\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947953682393\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947965323925\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947907116264\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947907116264\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00042169478838331996\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00042169478721916677\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00042169478721916677\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947837267071\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947837267071\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947825625539\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947825625539\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00042169478023424745\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00042169478139840065\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947755776346\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00042169477324932814\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947755776346\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947662644088\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00042169476859271526\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947697568685\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00042169476510025563\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947662644088\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947639361024\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00042169475695118307\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00042169475695118307\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947557870299\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947592794895\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00042169476160779595\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00042169475695118307\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00042169475695118307\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00042169475695118307\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947581153363\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947581153363\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947557870299\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00042169476044364275\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947557870299\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00042169474996626375\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.00042169474880211056\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947441454977\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947453096509\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.00042169474065303805\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947383247316\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947383247316\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00042169473716057837\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947383247316\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947348322719\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947348322719\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947348322719\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00042169473716057837\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947348322719\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947325039655\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00042169472901150586\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947313398123\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00042169472319073973\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00042169472202658654\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00042169472319073973\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947208624333\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947196982801\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00042169471736997366\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00042169471853412686\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947196982801\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00042169471736997366\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947127133608\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947150416672\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947150416672\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947115492076\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947115492076\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947115492076\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.00042169471038505435\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00042169470572844147\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947068925947\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947080567479\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947080567479\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947045642883\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00042169470572844147\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00042169470572844147\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947045642883\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.0004216947045642883\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00042169470340013503\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469757936896\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946964152157\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946964152157\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469757936896\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946952510625\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946952510625\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946940869093\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469175860284\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469175860284\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946882661432\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946929227561\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946894302964\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469059444965\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469059444965\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00042169469059444965\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468710198996\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468710198996\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468710198996\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468360953033\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946847736835\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468360953033\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946812812239\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468360953033\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468360953033\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468244537714\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468244537714\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467895291745\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00042169468244537714\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467895291745\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946812812239\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467778876426\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467778876426\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946801170707\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946801170707\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467895291745\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467778876426\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467778876426\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946801170707\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946754604578\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946742963046\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946731321514\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.000421694676624611\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467895291745\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467778876426\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.000421694676624611\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.000421694676624611\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946801170707\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467778876426\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946754604578\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946731321514\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946731321514\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946754604578\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946731321514\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467196799814\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946696396917\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946684755385\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946684755385\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467080384494\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946696396917\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467196799814\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467080384494\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00042169467196799814\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946696396917\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946696396917\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946684755385\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466614723207\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466614723207\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466381892563\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946649830788\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466731138526\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466381892563\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946684755385\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466614723207\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466032646594\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466381892563\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466381892563\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466381892563\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466032646594\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946614906192\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465916231275\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946614906192\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00042169466032646594\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946626547724\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946568340063\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946568340063\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946579981595\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946579981595\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465566985307\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465566985307\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946568340063\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946568340063\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465217739343\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946568340063\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946545056999\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465217739343\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465566985307\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946533415467\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946533415467\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946486849338\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465217739343\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.00042169465101324024\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00042169464635662736\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946486849338\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00042169464752078056\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946486849338\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946451924741\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946451924741\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946486849338\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946440283209\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946440283209\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00042169464053586124\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463937170805\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946428641677\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00042169464053586124\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946417000145\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946417000145\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946417000145\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463937170805\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946370434016\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463587924836\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946382075548\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463587924836\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946370434016\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463937170805\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463937170805\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00042169464053586124\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946382075548\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463471509517\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946335509419\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946335509419\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.00042169462889432905\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00042169463238678873\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946335509419\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946300584823\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946312226355\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00042169462773017585\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946265660226\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946254018694\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.00042169462773017585\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946254018694\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946254018694\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946300584823\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00042169462773017585\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00042169462889432905\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00042169462773017585\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.000421694623073563\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946254018694\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946254018694\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946242377162\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946219094098\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00042169462074525654\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461958110335\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946219094098\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946219094098\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.000421694623073563\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461958110335\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946184169501\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946184169501\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461608864366\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946184169501\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946172527969\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461608864366\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946184169501\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461608864366\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461608864366\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460793957115\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946067754179\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946056112647\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460793957115\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946067754179\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460793957115\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946056112647\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946067754179\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946056112647\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460793957115\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461608864366\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.00042169460910372435\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461492449047\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946137603372\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946114320308\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.0004216946102678776\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00042169461259618403\n",
      "Autoencoder11 training DONE... TOTAL TIME = 369.10935521125793\n",
      "start evaluation on test data for Autoencoder11\n",
      "MSE is 0.6575595088471847\n",
      "mutls per sample is 5592400\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder12\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.014952807798981666\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.003469135505147278\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.000995880865957588\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0005269536084961146\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.0004409749852493405\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.0004249359224922955\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00042144743958488106\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.0004202017211355269\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.00041920353076420726\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.00041793199023231864\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.0004161496041342616\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00041367783327586946\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.0004103841399773955\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00040620913379825653\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00040119383484125135\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.000395465720212087\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.0003891904815100133\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00038253115257248283\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.0003756173234432936\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.0003685528365895152\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00036144786747172475\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.0003545000904705375\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.0003476949012838304\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.00034112182911485436\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.00033494741772301497\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.0003291755123063922\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00032378982868976893\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.0003188351250719279\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.000314226308837533\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.000309867492178455\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00030565437977202235\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00030147548532113435\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.0002972387196496129\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.000292912891600281\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00028845264110714195\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00028388046193867923\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0002792880602646619\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.0002747646754141897\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00027020993293263017\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00026582611608318987\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.0002615997928660363\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.0002575257350690663\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00025364266824908557\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.0002499902085401118\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00024642325181048366\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00024303137965034694\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00023986740794498473\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00023684163228608667\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00023401130049023776\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00023133924813009799\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.0002287757070735097\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00022635651344899088\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00022410880133975298\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.0002219998894724995\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00022002733661793172\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00021817638189531864\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00021644658758305014\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00021483139018528164\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.0002133230003528297\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00021191952168010176\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00021061872015707195\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.0002094198646955192\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00020832587324548513\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00020733724406454712\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.0002063165931031108\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.0002053724171128124\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00020456069614738226\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.0002038070873823017\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00020312417706009\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00020249769149813802\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.0002019286109134555\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.00020141072745900602\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00020094188395887612\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.00020050914608873426\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00020012198016047477\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.0001997581502655521\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00019941154110711068\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00019910389673896133\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00019883769971784204\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00019860083295498044\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00019838752050418406\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.0001981954200891778\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00019802266266196966\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00019786740245763212\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00019772802654188126\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.00019760297494940458\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00019749084720388054\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.00019739037961699068\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.0001973004126921296\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00019721993478015066\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.0001971479383064434\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.0001970835856627673\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00019702608173247427\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00019697470648679882\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00019692880334332586\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.000196887738420628\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00019685099250636994\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00019681814359501004\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00019678882730659098\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.0001967626332771033\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00019673923787195236\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00019671842164825648\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00019669998495373874\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00019668372522573918\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00019666936655994506\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.0001966567279305309\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00019664567313157022\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00019663592509459705\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00019663106766529381\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.0001966428803279996\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00019662873877678065\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00019663960090838373\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00019662158680148423\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.00019662045349832624\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00019662251346744597\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00019663445826154202\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.0001966072537470609\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00019658992649056019\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00019659097422845663\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00019659574201796203\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00019659143348690122\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00019659092475194485\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.00019659123034216464\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00019659194571431727\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00019658447185065596\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00019658460631035267\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00019658829492982477\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00019659002020489425\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.000196589341503568\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00019658576347865165\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.0001965972880134359\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00019658716861158609\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00019658967154100536\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00019658380071632564\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00019658552191685884\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00019658176344819367\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.0001965805678628385\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.00019657905155327172\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00019658164877910166\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00019658357370644808\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00019658525532577188\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00019658804929349573\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00019659183220937848\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.0001965976582141593\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00019664013700094074\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00019662607985083014\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00019667475076857956\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00019660487130749972\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00019658886652905493\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.0001965900865616277\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00019658402365166694\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00019658625416923314\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.0001965880172792822\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00019658760342281312\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.0001965867663966492\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00019658620585687457\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.0001965858752373606\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00019658582226838916\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00019658605218864978\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00019658646197058262\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00019658695498947054\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.00019658748002257198\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.00019658818084280938\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00019658953126054258\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.0001966467662714422\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.0001966153591638431\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00019662055419757962\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.0001966160733718425\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00019660310994368046\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00019660847377963363\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.0001966105797328055\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00019660820544231683\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00019661272293888032\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00019661471247673035\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.0001966180617455393\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.0001966273447033018\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00019662850012537093\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.00019661826372612267\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00019662400707602502\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00019662237435113638\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00019662072125356645\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.00019663328304886818\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.0001966456283116713\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00019666208128910512\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00019665918953251093\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.00019661995582282544\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.0001966169080697\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00019661783007904888\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.0001966138300485909\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00019661216239910573\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.00019661312166135758\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.00019661407161038368\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00019661480677314104\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.0001966155058471486\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00019661614729557186\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00019661681668367237\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.0001966175518464297\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.0001966190367238596\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.00019663833838421852\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.0001966401230311021\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00019663716841023416\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00019663556071463972\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.0001966332964366302\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00019663421611767261\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00019664468942210078\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00019664810330141335\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00019664910330902786\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00019664064107928426\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00019663227314595133\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.00019663405953906476\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.0001966345653636381\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00019663216546177863\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00019663082202896475\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00019663112703710795\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.0001966326462570578\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.00019663738959934562\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.0001966861862456426\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00019666721869725733\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.00019666460866574198\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00019667478802148252\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.0001966710708802566\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00019665281113702803\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00019663899845909327\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.0001966415235074237\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00019664475228637458\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.0001966480287956074\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00019665210216771812\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.0001966571935918182\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.0001966616394929588\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00019665794156026095\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00019665165396872908\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.0001966455567162484\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.00019664237974211573\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00019664122723042965\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00019664135936181993\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00019664187275338917\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.0001966425240971148\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00019664326275233178\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00019664410443510862\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00019665819127112627\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.00019666254753246903\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00019666180247440934\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.0001966563903260976\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.00019666432752273976\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00019667136890348048\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.0001966759341303259\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.0001966555992839858\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00019665388274006545\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00019665607425849884\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.0001966587093193084\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00019666526408400385\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00019667808373924345\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00019666868261992933\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.0001966580533189699\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00019665720174089074\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.0001966583641478792\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00019665830535814166\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00019666068663354964\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.0001966697134776041\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00019670139299705626\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00019667662796564401\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00019666791602503507\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00019666822860017419\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.0001966717530740425\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00019666881766170264\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00019666668260470032\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00019666883745230734\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00019666697713546455\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00019667025364469737\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00019667638989631086\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00019667347602080553\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.0001966865878785029\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.0001966824888950214\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00019670162873808295\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00019668050576001405\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00019667786720674484\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00019667810411192477\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00019667726766783744\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.00019667967746499925\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.0001966851024189964\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.00019669932895340027\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00019672599679324777\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.0001967062003677711\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00019667899119667708\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.0001966845546849072\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00019668004708364606\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.0001966724917292595\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00019667398766614496\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.00019667747954372317\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00019667987420689313\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.0001966815470950678\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.0001966824766714126\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.0001966826687566936\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00019668262219056488\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00019668242486659438\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.00019668189634103328\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.0001966811378952116\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00019667990040034055\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00019667822343762963\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.0001966765068937093\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00019667573506012558\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00019667551387101412\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00019667600630782545\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.0001966800255468115\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00019668199529405684\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.0001966874790377915\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00019668617576826364\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.0001966930372873321\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00019668500463012605\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00019669459550641477\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00019669328059535474\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00019669661414809526\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.0001966922584688291\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.0001966870581964031\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.0001966861286200583\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00019668729684781282\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00019669024099130183\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00019669347500894218\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00019668663560878485\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00019668717461172492\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.00019668757158797234\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.0001966820319648832\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00019668135093525051\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.000196680260123685\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00019668005348648875\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.00019668113440275193\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.00019672013353556395\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00019669109664391727\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.0001966870215255767\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00019668632303364574\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.0001966846303548664\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.0001966852048644796\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.00019668503606226295\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.000196683254907839\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00019668408203870057\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00019668553839437663\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00019671753922011702\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00019671113812364639\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.0001966879569226876\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00019668489519972354\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00019669212866574526\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00019668347842525692\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00019668178865686058\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00019668661057949067\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00019668977067340164\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.0001966863393317908\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00019668464490678163\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00019668177876155823\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.0001966819359222427\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.0001966810825979337\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.0001966809929581359\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00019668224675115198\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.00019668756984174253\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00019669544650241733\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.00019670470734126867\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00019669730681926012\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.00019668699998874218\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00019668296910822392\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.00019667952787131071\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00019668005057610572\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.0001966786419507116\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.0001966793037718162\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.0001966830313904211\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00019671366724651306\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00019669079512823372\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.0001966838480439037\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.0001966786861885339\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.0001966768188867718\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.0001966767309932038\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00019667596730869264\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00019667767512146383\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.000196695999475196\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.0001966966246254742\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.00019669075729325414\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.00019668738124892115\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00019668006396386773\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.00019667778513394297\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.0001966794114559889\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.0001966775330947712\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00019667376182042062\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00019667280197609215\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00019667232350911945\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.00019667019660118967\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00019666584383230658\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00019666011328808963\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.00019665110507048665\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00019660230085719377\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00019612071453593673\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00019399253418669104\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00019008739152923226\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00018480449449270964\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00017804995237383992\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.00016986369097139688\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00016357527871150524\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.00016142230189871042\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00016068881435785443\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.0001603858609450981\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.0001602206117240712\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00016010572551749646\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.0001600297068944201\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00015994729415979235\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.00015988767088856548\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.0001598422706592828\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00015982905635610222\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00015975520247593522\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00015970141044817864\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.000159659911878407\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.0001595983182778582\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00015952735731843858\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.0001594185142312199\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.000159307683352381\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00015913824667222797\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00015893800707999616\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.0001587290968745947\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.0001584941300097853\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.00015820196305867283\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.0001578239747323096\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00015737843932583928\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00015683794219512493\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00015616800053976475\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00015538272098638116\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.00015445792581886053\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.0001532105205114931\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00015166054130531846\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.0001499962224625051\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.0001483302202541381\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00014663010544609277\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00014510545821394772\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.0001438161713303998\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.000142779610469006\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.0001419902517227456\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00014132300508208574\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00014082394307479262\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.0001403128867968917\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.0001399519247934222\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00013964543060865253\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00013938570104073732\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.0001391925901407376\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00013905774569138884\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00013902113016229123\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00013906089239753782\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00013885486347135157\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00013855848286766558\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00013838957296684383\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.000138265288551338\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.00013818285951856525\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00013810675765853374\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00013803287176415325\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.0001379409502260387\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00013785564980935304\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.0001377698901342228\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.00013767138414550573\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00013757185777649284\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.0001374535821378231\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.00013728933176025747\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00013725827273447066\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00013698089634999632\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.0001366789499297738\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00013639208511449398\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00013607722881715746\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00013574635377153755\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00013539401872549205\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00013502266316208988\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.00013460256450343878\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.00013418271148111672\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.00013382872508373113\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.0001335693197324872\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.00013315838121343403\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.0001327894302085042\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00013233962934464216\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00013208923453930765\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00013190312602091582\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00013166910852305591\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.0001314353384077549\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00013110675325151534\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.00013079756929073482\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.0001305080985184759\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.0001301438669906929\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00012988773232791573\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.00012967335991561412\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00012944124639034272\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00012922558118589223\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.0001290622871601954\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.0001289147255010903\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00012879759480711073\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00012876791879534721\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00012866955657955259\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00012846372730564325\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00012838889961130917\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00012834573746658862\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.00012819057039450854\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00012796485621947796\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.00012788700696546583\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.00012780681834556162\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00012773869500961154\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00012767191394232212\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00012761001707985997\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00012755749397911132\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00012751416128594428\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00012748702254612\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.00012745784595608712\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00012742223101668059\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00012738039426039903\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.0001273364608641714\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00012729598733130843\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00012723923719022424\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00012717897712718695\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00012715743447188287\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.0001271510892547667\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.000127149221370928\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.0001270982262212783\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.00012694157077930868\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00012698932318016886\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00012672517972532658\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.0001266787975328043\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00012656190432608127\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00012636624800506978\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00012625554052647203\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00012616333202458918\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.00012609257828444242\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.0001260136830387637\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00012570905615575612\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.0001254824228817597\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00012535877351183445\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00012524921097792685\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.00012514961301349103\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00012506175320595503\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.0001249826920684427\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00012491048430092632\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00012483998027164488\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00012477431388106198\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00012471413589082659\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00012465680192690342\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00012460214435122907\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.000124550627078861\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00012450308364350348\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00012444990570656956\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.0001244134089211002\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.0001243671344127506\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00012433209572918714\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00012428533926140518\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.0001242342102341354\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00012419143051374702\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.0001241508958628401\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00012411307980073616\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00012407501402776687\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00012404111213982106\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00012401239364407958\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.0001240527504705824\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.00012406630645273254\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00012393968150718138\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.0001238979378831573\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00012388698902213947\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00012390709918690845\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00012396254227496682\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.00012384175322949887\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00012378066167002545\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.00012375698250252754\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00012374097597785293\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00012372093595331535\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.0001236955146305263\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.00012367151823127642\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.0001236571485060267\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00012364673224510625\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00012362720357486978\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00012359591171843932\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00012357569154119118\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00012356118095340206\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00012355411367025225\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00012357543280813843\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.0001236633426742628\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00012361835571937264\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.0001235165007528849\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.0001235682689002715\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00012342509668087588\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.00012329851888353004\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.0001232200546655804\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.0001231549622025341\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00012303573865210638\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.0001227022835519165\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00012211555207613855\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00012158704135799781\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.00012109561212128028\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.0001206544425804168\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00012027789140120148\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00011998046247754245\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.00011975076951785013\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00011956029309658334\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.00011940729164052754\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00011927297222428024\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.0001191713020671159\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00011908729851711541\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00011902183323400095\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.00011897755466634407\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00011893160350155085\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.00011889459099620581\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00011889721237821504\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00011880878504598513\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00011869246984133497\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00011862843821290881\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00011858846613904461\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00011856500088470057\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00011852707131765783\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.0001184886324335821\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.00011844399414258077\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00011840526771266013\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.00011836760560981929\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00011833367752842606\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00011831154522951693\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00011828554066596553\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.0001182586871436797\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.00011822546395706012\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00011818901257356628\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00011816356767667458\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.0001181409734999761\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.0001181263048783876\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00011809935502242297\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.00011807630042312667\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00011804036941612139\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.00011801294982433319\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00011799072701251134\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00011796267470344901\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00011795205471571536\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00011793290730565786\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.00011790982476668432\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00011789121228503063\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.00011787389143137261\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00011787061346694827\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00011786381219280884\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00011784474016167223\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00011776882456615567\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00011773403268307448\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00011771440331358463\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00011768374592065812\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00011767560295993463\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00011765336501412093\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00011763682792661711\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00011761948961066082\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00011759901652112603\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00011759426852222531\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00011758527165511623\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00011757494270568714\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00011757052940083667\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.0001175441249506548\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00011753143073292449\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00011751339741749689\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00011748420598451048\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00011746408854378388\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00011744943301891908\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00011743344715796411\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.00011740922083845362\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00011737278924556449\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00011734373489161953\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00011731905076885596\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00011729973688488826\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00011726265598554165\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00011724298063199968\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00011721656832378358\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00011717726069036871\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00011715351545717568\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00011714212916558609\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00011713804531609639\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00011712489911587909\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00011709184618666769\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00011705815995810553\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00011703798052622006\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.00011703117314027623\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00011701589130098\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.00011699354334268719\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00011696472385665402\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00011694805900333449\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.00011692775879055262\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.00011691557709127665\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.00011690098035614938\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.0001168862107442692\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.00011687002843245864\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.00011685597273753956\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00011685339297400787\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00011686590471072123\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00011683665419695899\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.0001168095480534248\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.00011678439535899088\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00011677313625114039\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00011677373317070305\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00011677835456794128\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00011675872810883448\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00011672482127323746\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.00011670933774439618\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00011668526567518712\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00011666786769637838\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00011666090809740126\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.00011666414298815653\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00011666360369417816\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.00011665829864796251\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00011664735822705552\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00011665964586427436\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00011663506244076416\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00011658697098027914\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00011656704125925899\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.00011654881265712902\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00011653747409582138\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00011653070105239749\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00011651811539195478\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00011651383945718408\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.00011651434964733198\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00011649537744233385\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00011650228436337783\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00011650525935692713\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00011649862863123417\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00011650444474071265\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.0001165371437673457\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00011649641150142998\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.0001164192202850245\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00011638950294582174\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00011637194285867735\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00011636412731604651\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.0001163574232487008\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.0001163580792490393\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00011634632479399442\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00011633388843620197\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.00011631852365098894\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00011630835913820192\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00011629887478193269\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00011629088257905095\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.00011628811247646809\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00011627575033344328\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.00011628094565821812\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.00011627310188487172\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.00011626773659372702\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00011624639941146598\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00011623288475675508\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00011622203572187573\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00011621992249274626\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00011620796838542447\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00011620461795246229\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00011619359400356189\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.00011618413438554853\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.00011618146381806583\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00011617473588557913\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00011616679286817088\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00011616720032179728\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.0001161669593420811\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00011616831907304003\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.00011616043950198218\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00011615172174060717\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00011613529117312283\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00011613678536377847\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00011614238319452852\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00011613483249675482\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00011612145521212369\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00011612184927798807\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.0001161198824411258\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00011611409863689914\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00011611404304858297\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00011609528271947056\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00011608720640651882\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00011608048516791314\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.00011607670661760494\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.0001160657693981193\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.00011606248241150751\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00011606351210502908\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00011606124084210024\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00011605360079556703\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00011605912557570264\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.0001160618755966425\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00011608512577367946\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00011609842826146633\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00011609312292421237\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00011609616834903136\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.00011609109526034445\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00011606616288190708\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00011604155472014099\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00011602668091654777\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.00011600845027714968\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00011602893006056548\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.00011599351855693386\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00011596332973567769\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.0001159504964016378\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00011595561983995139\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.0001159639743855223\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00011596344207646325\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.0001159514137543738\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00011592089635087177\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.0001159153482876718\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00011589666944928468\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00011588840221520514\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00011588206485612318\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00011588676483370363\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00011588158609811217\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00011587417771806941\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.0001158706028945744\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00011587028129724785\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00011587564047658816\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00011590585752855986\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00011591722839511931\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00011589773930609226\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00011588847206439823\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00011586851178435609\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00011585795291466639\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00011585074738832191\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00011584385414607823\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.00011584347987081856\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00011585494154132903\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00011584334482904524\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.00011583422048715874\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00011582313396502286\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.000115808836708311\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.00011579019250348211\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00011578123696381226\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00011577691446291283\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.0001157767398399301\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00011577468685572967\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00011576986231375486\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00011577222816413269\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00011578621080843732\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00011576856428291649\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00011575495736906304\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.0001157520484412089\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.0001157544576562941\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00011575785087188706\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00011575108539545908\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00011575022072065621\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.00011575763986911625\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00011576817487366498\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.0001157760905334726\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00011577254656003789\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00011578952748095617\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00011578879988519475\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00011579761281609536\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00011581852682866156\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00011587767105083913\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.0001158249712898396\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00011577089579077437\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00011575141921639442\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00011571871233172715\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.0001157038111705333\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00011569612659513951\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00011569088062969968\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.00011568758520297707\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00011568844289286062\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00011569570051506162\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00011571101669687777\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00011570683534955605\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00011570348200621084\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00011569973081350326\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00011568990506930277\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00011568305373657495\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00011567555979127065\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.00011566787550691515\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00011566543078515678\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00011566256900550797\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.00011565819062525406\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00011565689230337739\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00011565433582291008\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00011565229418920353\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.0001156511448789388\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.00011564952321350574\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00011565230059204623\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.00011564830347197131\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00011567180219572037\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00011566951783606783\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00011566245957510546\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00011565761582460254\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00011565314809558914\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00011564829881535843\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00011563712323550134\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00011562928993953392\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.00011562332656467333\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00011561732913833112\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00011561153049115091\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.00011560453509446234\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.0001155991101404652\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00011559963459149004\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00011560057522729039\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00011560016282601282\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.00011560489598196\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00011561023246031254\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00011560897721210495\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.00011560276296222582\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00011559841193957254\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00011559313716134057\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00011558771977433935\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00011558401252841577\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00011558147292817011\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00011558092955965549\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00011557781661394984\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00011557673802599311\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00011557501537026838\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00011557406338397414\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00011557442543562502\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.00011557560617802664\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00011557550024008379\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00011557427613297478\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00011558047292055562\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00011558785365195945\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.00011559147271327674\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.00011558686790522188\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00011558893893379719\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00011558139260159806\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00011560235114302486\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00011559350445168092\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00011561845982214435\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00011556960875168443\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00011556945304619148\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.00011557481804629787\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.00011558485653949902\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00011560095619643107\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.00011557193676708265\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00011555740842595696\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.00011557179037481546\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00011555691220564768\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.0001155337953241542\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.00011552716110600158\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00011552162643056362\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00011551584029803052\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00011551209317985922\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00011551183182746171\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00011550946976058186\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00011550708906725049\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00011550684255780653\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.0001155048911459744\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00011550304188858717\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.0001155011891387403\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00011550033028470352\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00011549845570698381\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00011549829505383969\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00011550643248483538\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00011550410446943715\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.00011550096794962883\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00011549599439604208\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00011548960232175887\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00011548749345820397\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.00011548570560989901\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00011548410780960694\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.0001154846019926481\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00011549229646334424\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00011548625363502651\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.00011549051589099691\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00011549202346941456\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00011549801827641204\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.00011551825766218826\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00011553982418263331\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.00011551488365512342\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.000115505802386906\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00011551744275493547\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00011551680625416339\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00011547779839020222\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.00011547015456017106\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00011546907277079299\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00011547306959982961\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00011547840113053098\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00011547495989361778\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00011547201953362673\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.0001154691560077481\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.00011546177236596122\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00011545765126356855\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00011545628862222657\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00011545391229446978\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.0001154529329505749\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.00011545505956746638\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00011545741988811643\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00011546200170414522\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00011546865716809407\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.0001154709534603171\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.0001154690160183236\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.0001154628992662765\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.00011545208020834252\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.00011544920562300831\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.0001154477073578164\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00011544912529643626\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00011545235378434881\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.00011544636043254287\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00011544462613528594\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00011544682289240881\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00011545996938366442\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00011545405723154545\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.0001154627138748765\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00011548185168066993\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.0001154951105127111\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00011549202055903151\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00011547508300282061\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.00011548087204573675\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.00011550297669600696\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.00011548908398253843\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00011545415822183713\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.00011545627960003913\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.00011546945461304858\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00011546174617251382\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00011545858578756451\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00011546942609129474\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00011549539980478585\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.0001155368739273399\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00011549193848622963\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00011545959307113663\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00011547639121999964\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00011549879942322149\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00011549851536983624\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.0001154673146083951\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.0001154430492897518\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00011544353212229908\n",
      "Autoencoder12 training DONE... TOTAL TIME = 178.23060703277588\n",
      "start evaluation on test data for Autoencoder12\n",
      "MSE is 0.031494235708879444\n",
      "mutls per sample is 131200\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder13\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.004124105698429048\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.0006239163118880242\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.00043143362388946114\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0004177184810396284\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.00041022040764801207\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.00039625383331440387\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.00037468441179953514\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.00034740972914732994\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.00031136519042775036\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.000269485826138407\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.00023566581134218723\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00021262667374685406\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.00019889074144884944\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00018979748128913342\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00018237384676467628\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.00017543663270771503\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.00016682954330462962\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00015052919334266335\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.00013844616245478391\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.0001342466549249366\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.000131925760069862\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00013017411867622287\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.0001287637371569872\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.0001276381203206256\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.0001267206558259204\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.0001259947387734428\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00012540756375528872\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.00012492759968154131\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00012441623577615245\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.0001239659864222631\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.0001236679672729224\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00012344687973381952\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00012323198781814427\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.00012300035770749674\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00012270281033124775\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.0001222790975589305\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0001214191401959397\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00011983889358816668\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00011801532236859203\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00011658966424874961\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00011571716953767463\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.0001151290425332263\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00011473107937490568\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.00011441547307185829\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00011416683031711727\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00011394621426006779\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00011374261870514602\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00011354244226822629\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00011335066752508282\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00011312007496599108\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00011288103414699435\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.00011261857085628435\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00011233450815780088\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.00011204374604858458\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00011174620711244643\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00011144625197630376\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00011114919529063627\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.0001108556950930506\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.00011056899733375757\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00011028771608835086\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00011003941966919228\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00010982326988596469\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00010961611289530992\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00010943120869342238\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00010926805407507345\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.00010912395518971607\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00010900176479481161\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.00010889278317335993\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.00010882735921768471\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.0001087343125254847\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00010863319737836719\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.00010857140558073297\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00010851217637537048\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.0001084587923833169\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00010841084382263943\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.00010836641391506418\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00010832504456629976\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.00010828487109392881\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.0001082475355360657\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.0001082083591609262\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00010816939116921276\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00010812535707373172\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00010808245307998732\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00010803751531057059\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00010798404313391075\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.0001079290735651739\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00010783304081996902\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.00010774722439236939\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00010764915175968781\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00010752008529379964\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00010736841126345098\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.000107227377302479\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00010707925335736945\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00010693400370655582\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00010679452330805361\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00010666083137039095\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.0001065360542270355\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.0001064280912396498\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00010633156110998243\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00010623780457535758\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00010614226019242779\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00010607255710056052\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00010604932380374521\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.0001059772534063086\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00010593465529382229\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00010588163160718978\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00010583732218947262\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.0001058019811171107\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00010576261294772848\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00010573074599960819\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00010570157115580514\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00010566847398877144\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00010561187285929918\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.00010546814301051199\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00010525084682740271\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00010513663175515831\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.00010506871360121294\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.0001050320322974585\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00010503864788915962\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00010496858623810113\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00010489841137314216\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00010487125837244093\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.00010485083563253283\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00010483123129233718\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00010481428384082392\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00010479809017851949\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00010478346841409803\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00010476871364517137\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.00010475376329850405\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00010473986360011622\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00010472614842001349\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00010471116314874962\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.00010469754139194265\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00010468191612744704\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00010466697509400546\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00010465125000337138\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.0001046369070536457\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.00010462177422596142\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00010460713529027998\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00010459335841005669\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00010457979049533606\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00010456637828610837\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00010456503776367753\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.00010454026429215446\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00010458876611664892\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00010454521281644702\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00010451117588672787\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00010449860157677903\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.00010448603570694104\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00010447145410580561\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00010445803258335218\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00010444568324601278\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00010443141480209305\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.00010444057494169101\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00010442316386615857\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00010441119462484494\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.00010439702164148912\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00010438772413181141\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.0001043824345106259\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00010438702534884214\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00010441370657645166\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.00010434099473059177\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.0001043251997907646\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00010431284899823368\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00010430273658130319\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00010429389687487855\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00010428903187857941\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00010428661946207285\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.0001042999720084481\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00010432378883706405\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.0001042787206824869\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00010424962092656643\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00010421933984616771\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00010420890088425949\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00010420053353300318\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.00010419144702609628\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00010418262041639537\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.0001041749250725843\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.000104168125835713\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00010416158649604767\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00010415445547550916\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.000104145708901342\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00010413586074719206\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.0001041232910938561\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00010411037772428244\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.00010409865441033616\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.00010408580681541934\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00010407476889668033\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00010406401270302013\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00010405370121588931\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.0001040433079469949\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.00010403469757875428\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00010402402782347053\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00010401367151644081\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00010404428147012368\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00010402085434179753\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00010399743536254391\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00010398706421256065\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.000103989647468552\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00010397418751381337\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00010403726017102599\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.00010399831429822371\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.00010396257770480589\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00010395530262030661\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00010395033837994561\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00010394821409136057\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00010395442164735869\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.0001039740111446008\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00010395037999842316\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.00010394068085588515\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00010393749020295218\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00010401480307336896\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00010396001976914704\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00010394711018307134\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.0001039457568549551\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.0001039258053060621\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00010389335860963911\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00010387596528744326\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.00010386586596723646\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00010385592060629278\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00010384527937276289\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.0001038376078940928\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.00010384159395471215\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00010383069049566984\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.00010386160196503625\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00010384619265096261\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00010380189400166273\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00010380235849879682\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00010379201296018437\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00010383590124547481\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00010379079612903297\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00010379552171798424\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.00010379477869719267\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00010378979961387813\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00010378810547990724\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00010377665486885235\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.00010376411228207871\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00010375750804087147\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00010374973877333104\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00010374703968409449\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.0001038325676927343\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00010378259787103162\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00010380451043602079\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.00010373984812758863\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00010374235163908452\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00010373334691394121\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.00010372799937613308\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00010372419405030086\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00010372014134190976\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00010371440264862031\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.00010370990086812526\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.0001037049197475426\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00010374728561146184\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00010372311779065057\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.0001037078324588947\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00010370394214987754\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.0001036950247362256\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00010369066818384454\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00010369191528297961\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00010370874544605612\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00010368097311584279\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00010367199865868315\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00010366446163970976\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.00010366099886596202\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00010365652822656557\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00010365289868786931\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00010365156427724287\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00010365548281697556\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00010366974864155054\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00010366393194999546\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.000103694673452992\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00010366348025854677\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00010367789887823164\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00010368268209276721\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00010367960872827098\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00010364823712734506\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00010364317946368828\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00010365056805312633\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.0001036516172462143\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00010361901338910684\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.00010357798950280995\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00010356733720982447\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.00010356124170357361\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00010355547274230049\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00010354961385019124\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00010354378202464431\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00010354013851610943\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00010353511868743225\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.00010352845565648749\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.0001035229000262916\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00010351714416174219\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00010351115226512774\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00010350548807764427\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00010350130527513102\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00010349673830205575\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.00010349309770390391\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00010349475924158469\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00010349973978009075\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00010349340998800472\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.0001035193307325244\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00010352892568334937\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00010348333715228364\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00010348486917791888\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00010348862939281389\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00010350833152187988\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.0001034988195169717\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00010350723372539506\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.0001035011574276723\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00010352682555094362\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.0001034700355376117\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00010346387949539349\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00010344997514039278\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.00010344750044168904\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00010344687150791287\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.00010343170753913\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00010341164597775787\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00010339309665141627\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00010338096035411581\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00010337350249756128\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00010337641229853034\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.00010336416336940601\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.00010331638448406011\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00010330887540476397\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.00010328908305382356\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.0001032733722240664\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.0001032577344449237\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.0001032447037869133\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00010323418537154794\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.000103271561965812\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00010325127484975383\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00010321716312319041\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.00010318888613255694\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.00010317321663023904\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00010314774874132127\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.00010310750862117857\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.000103089522162918\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00010307847463991493\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00010308749275282025\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00010303782270057127\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00010301418718881905\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00010300315771019086\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00010298313369276001\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00010300463560270146\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.00010299114685039967\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00010292668885085732\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.00010298999171936884\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00010291391634382308\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00010287884448189289\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00010285861499141902\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00010284348507411779\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00010291498678270727\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.0001028617934207432\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.00010281112132361159\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.0001027939299819991\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.0001027790829539299\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00010279164125677198\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.0001027543202508241\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00010272850573528558\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.00010271387174725532\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00010270319558912889\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.00010274619533447549\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.0001026988567900844\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00010267011093674227\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00010265104763675481\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00010264394368277862\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00010267454577842727\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00010264603421092033\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00010261678864480927\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00010259719769237563\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00010259356291498989\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00010260242386721074\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00010259593633236363\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00010261438845191151\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.00010257348068989813\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.00010254147171508521\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.0001025259229936637\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.00010251836822135374\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00010250018967781216\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.00010256004956318066\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00010249677405226975\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00010247958794934675\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00010245963116176427\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.0001024438432068564\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00010242649703286589\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00010241826937999576\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.00010243388649541885\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00010240006056847051\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00010239840252324938\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00010239496477879584\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00010240793257253245\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.0001023977147997357\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00010236038448056206\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.00010233355860691517\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00010232141765300185\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.00010231139196548611\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00010238004731945694\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.00010232284548692406\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00010229990002699196\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00010230354266241193\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00010231719847070053\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00010229497216641903\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.00010224472905974836\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00010222559329122305\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00010221895150607452\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00010221264441497625\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00010220209078397602\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00010218946175882593\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.00010218095849268138\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.0001021771421073936\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00010221389937214553\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00010216486029094085\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00010217268485575915\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00010218163894023746\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00010217409784672782\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00010213030909653754\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.00010212212218903005\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.00010212975146714598\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00010211180197075009\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00010218160547083244\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00010212890629190952\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.0001021042637876235\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.00010209127562120557\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00010208894964307547\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00010210040607489646\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00010214992769761011\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00010208689607679844\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00010207576298853382\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00010208009858615696\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00010208283638348802\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.00010205808939645067\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00010203292855294421\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00010206491278950126\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00010220788070000708\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00010203397716395556\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.00010200781165622175\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00010200112068559974\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00010199665208347141\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00010199190699495376\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00010199028183706104\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00010199515148997307\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00010202962381299586\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00010199708805885165\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00010200898366747425\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00010198651143582538\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.00010198299103649333\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.00010196604882366955\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00010203566431300715\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00010199687734711915\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.0001019732709391974\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00010195800947258248\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00010195689334068448\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.0001019576852559112\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00010201836936175823\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.0001019566977629438\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.0001019484328571707\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00010195334354648366\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00010196879971772432\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00010199642361840234\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00010197142342804\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00010193770256591961\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00010192837216891349\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00010192726884270087\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00010193885827902704\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.00010199260694207624\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.0001019221343449317\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.00010194737318670377\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00010195113514782861\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.0001019899154198356\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.0001020074964617379\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00010194320988375694\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00010191358654992655\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00010191018460318446\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00010190916567808017\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.0001019095748779364\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00010191106557613238\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.00010191196575760841\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.00010191456269240007\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.00010192082409048453\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.00010193314083153381\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.00010195157286943868\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00010194157104706392\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00010192748857662081\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.0001019254422863014\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00010193607391556725\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00010195422306424007\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00010194678121479228\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00010192855552304536\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00010192613466642797\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00010193470952799544\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00010194669885095209\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.00010196381626883521\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.0001019747109967284\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.00010198578384006396\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.0001020472389063798\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.00010201055847574026\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00010200258722761646\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00010197648836765438\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00010198308009421453\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00010198803269304335\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00010195483337156474\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.00010191358276642859\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00010191315144766123\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00010189894179347902\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00010187479114392772\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00010187432228121907\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00010186822153627872\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00010191228560870514\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00010186903149588034\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.00010185277118580415\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.00010185811610426754\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00010192440124228596\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.00010184742772253231\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00010181758960243315\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00010181071527767927\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.0001018025740631856\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00010179967852309346\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00010179266799241304\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00010187051229877397\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.0001018203780404292\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.0001017913845134899\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.00010178719792747871\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00010178197611821815\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00010178356315009296\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00010183370497543365\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00010180322016822174\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.00010179543547565117\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00010177958698477596\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00010177657415624708\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00010178135387832299\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00010179437813349069\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00010176044539548456\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.0001017425284953788\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.0001017365456209518\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00010174375463975594\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00010177818941883743\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00010175129893468692\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00010177773190662265\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.00010177466494496911\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00010175359580898658\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00010172904876526445\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00010171784524573013\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.00010171178058953956\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00010170792433200404\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00010170485475100577\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00010170160414418205\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00010171911475481465\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00010170821245992556\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00010173946939175948\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.00010170659370487556\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.00010169177228817716\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00010169877670705318\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00010171920323045924\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00010174104856560007\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00010172510606935247\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00010171254951274022\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.00010168318898649886\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00010167218395508826\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.00010166715655941516\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00010180111334193498\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00010168297216296195\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00010167196625843645\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.00010166653228225187\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.00010166412364924326\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00010166663327254355\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.0001016582065494731\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00010166739608393982\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00010171644564252347\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00010167351138079539\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00010166029416723176\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.0001016517067910172\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00010165060171857476\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00010165727755520493\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00010169355489779264\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00010168549633817747\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00010167941596591845\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.00010167086409637705\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00010164851031731815\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.00010163919650949538\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00010164114471990616\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.0001016475589131005\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00010166715073864908\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00010164722916670144\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.0001016537012765184\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00010165574407437816\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00010166718013351783\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.0001016878805239685\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.00010171101021114737\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00010167694155825302\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.0001016387838171795\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00010163755650864914\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.00010163670114707202\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.0001016400137450546\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00010164750157855451\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.00010166554828174413\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00010165684157982469\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.00010162777092773468\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00010162608523387462\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00010160705947782844\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00010159680095966905\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00010159529309021309\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00010159275901969523\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00010158832068555057\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00010158383433008567\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.00010160856210859492\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.00010159607656532898\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00010159960627788678\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.0001016226047067903\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.000101589341647923\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00010159459197893739\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00010159142344491556\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.00010160483856452629\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.00010161047917790711\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.0001016193866962567\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.0001015859714243561\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.0001015691165230237\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00010156759759411216\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00010162950697122142\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.0001015744570759125\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00010156823962461204\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.0001015608524903655\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00010156439762795344\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00010155546944588422\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00010161751211853698\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00010157663375139237\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.00010155295371077955\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00010155179596040398\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.00010155206662602722\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.0001015481274225749\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00010157069627894088\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00010160121921217069\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00010159235447645188\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00010158595541724935\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00010162159655010328\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00010159061144804582\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00010154893388971686\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00010155226016649976\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00010154982417589054\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00010154979448998347\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00010157337470445782\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00010155667463550344\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00010154476360185071\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00010152951028430835\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00010152228613151237\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00010151734517421573\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.0001015777513384819\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00010154843708733097\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00010152324917726218\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00010151594207854941\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00010151442489586771\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.000101516182476189\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.00010150395799428224\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00010156952426768839\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00010152900649700314\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00010151268157642335\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00010150253598112613\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00010150336078368127\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00010150520101888105\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00010155164520256221\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00010152685164939612\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00010150458372663706\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00010149691457627342\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00010149270470719784\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00010148936678888276\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.0001015518291387707\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00010151423804927618\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00010153734881896526\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.0001015298254787922\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00010148700006539002\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.00010149050271138549\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00010157123382668942\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00010152812639717012\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.00010150787071324885\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.00010151769733056426\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.00010150975867873058\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00010150127433007583\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.00010149755718884989\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.00010151158814551308\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00010150306712603197\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00010149257810553536\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00010154606366995723\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.00010152234986890107\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.00010149547888431698\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00010151026450330391\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00010152510658372194\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00010151237976970151\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00010148825705982745\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00010147086897632106\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.00010146967950277031\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00010148317203857005\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.0001014999853214249\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00010148666944587604\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.00010145268519409001\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00010144571104319766\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.0001014428137568757\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00010143876133952289\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.0001014375634258613\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00010143695079023018\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00010143372986931354\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00010150709567824379\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.00010144660860532895\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.0001014291838509962\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00010142596467630937\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00010141556325834245\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00010140467580640688\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.00010139807622181252\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00010139673220692202\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.00010142814047867433\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00010140739206690341\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00010141522448975593\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00010139801655896008\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00010138595185708254\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00010137966746697203\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.00010138376441318542\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00010141105158254505\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00010137904726434499\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00010137234465219081\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00010136140946997329\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00010137049219338223\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00010141123697394505\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00010137916542589665\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.00010137661040062085\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00010137043747818097\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00010138475394342095\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00010140491765923798\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.00010135409509530291\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00010135809221537783\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.00010139153630007059\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.00010135032760445029\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.00010134667594684288\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00010133962146937847\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00010133218369446694\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00010140672733541578\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00010135401011211798\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00010134570096852258\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00010132744122529403\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00010132976109161973\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.00010134964424651115\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.00010137041768757626\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00010136574011994526\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00010134361538803205\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.0001013275294099003\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.0001013320844504051\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00010134413576452062\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.0001013661830802448\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00010136021417565644\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00010133541276445612\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00010132018651347608\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.0001013152115046978\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00010134441079571843\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00010132795287063346\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00010132926283404231\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.00010133818868780508\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00010134870128240436\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00010132512485142797\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00010130711481906474\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00010129729489563032\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00010129184956895188\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.0001014200851204805\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00010132782015716657\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.00010130217968253419\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00010129800008144229\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00010129458794835955\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00010129510192200542\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00010129430913366377\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.00010130432987352833\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00010130696813575923\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00010130846610991285\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00010131671355338767\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00010135398013517261\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.0001013585936743766\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00010131783987162635\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00010129761241842061\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00010130142938578501\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.00010129516798770056\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00010130504233529791\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.00010140191501704976\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00010132579976925627\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.00010129336093086749\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00010128820402314886\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00010128655791049823\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00010128744441317394\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00010129010508535429\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00010129351489013061\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00010130271897651255\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00010129097383469343\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00010130150418262929\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00010130849725101143\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00010129656875506044\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00010129515052540228\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00010130718466825783\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.0001013596827397123\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00010132563184015453\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.0001013113118824549\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00010132814088137821\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00010128776513738557\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00010125949076609686\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.0001012637501116842\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00010125701664946974\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00010130927403224632\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00010125770815648138\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00010125033440999687\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.00010124691820237786\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00010124562453711405\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00010129851551027969\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.00010125830915058031\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00010124797176104038\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.0001012383759371005\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.00010123793763341382\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00010126635752385482\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00010124482738319784\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.0001012529176659882\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00010125855798833072\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00010126097622560337\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00010125366563443094\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00010123528045369312\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00010123624640982598\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.0001012544531840831\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.00010123818152351305\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.00010123886546352879\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.0001012484446982853\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00010126031585969031\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00010125281231012196\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.00010126426233910024\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00010128121881280095\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.00010123318614205346\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00010122425766894593\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00010124566208105534\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00010123678541276604\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00010123734944500029\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00010124647291377186\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00010123418236616999\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00010124284744961187\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00010123236657818779\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.0001012301939772442\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00010123486077645794\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.00010123476648004725\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00010124883177923039\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00010128607187652961\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.00010126846987986937\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00010122948355274274\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00010122455802047625\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00010122015461092814\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00010122749488800765\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.0001012447476387024\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.0001012211458873935\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00010121890605660155\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00010122965468326584\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00010127171262865886\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.00010123946849489584\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00010123563319211826\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00010124544409336523\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.00010126322857104242\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00010128656664164737\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.0001012817895389162\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00010124477092176675\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.0001012332137906924\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.00010123278305400163\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00010123424552148208\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.00010123720043338836\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00010124053515028208\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00010123584477696568\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00010123797546839342\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00010123888932866976\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00010124080843525007\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00010124829539563507\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00010125794651685282\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00010126757086254657\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.000101263482356444\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00010125199769390747\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00010125371831236407\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.00010126351524377242\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00010127026063855738\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00010125908651389182\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.0001012287664343603\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.0001012239049305208\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.00010122678475454449\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00010123242507688702\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00010123672720510512\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.0001012335906852968\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.0001012272437219508\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00010123022511834278\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00010125423315912485\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00010127821064088493\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00010124292602995411\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00010122188541572541\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00010122298612259328\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00010122709471033887\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00010122823587153107\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00010123298445250839\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00010124057356733829\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.00010124539316166193\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00010123734566150233\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00010122312349267304\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00010121269908268004\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00010120804363396018\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.00010120771097717807\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.00010121958126546815\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00010123490501428023\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00010122075502295047\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.0001012171633192338\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00010121629224158823\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00010122532112291082\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00010122541105374694\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00010122778039658442\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.00010123539861524478\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.00010125925356987864\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00010123084735823796\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.0001012282312149182\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00010123203275725246\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.0001012334183906205\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00010122817242518067\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.0001012270973296836\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.00010135629388969392\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00010133085073903203\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00010123160056537017\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00010122020583366975\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.0001012168254237622\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.0001012168632587418\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.000101216672337614\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00010123794520040974\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00010121448576683179\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00010122187930392102\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.00010120416874997317\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00010120202088728547\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00010120326682226732\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00010122211271664128\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00010123692016350105\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00010123387794010341\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.0001012225731392391\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00010119417391251772\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00010118766134837642\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00010118849982973188\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.00010123645392013713\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00010118775302544237\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.00010117740312125534\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00010117384750628844\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00010117027093656361\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.00010117082070792094\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00010117161757079885\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00010122618987224996\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.00010118445701664314\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00010116502264281735\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.00010116404795553536\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00010116238088812679\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00010116169403772801\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00010120812861714512\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00010119808313902468\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.00010120533348526805\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00010116584628121927\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00010116102290339767\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00010115901968674735\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00010115280805621296\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.00010121035476913676\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.0001011803510482423\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.0001011705445125699\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00010118645412148907\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00010121443425305188\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.0001012431300478056\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00010119996615685522\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.0001011536386795342\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00010115409502759576\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00010115073702763766\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00010114579956280067\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00010114322736626491\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00010114326636539772\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00010114048462128266\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.00010113829543115571\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.00010113525844644755\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.00010113177704624831\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00010116026911418885\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00010113335010828451\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.00010114052536664531\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00010113753902260214\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00010114712262293324\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00010116028570337222\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.00010114042088389396\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00010112489311723039\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00010112492163898424\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.00010112663585459813\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.00010112829215358943\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00010117529862327501\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.00010116391786141321\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.00010114828852238133\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.00010113771131727844\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00010114135104231536\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.00010114854434505105\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.00010116987366927788\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00010118006292032079\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00010114516917383298\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00010113757569342852\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00010114497999893501\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.00010114444681676105\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00010113736265338957\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00010113806638401002\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00010113001859281212\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00010115185636095702\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00010117524507222698\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.0001011383833247237\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.00010114163858816028\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00010114605771377683\n",
      "Autoencoder13 training DONE... TOTAL TIME = 201.1874225139618\n",
      "start evaluation on test data for Autoencoder13\n",
      "MSE is 0.0035323940070999525\n",
      "mutls per sample is 266496\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder14\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.020780563354492188\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.01176539495587349\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.006003518532961607\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0026449501141905785\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.00114451858215034\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.0006377403624355793\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.0004880712286103517\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.0004430570616386831\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.00042833505780436097\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.00042299431283026933\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.00042080084909684956\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.0004196855297777802\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.0004189239616971463\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.0004182232078164816\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.00041742020286619666\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.00041639007511548695\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.0004150092520285398\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.00041310427710413935\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.0004104600625578314\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00040690121124498547\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.00040232779923826456\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.000396750575164333\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.0003903414215892553\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.00038341165287420154\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.0003762924124021083\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.00036924086278304456\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00036243528476916255\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.00035598089336417614\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.0003499257785733789\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00034428062732331453\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.0003390333720017225\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00033416004152968525\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.0003296296508051455\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.0003254084812942892\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.0003214627387933433\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.0003177595674060285\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0003142684686463326\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.00031096054241061213\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00030780912842601535\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.00030478982720524075\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00030188084463588893\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.000299062798731029\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.00029631832614541054\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.0002936326141934842\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.000290993646485731\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00028839276637881994\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.0002858232450671494\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.000283279464347288\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.000280757921282202\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.0002782566344831139\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.0002757750102318823\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.0002733132743742317\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.00027087227324955164\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.00026845299289561807\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00026605709572322664\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.0002636864292435348\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00026134238578379154\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.0002590268617495894\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.0002567407500464469\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.00025448543368838727\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.00025226195342838766\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00025007021031342447\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00024791369680315255\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00024578781856689604\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.00024369612568989396\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.0002416435326449573\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00023961835191585123\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.0002376292337430641\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.0002356751885963604\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.00023376195225864648\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00023187491984572262\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.0002300224971259013\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.00022820762416813522\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.0002264279656810686\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00022468533890787512\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.0002229711483232677\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00022129721066448837\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.0002196563925826922\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.00021804797579534353\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00021648300695233048\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00021494214714039118\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.00021344136097468437\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.00021197677357122303\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.00021054496348369868\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00020915189990773796\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.00020779353042598813\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00020646255929023026\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.0002051695459522307\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00020391680358443408\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00020268672262318432\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00020149618329014628\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.00020033854583743958\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.0001992135780164972\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00019812808954156936\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.0001970550447003916\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00019600277533754707\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00019486984703689814\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00019240932073444128\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.0001864634599769488\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00017792122962418945\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00016888378479052335\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.0001621302036801353\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.0001584749756148085\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.00015667964238673447\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.00015569095558021218\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00015506308525800705\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00015460561960935592\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00015424403361976146\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00015394215821288527\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.00015368140069767832\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.00015345136285759508\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00015324541891459376\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00015305956127122046\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.0001528909127227962\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.0001527371641714126\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.0001525965251494199\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.0001524680497823283\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.0001523498579626903\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.0001522412052145228\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.00015214097744319587\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00015204852621536702\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.0001519635255681351\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.00015188480843789876\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00015181171125732361\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.0001517441333271563\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.0001516814745264128\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.0001516222854843363\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00015156723326072098\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.00015151655185036361\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00015146673074923456\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00015141980547923596\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.00015137460082769393\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.0001513276802143082\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.000151282183942385\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.0001512389024719596\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.00015119723160751165\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00015115596062969416\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.00015111522690858692\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00015107484941836446\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.0001510342606343329\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.0001509934541536495\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00015095194859895855\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00015090981323737652\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.0001508669659961015\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.0001508229790488258\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.0001507755706552416\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.0001507265557302162\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00015067457454279065\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.0001506211649393663\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00015056265459861606\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00015049997775349767\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.00015043462859466673\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00015036650991532953\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.0001502985047409311\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00015022503735963256\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00015013691911008208\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.0001500560832209885\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00014995357836596667\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00014985141635406762\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00014974335266742855\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00014962800312787294\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.0001495037047425285\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.00014937223109882326\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.0001492344617145136\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.0001490880601340905\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.0001489309227326885\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00014876851870212704\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00014860204479191452\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00014842572738416493\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00014824303274508567\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00014806068036705256\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00014786212414037437\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.0001476541056763381\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00014744190266355873\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00014722419902682304\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.0001470024255104363\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.0001467766339192167\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.00014653639402240515\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00014628768432885407\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.00014604217605665326\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00014577717636711894\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.00014549732382874937\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00014522679150104522\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00014494835573714227\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.0001446669513825327\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.00014437781646847725\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.0001440871541853994\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.0001437996089225635\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00014349961013067513\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.0001431970449630171\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.0001428993308218196\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.0001425948488758877\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00014229267719201744\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00014200772042386234\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.0001416651427280158\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00014133661170490086\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00014101841545198114\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00014070037985220552\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.0001403837598627433\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00014006812998559325\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00013975135108921678\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.000139434005250223\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.0001391170109855011\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.000138796305982396\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.0001384572306415066\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00013810905278660357\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.00013778130582068116\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.0001374556211521849\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.0001371357007883489\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.0001368072663899511\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.00013648996828123926\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00013618753117043526\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00013588064815849064\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00013558726175688207\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00013529362739063798\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.0001349959790240973\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00013471777725499124\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00013444189331494273\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.0001341720373602584\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00013390033564064652\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.00013364462705794722\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00013338461169041694\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.000133131041075103\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.00013289222959429026\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.0001326707418775186\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00013245704933069645\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.0001322474121116102\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00013203791342675686\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.0001318479870678857\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00013167303754016757\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00013150968472473324\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00013135628250893205\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.00013120959454681724\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00013107243343256413\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.00013094212859869004\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00013081959390547125\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.000130703731556423\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.0001305947813671082\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.000130489191506058\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.0001303898572223261\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.0001302990864496678\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00013021172489970922\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00013013079296797514\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.00013005880580749363\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.00012998726277146488\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00012992143863812088\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.0001298619172303006\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00012980318220797926\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.00012975231453310698\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00012970369018148632\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.0001296578999608755\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00012961748405359685\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.00012957153317984195\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00012953383207786828\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00012949626368936152\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.0001294614525977522\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.00012943137844558805\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00012940681423060597\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.00012938720115926116\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.0001293783698929474\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.0001293505192734301\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.0001292992220260203\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00012927635572850704\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.00012925539922434837\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.000129237737855874\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.0001292210246901959\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.00012920434703119098\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.0001291891181608662\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.0001291759154992178\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.00012916266277898102\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00012914967432152479\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.0001291360385948792\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.0001291235798271373\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00012911114085000007\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00012909932935144751\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.000129088400863111\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.0001290786685422063\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00012906907708384096\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.00012905960669741035\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00012904936156701296\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.00012904238246846944\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.000129033992998302\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.00012902566289994864\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.0001290187449194491\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.00012901405396405606\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.00012901039444841446\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.0001290050457464531\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.00012899753462988883\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.00012899171444587408\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.00012898638262413443\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00012898215616587548\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.000128977345302701\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.00012897170789074152\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.0001289664173964411\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00012896134227048607\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.0001289559144061059\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.00012894738058093935\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.00012893874081782998\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.0001289297401672229\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00012892388156615197\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00012892136292066425\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.000128921055002138\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00012891874241176992\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00012891793681774289\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00012891657650470733\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.00012891625927295537\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00012892257422208786\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00012894685263745487\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00012891661084722726\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00012890428595710546\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00012889870209619402\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00012889519683085383\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.0001288923469837755\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.0001288896199548617\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.0001288869243580848\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.00012888458557426928\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.0001288823800859973\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.0001288799662142992\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00012887815304566175\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00012887562974356115\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.0001288746017962694\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.00012887304939795285\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.00012887134333141147\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.00012886901327874512\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00012886622454971074\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.000128863463178277\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.00012886053475085645\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00012885679316241295\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00012885340722277761\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00012885141768492758\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00012884850904811172\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.0001288472069427371\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.00012884668598417192\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.00012884710216894745\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.0001288477925118059\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00012885293399449437\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.00012886449287179857\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00012885942414868622\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00012884503172244877\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00012884817144367844\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.0001288559491513297\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.00012886047072242944\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00012884710857179017\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.0001288472954183817\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.00012885002535767854\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.00012884368654340507\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.00012883428251370788\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00012882903567515314\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00012882802926469594\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00012882780458312482\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00012882750714197756\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.0001288269757060334\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.00012882637500297278\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00012882633542176335\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.0001288263703463599\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00012882644427008927\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.00012882615090347826\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00012882624403573573\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.0001288270519580692\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.0001288288237992674\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.00012882923998404296\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.0001288326981011778\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.00012883389659691603\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.0001288125931750983\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00012859978480264544\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00012840623036026956\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00012828020029701293\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.000128183780470863\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00012813391163945197\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00012806617480237036\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00012801395438145847\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00012797711999155582\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00012796289869584142\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.0001279339729808271\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.00012790112872608007\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00012788948952220381\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.00012788088235538453\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00012787258077878505\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.00012786517909262328\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00012785735132638364\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00012785045022610574\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00012784515449311583\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.00012783889251295478\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.0001278395246481523\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00012784325517714023\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.0001278002036269754\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00012777710217051207\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.0001277767156716436\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00012774538714438677\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.00012768489948939533\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00012767245003487916\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.00012764245097059756\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.00012762804341036826\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00012759032426401973\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.00012757402670104056\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00012755696836393325\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.0001275287166936323\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00012753957242239266\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.0001275212236214429\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.00012748210632707925\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00012746464693918824\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.0001274614076828584\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00012746299558784812\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00012740949692670257\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00012738416320644318\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00012737042794469744\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00012735960131976754\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.00012734746327623724\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.00012733916053548456\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00012733866285998373\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00012734392832498996\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00012731760856695472\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.0001273335481528193\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00012729009671602398\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00012729045178275556\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.0001273024920374155\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.00012728846457321196\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.00012721536273602396\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00012711235554888845\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.0001270437048515305\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00012700402934569865\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.00012697343539912252\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00012694356846623122\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00012696591555140912\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00012693124008364976\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00012689088587649166\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00012687177280895412\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00012685837980825455\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.00012684365909080954\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.00012683533714152873\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.0001268897991394624\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.0001267719961469993\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00012676811194978655\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.00012674716883338987\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.0001267141819698736\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00012668027949985116\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00012665735906921326\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.0001266390533419326\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.00012662821274716406\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.0001266152976313606\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00012659764266572892\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00012666239053942262\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00012659365427680313\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00012660060136113316\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.00012657424493227155\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.00012651866127271204\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.0001264981913845986\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.0001264878874644637\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.00012648325937334449\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00012646992807276546\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00012646692804992198\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.0001264592830557376\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00012646398507058622\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.0001264661888126284\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.00012645506532862782\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00012645270035136492\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00012645432958379387\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00012642224843148142\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00012639401946216823\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00012638482672628015\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00012638510088436306\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.000126380063011311\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.00012637978652492167\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.00012637365492992102\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.000126423598267138\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.00012641629844438286\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00012635393592063337\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.0001263520709471777\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.0001263439084868878\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00012633709760848433\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00012633512378670275\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00012635597202461212\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00012633162958081812\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.0001263209234457463\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00012633079371880739\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.0001263002509949729\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.0001263073628069833\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.000126311537460424\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.0001263552304590121\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.00012629632605239748\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00012625811446923763\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00012625025818124413\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00012624853989109396\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00012624116148799659\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00012623033253476024\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00012622248032130302\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00012621717643924058\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.00012621127476450056\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00012620558787602933\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00012619878631085158\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.0001261944166617468\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00012619777466170488\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.00012619988119695337\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.0001261836144840345\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.0001262694539036602\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00012622804671991617\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.000126218184013851\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00012617789208889009\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.00012615347397513689\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.0001261481421533972\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.0001261435786727816\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00012614164093974978\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.00012614168401341885\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.00012615170737262816\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.0001261703809723258\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00012618256034329533\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00012615590880159287\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00012613303551916032\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.00012613861763384194\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.0001261419733054936\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00012613931437954306\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.00012615097046364098\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00012616717081982644\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00012613284518010913\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00012610845267772676\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00012610724370460958\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00012610416044481099\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00012609931523911656\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00012609963479917496\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.0001260887592798099\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.0001261228631483391\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.0001261270744726062\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00012613510247319937\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00012610690610017627\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00012611658195964991\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.0001260758104035631\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00012606611242517828\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00012606121192220598\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.0001260576961794868\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.0001260573387844488\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00012605400464963168\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.00012605503085069358\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00012605859781615437\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00012608110788278281\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.00012612198595888914\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00012607361830305308\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00012609472556505353\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.00012608234421350063\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.0001260499731870368\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00012604179326444864\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00012604036019183695\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.00012604021583683788\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00012604350748006256\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00012604394345544278\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00012604311748873443\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00012604112213011832\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00012604170013219118\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.00012604748306330294\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.0001261330727720633\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.0001260689174523577\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00012607454555109144\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00012606782431248575\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.0001260676037054509\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00012605137948412448\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00012604692950844763\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.00012604717572685332\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00012604585906956345\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.0001260467735119164\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.00012605361989699305\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.00012605683645233512\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00012606913165654987\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.0001260856696171686\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.00012606098433025182\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00012600718066096305\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00012599925801623613\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.0001259913749527186\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00012598456407431514\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00012597894645296036\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.000125976259005256\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00012597516062669455\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00012597552151419222\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00012600773887243123\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.0001260447537060827\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.0001259726088028401\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00012599456240423024\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.00012600234593264758\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00012602240836713464\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.0001260280719725415\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00012596726126503198\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.00012596141023095696\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.00012595632520969957\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00012595240958034992\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.00012595035950653256\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00012594915286172183\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.00012595016916748137\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.0001259519613813609\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.0001259486860362813\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.000126098922919482\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.00012599072069860995\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.00012598465953487904\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.00012594730185810476\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.0001259445591131225\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00012594211963005365\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.00012593864055816085\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00012593563296832145\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.00012593452527653425\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.00012593413761351256\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00012593703053425997\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00012595004227478057\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00012604800460394472\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00012595728563610464\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.0001259729074081406\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00012597900931723415\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.00012595044856425375\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.00012594064115546645\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00012593693274538963\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.0001259330118773505\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00012593365856446325\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00012592978950124234\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00012592490995302796\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.000125929742353037\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.0001259479229338467\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00012598528468515724\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00012594897765666246\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.00012597895518410952\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.0001259740930981934\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00012593271501827984\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.00012592226208653302\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00012592435465194285\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.0001259182847570628\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00012590826372615993\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.0001259146904340014\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00012591229169629515\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.00012590782134793698\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.00012590768572408705\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00012590365950018167\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.0001259114796994254\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00012590482481755316\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00012603647832293063\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.00012593723135069014\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00012594722094945608\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.00012591457867529245\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00012590744358021767\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00012590792030096055\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.0001259047072380781\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.00012590772239491345\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00012591009319294245\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00012591286445967853\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.0001259170222328976\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.00012592231621965766\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00012594929488841445\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.00012598641857039183\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.00012598814675584435\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.0001259683829266578\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00012598557746969165\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.00012600163870956748\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00012604886549524963\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.00012608082906808704\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00012606193835381418\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00012607468815986067\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.0001260418730089441\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.0001259645033860579\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.00012591793842148036\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00012589717167429626\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00012588395329657943\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.00012588925543241203\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00012587618664838373\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00012587555858772247\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.00012589095218572765\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00012594966683536767\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00012589610822033137\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00012586851546075194\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.00012586735130753367\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.0001258597505511716\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00012586246302817016\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00012589131947606801\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.0001259381865384057\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00012587285309564322\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.00012587597593665122\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00012589464196935296\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.00012588395446073265\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.0001258646254427731\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.00012585871736519038\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.00012584527779836208\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00012584053096361458\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.0001258430868620053\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.00012583676143549383\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00012583538540638983\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00012583464151248337\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00012583600415382533\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.00012586391647346318\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.00012589418271090834\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.0001258493959903717\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.00012588641082402318\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.0001258810848230496\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00012584111012984068\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.000125835970393382\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.00012582046387251466\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00012581998948007822\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00012581253075040876\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.0001258119649719447\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.0001258123479783535\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00012580854527186603\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.0001258065161528066\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00012581729446537794\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00012583021773025394\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00012591421138495208\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00012582544120959938\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00012583621020894497\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.00012583086267113684\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00012580708134919406\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.0001258074352517724\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00012580705573782323\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00012579698173794895\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.00012579894450027496\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00012580586946569382\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.0001258148648776114\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00012583055708091705\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00012583530391566456\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00012579952890519054\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00012579519476275892\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00012579464411828666\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.00012579227157402784\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00012578723835758864\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.00012578917667269707\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.0001258086698362604\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00012582626135554164\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00012582050927449019\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.00012585716729518026\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00012582134106196464\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.00012581556162331254\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00012579424190334976\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00012580411101225763\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00012579383852425962\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.00012579982401803136\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00012580976530443877\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.0001257965061813593\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.0001257469819393009\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.0001257314649410546\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00012572820531204343\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.0001257204223657027\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00012570091581437737\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00012567991332616658\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.00012565676996018738\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00012562587449792772\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00012558447022456676\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.0001255149906501174\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.0001253538846503943\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00012517792056314646\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00012484559731092303\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00012442609062418341\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.00012393823155434802\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00012337354710325598\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.0001228149514645338\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00012220555217936634\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00012163130450062453\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00012119058606913313\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00012071093748090789\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00012028560129692778\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00011989762773737312\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00011962393560679629\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.00011939947609789669\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00011923254962312058\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00011910898523638024\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00011899300763616338\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.00011890277557540684\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00011883060418767855\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.00011877984565217048\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00011872834584210068\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.00011867980618262663\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.00011864636297104881\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00011861600301926956\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00011858162702992559\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00011854894342832268\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.00011851622955873609\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00011849814734887331\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00011846004548715427\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00011843948101159185\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00011845024244394153\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.00011843116022646428\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.00011840682185720652\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.0001183861299068667\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.000118376559112221\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.0001183949809637852\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00011836943740490824\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.00011831693642307073\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00011829640803625807\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.0001182848055032082\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00011827316280687228\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00011826123110949993\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00011824696324765683\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.00011823612177977338\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.00011823872424429282\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00011824751185486093\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.00011822026513982564\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00011819190811365843\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00011817026213975623\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00011815631936769933\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00011823997745523229\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00011817075399449095\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.00011810836440417916\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00011807797272922471\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.0001180642656981945\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00011804181180195883\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00011803091911133379\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00011804094421677291\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00011800940817920491\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.0001180506614036858\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00011799794127000496\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00011796984064858406\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00011795631202403456\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.00011796320410212502\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00011796253296779469\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00011795486730989068\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.00011793252371717245\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00011791705794166773\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00011792689969297498\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.00011790864227805287\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00011786381277488545\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00011784773931140081\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.00011783901980379597\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00011783018242567778\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00011782304034568369\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00011781515670008957\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00011781472654547543\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.00011782002460677177\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00011782560031861066\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.00011779088235925883\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.00011775478895287961\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00011773765872931108\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00011772581841796636\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00011771489313105122\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.00011770386743592098\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00011769838980399072\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.00011768168566050008\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00011767393705667928\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00011769791104597971\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00011770270182751119\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00011763758375309408\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00011763469345169142\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00011762522801291198\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00011760514287743717\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00011763282091123984\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00011762609530705958\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00011757356638554484\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.00011756517225876451\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00011755256360629574\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.00011753688158933073\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.00011753665865398943\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00011752613383578136\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00011752216174500063\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00011752358608646318\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00011752214370062574\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.00011750043253414333\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00011746026633772999\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00011744223098503426\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.00011742113361833618\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00011740679736249148\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.000117390695377253\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00011737814405933022\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00011736471060430631\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.00011735145904822276\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00011733485269360244\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00011732018843758851\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.00011730471276678146\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.0001172887408756651\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.00011728186014806851\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.00011735603475244716\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.00011725579184712842\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00011724273848813027\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00011720544251147657\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00011718762834789231\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.0001171706456807442\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.00011714981956174597\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00011713992338627577\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.0001171290484489873\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00011710346268955618\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.00011706721415976063\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00011704600125085562\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.00011702937830705195\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.00011700894479872659\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.0001169823034433648\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00011697210604324937\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.0001169502895209007\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00011693133623339236\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.00011693080217810348\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00011689747596392408\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00011683602147968486\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.0001168252743082121\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00011679942923365161\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00011673757311655209\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.00011669770523440093\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00011666202219203115\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.00011661797267151996\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00011657387396553531\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.00011653473338810728\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.00011649340507574379\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00011645061225863173\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.00011640647862805053\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00011636224400717764\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.0001163237661239691\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00011627880769083277\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00011623341822996735\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.00011620708392001688\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.00011615629860898479\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.00011611705558607354\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.00011611791647737846\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.00011607584572630003\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00011598979588598013\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00011600924248341472\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.00011588129884330555\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00011576756485737861\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00011570943286642432\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00011564489308511838\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.00011549901275429874\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.00011499354935949668\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00011447494121966883\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.0001141754409763962\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00011388710263418033\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.00011362047371221706\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.00011338383541442454\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.00011315810435917228\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.00011293835035758093\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00011271575756836682\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00011244940775213763\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.00011213537305593491\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00011186066752998158\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00011167663789819926\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00011150773381814361\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00011138835980091244\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.00011123552860226483\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00011107904836535454\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.00011100908712251112\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.00011092455766629427\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00011083184625022114\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00011070438631577417\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00011057811905629934\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00011048566724639386\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.0001104052781010978\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00011034625291358679\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00011027503554942087\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00011021056561730803\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.0001101509077125229\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00011010008252924308\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.00011005272797774523\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00011000647995388136\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00010996824072208255\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.00010993544769007713\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00010989899688865989\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00010987314861267805\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.0001098543536500074\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00010987447516527027\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.00010980447666952386\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.00010977981699397788\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.00010971721640089527\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00010969813156407327\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00010965706198476255\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.00010963115695631132\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00010961102962028235\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.00010958938684780151\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00010960054089082405\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00010958282247884199\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.0001095958353835158\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00010955286561511456\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.0001095595708466135\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00010954783268971369\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.0001095037508639507\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.0001094843799364753\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00010947235074127093\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.00010946629743557423\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00010946239432087168\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.00010945705958874897\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00010945469257421792\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00010943789064185694\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.00010940853389911354\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00010937682498479262\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.00010936417907942087\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.0001093490474158898\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.00010933926561847329\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.00010934096149867401\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00010933150100754574\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.0001093122357269749\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00010930710384855048\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00010931364027783275\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.00010928938776487485\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.0001092932908795774\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00010930406977422536\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00010929707175819203\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.00010930692835245281\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.0001092780617182143\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00010924107948085294\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.0001092329464154318\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.00010922475106781349\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.0001092224766034633\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.00010921257169684396\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.00010919787077000365\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.0001091923302737996\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00010918772110017016\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00010919886291958391\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00010918331681750715\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00010918150830548257\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.00010917021631030365\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00010918649495579302\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00010917299950961024\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.00010922987828962505\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00010920022963546216\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00010911566962022334\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.00010910317621892318\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.00010909538134001195\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.00010908814467256888\n",
      "Autoencoder14 training DONE... TOTAL TIME = 178.6019926071167\n",
      "start evaluation on test data for Autoencoder14\n",
      "MSE is 0.018985964197537997\n",
      "mutls per sample is 65856\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n",
      "Training Autoencoder15\n",
      "epoch 0,100000 samples processed..., training loss per sample for the current epoch is 0.024785786867141724\n",
      "epoch 1,100000 samples processed..., training loss per sample for the current epoch is 0.011466944925487041\n",
      "epoch 2,100000 samples processed..., training loss per sample for the current epoch is 0.005805510636419058\n",
      "epoch 3,100000 samples processed..., training loss per sample for the current epoch is 0.0028549594152718783\n",
      "epoch 4,100000 samples processed..., training loss per sample for the current epoch is 0.0013672373606823385\n",
      "epoch 5,100000 samples processed..., training loss per sample for the current epoch is 0.0007539302739314735\n",
      "epoch 6,100000 samples processed..., training loss per sample for the current epoch is 0.0005320716800633818\n",
      "epoch 7,100000 samples processed..., training loss per sample for the current epoch is 0.0004574570036493242\n",
      "epoch 8,100000 samples processed..., training loss per sample for the current epoch is 0.0004333613021299243\n",
      "epoch 9,100000 samples processed..., training loss per sample for the current epoch is 0.00042565366136841474\n",
      "epoch 10,100000 samples processed..., training loss per sample for the current epoch is 0.00042314454913139343\n",
      "epoch 11,100000 samples processed..., training loss per sample for the current epoch is 0.00042228113510645927\n",
      "epoch 12,100000 samples processed..., training loss per sample for the current epoch is 0.0004219386738259345\n",
      "epoch 13,100000 samples processed..., training loss per sample for the current epoch is 0.00042175887734629213\n",
      "epoch 14,100000 samples processed..., training loss per sample for the current epoch is 0.0004216320370323956\n",
      "epoch 15,100000 samples processed..., training loss per sample for the current epoch is 0.0004215192550327629\n",
      "epoch 16,100000 samples processed..., training loss per sample for the current epoch is 0.0004214053088799119\n",
      "epoch 17,100000 samples processed..., training loss per sample for the current epoch is 0.0004212825291324407\n",
      "epoch 18,100000 samples processed..., training loss per sample for the current epoch is 0.0004211454337928444\n",
      "epoch 19,100000 samples processed..., training loss per sample for the current epoch is 0.00042098861653357746\n",
      "epoch 20,100000 samples processed..., training loss per sample for the current epoch is 0.0004208061099052429\n",
      "epoch 21,100000 samples processed..., training loss per sample for the current epoch is 0.00042059060651808975\n",
      "epoch 22,100000 samples processed..., training loss per sample for the current epoch is 0.00042033313191495834\n",
      "epoch 23,100000 samples processed..., training loss per sample for the current epoch is 0.0004200222564395517\n",
      "epoch 24,100000 samples processed..., training loss per sample for the current epoch is 0.0004196436400525272\n",
      "epoch 25,100000 samples processed..., training loss per sample for the current epoch is 0.0004191789368633181\n",
      "epoch 26,100000 samples processed..., training loss per sample for the current epoch is 0.00041860437486320736\n",
      "epoch 27,100000 samples processed..., training loss per sample for the current epoch is 0.00041788970818743107\n",
      "epoch 28,100000 samples processed..., training loss per sample for the current epoch is 0.00041699649882502853\n",
      "epoch 29,100000 samples processed..., training loss per sample for the current epoch is 0.00041587715852074323\n",
      "epoch 30,100000 samples processed..., training loss per sample for the current epoch is 0.00041447415016591547\n",
      "epoch 31,100000 samples processed..., training loss per sample for the current epoch is 0.00041272137314081193\n",
      "epoch 32,100000 samples processed..., training loss per sample for the current epoch is 0.00041054905857890845\n",
      "epoch 33,100000 samples processed..., training loss per sample for the current epoch is 0.0004078913410194218\n",
      "epoch 34,100000 samples processed..., training loss per sample for the current epoch is 0.00040470153093338015\n",
      "epoch 35,100000 samples processed..., training loss per sample for the current epoch is 0.00040096866083331404\n",
      "epoch 36,100000 samples processed..., training loss per sample for the current epoch is 0.0003967303887475282\n",
      "epoch 37,100000 samples processed..., training loss per sample for the current epoch is 0.0003920552274212241\n",
      "epoch 38,100000 samples processed..., training loss per sample for the current epoch is 0.00038706843508407474\n",
      "epoch 39,100000 samples processed..., training loss per sample for the current epoch is 0.0003818965412210673\n",
      "epoch 40,100000 samples processed..., training loss per sample for the current epoch is 0.00037670623743906617\n",
      "epoch 41,100000 samples processed..., training loss per sample for the current epoch is 0.0003716340207029134\n",
      "epoch 42,100000 samples processed..., training loss per sample for the current epoch is 0.0003667658905033022\n",
      "epoch 43,100000 samples processed..., training loss per sample for the current epoch is 0.0003621573734562844\n",
      "epoch 44,100000 samples processed..., training loss per sample for the current epoch is 0.00035782676190137863\n",
      "epoch 45,100000 samples processed..., training loss per sample for the current epoch is 0.00035378238651901484\n",
      "epoch 46,100000 samples processed..., training loss per sample for the current epoch is 0.00035002249758690594\n",
      "epoch 47,100000 samples processed..., training loss per sample for the current epoch is 0.00034653762355446816\n",
      "epoch 48,100000 samples processed..., training loss per sample for the current epoch is 0.00034329254645854235\n",
      "epoch 49,100000 samples processed..., training loss per sample for the current epoch is 0.00034028994385153054\n",
      "epoch 50,100000 samples processed..., training loss per sample for the current epoch is 0.00033753028721548615\n",
      "epoch 51,100000 samples processed..., training loss per sample for the current epoch is 0.000334973584394902\n",
      "epoch 52,100000 samples processed..., training loss per sample for the current epoch is 0.0003325854998547584\n",
      "epoch 53,100000 samples processed..., training loss per sample for the current epoch is 0.00033033677958883346\n",
      "epoch 54,100000 samples processed..., training loss per sample for the current epoch is 0.00032825531088747083\n",
      "epoch 55,100000 samples processed..., training loss per sample for the current epoch is 0.00032633091090247037\n",
      "epoch 56,100000 samples processed..., training loss per sample for the current epoch is 0.00032453144900500774\n",
      "epoch 57,100000 samples processed..., training loss per sample for the current epoch is 0.00032284356420859696\n",
      "epoch 58,100000 samples processed..., training loss per sample for the current epoch is 0.000321225980296731\n",
      "epoch 59,100000 samples processed..., training loss per sample for the current epoch is 0.0003196541965007782\n",
      "epoch 60,100000 samples processed..., training loss per sample for the current epoch is 0.0003181941621005535\n",
      "epoch 61,100000 samples processed..., training loss per sample for the current epoch is 0.00031680840998888016\n",
      "epoch 62,100000 samples processed..., training loss per sample for the current epoch is 0.00031546201673336325\n",
      "epoch 63,100000 samples processed..., training loss per sample for the current epoch is 0.00031414467724971473\n",
      "epoch 64,100000 samples processed..., training loss per sample for the current epoch is 0.0003128502564504743\n",
      "epoch 65,100000 samples processed..., training loss per sample for the current epoch is 0.00031156499986536803\n",
      "epoch 66,100000 samples processed..., training loss per sample for the current epoch is 0.00031027144053950904\n",
      "epoch 67,100000 samples processed..., training loss per sample for the current epoch is 0.00030895094270817937\n",
      "epoch 68,100000 samples processed..., training loss per sample for the current epoch is 0.0003075851930771023\n",
      "epoch 69,100000 samples processed..., training loss per sample for the current epoch is 0.0003061559854540974\n",
      "epoch 70,100000 samples processed..., training loss per sample for the current epoch is 0.00030464723822660745\n",
      "epoch 71,100000 samples processed..., training loss per sample for the current epoch is 0.0003030423854943365\n",
      "epoch 72,100000 samples processed..., training loss per sample for the current epoch is 0.0003013260615989566\n",
      "epoch 73,100000 samples processed..., training loss per sample for the current epoch is 0.0002994856715667993\n",
      "epoch 74,100000 samples processed..., training loss per sample for the current epoch is 0.00029751303838565946\n",
      "epoch 75,100000 samples processed..., training loss per sample for the current epoch is 0.000295405084034428\n",
      "epoch 76,100000 samples processed..., training loss per sample for the current epoch is 0.00029316397150978444\n",
      "epoch 77,100000 samples processed..., training loss per sample for the current epoch is 0.0002907963073812425\n",
      "epoch 78,100000 samples processed..., training loss per sample for the current epoch is 0.0002883130870759487\n",
      "epoch 79,100000 samples processed..., training loss per sample for the current epoch is 0.00028572859358973803\n",
      "epoch 80,100000 samples processed..., training loss per sample for the current epoch is 0.00028306005522608756\n",
      "epoch 81,100000 samples processed..., training loss per sample for the current epoch is 0.0002803246839903295\n",
      "epoch 82,100000 samples processed..., training loss per sample for the current epoch is 0.0002775394369382411\n",
      "epoch 83,100000 samples processed..., training loss per sample for the current epoch is 0.0002747197460848838\n",
      "epoch 84,100000 samples processed..., training loss per sample for the current epoch is 0.00027187988394871353\n",
      "epoch 85,100000 samples processed..., training loss per sample for the current epoch is 0.000269031950738281\n",
      "epoch 86,100000 samples processed..., training loss per sample for the current epoch is 0.00026618596632033587\n",
      "epoch 87,100000 samples processed..., training loss per sample for the current epoch is 0.0002633504581172019\n",
      "epoch 88,100000 samples processed..., training loss per sample for the current epoch is 0.00026053292443975804\n",
      "epoch 89,100000 samples processed..., training loss per sample for the current epoch is 0.00025773963425308467\n",
      "epoch 90,100000 samples processed..., training loss per sample for the current epoch is 0.00025497600086964666\n",
      "epoch 91,100000 samples processed..., training loss per sample for the current epoch is 0.0002522472839336842\n",
      "epoch 92,100000 samples processed..., training loss per sample for the current epoch is 0.00024955819128081203\n",
      "epoch 93,100000 samples processed..., training loss per sample for the current epoch is 0.00024691366648767145\n",
      "epoch 94,100000 samples processed..., training loss per sample for the current epoch is 0.00024431776895653454\n",
      "epoch 95,100000 samples processed..., training loss per sample for the current epoch is 0.00024177505052648486\n",
      "epoch 96,100000 samples processed..., training loss per sample for the current epoch is 0.00023928959446493537\n",
      "epoch 97,100000 samples processed..., training loss per sample for the current epoch is 0.00023686534084845333\n",
      "epoch 98,100000 samples processed..., training loss per sample for the current epoch is 0.00023450640961527824\n",
      "epoch 99,100000 samples processed..., training loss per sample for the current epoch is 0.00023221693933010102\n",
      "epoch 100,100000 samples processed..., training loss per sample for the current epoch is 0.00023000052489805968\n",
      "epoch 101,100000 samples processed..., training loss per sample for the current epoch is 0.00022786059824284166\n",
      "epoch 102,100000 samples processed..., training loss per sample for the current epoch is 0.00022579963435418905\n",
      "epoch 103,100000 samples processed..., training loss per sample for the current epoch is 0.0002238205453613773\n",
      "epoch 104,100000 samples processed..., training loss per sample for the current epoch is 0.0002219253545626998\n",
      "epoch 105,100000 samples processed..., training loss per sample for the current epoch is 0.00022011579130776227\n",
      "epoch 106,100000 samples processed..., training loss per sample for the current epoch is 0.00021839316352270543\n",
      "epoch 107,100000 samples processed..., training loss per sample for the current epoch is 0.00021675814292393625\n",
      "epoch 108,100000 samples processed..., training loss per sample for the current epoch is 0.00021521115617360918\n",
      "epoch 109,100000 samples processed..., training loss per sample for the current epoch is 0.0002137521147960797\n",
      "epoch 110,100000 samples processed..., training loss per sample for the current epoch is 0.0002123803086578846\n",
      "epoch 111,100000 samples processed..., training loss per sample for the current epoch is 0.00021109477616846562\n",
      "epoch 112,100000 samples processed..., training loss per sample for the current epoch is 0.00020989345561247318\n",
      "epoch 113,100000 samples processed..., training loss per sample for the current epoch is 0.0002087741531431675\n",
      "epoch 114,100000 samples processed..., training loss per sample for the current epoch is 0.00020773397583980112\n",
      "epoch 115,100000 samples processed..., training loss per sample for the current epoch is 0.00020676993066444992\n",
      "epoch 116,100000 samples processed..., training loss per sample for the current epoch is 0.00020587854960467665\n",
      "epoch 117,100000 samples processed..., training loss per sample for the current epoch is 0.00020505604508798568\n",
      "epoch 118,100000 samples processed..., training loss per sample for the current epoch is 0.00020429858646821232\n",
      "epoch 119,100000 samples processed..., training loss per sample for the current epoch is 0.0002036021393723786\n",
      "epoch 120,100000 samples processed..., training loss per sample for the current epoch is 0.00020296299830079077\n",
      "epoch 121,100000 samples processed..., training loss per sample for the current epoch is 0.00020237738906871526\n",
      "epoch 122,100000 samples processed..., training loss per sample for the current epoch is 0.0002018416946521029\n",
      "epoch 123,100000 samples processed..., training loss per sample for the current epoch is 0.00020135222934186458\n",
      "epoch 124,100000 samples processed..., training loss per sample for the current epoch is 0.00020090558740776032\n",
      "epoch 125,100000 samples processed..., training loss per sample for the current epoch is 0.00020049856218975036\n",
      "epoch 126,100000 samples processed..., training loss per sample for the current epoch is 0.00020012803841382265\n",
      "epoch 127,100000 samples processed..., training loss per sample for the current epoch is 0.00019979111093562096\n",
      "epoch 128,100000 samples processed..., training loss per sample for the current epoch is 0.0001994849508628249\n",
      "epoch 129,100000 samples processed..., training loss per sample for the current epoch is 0.00019920701684895904\n",
      "epoch 130,100000 samples processed..., training loss per sample for the current epoch is 0.00019895473320502788\n",
      "epoch 131,100000 samples processed..., training loss per sample for the current epoch is 0.0001987261336762458\n",
      "epoch 132,100000 samples processed..., training loss per sample for the current epoch is 0.0001985190948471427\n",
      "epoch 133,100000 samples processed..., training loss per sample for the current epoch is 0.00019833184487652033\n",
      "epoch 134,100000 samples processed..., training loss per sample for the current epoch is 0.00019816243613604456\n",
      "epoch 135,100000 samples processed..., training loss per sample for the current epoch is 0.0001980090537108481\n",
      "epoch 136,100000 samples processed..., training loss per sample for the current epoch is 0.00019787069817539304\n",
      "epoch 137,100000 samples processed..., training loss per sample for the current epoch is 0.00019774593820329756\n",
      "epoch 138,100000 samples processed..., training loss per sample for the current epoch is 0.00019763271848205477\n",
      "epoch 139,100000 samples processed..., training loss per sample for the current epoch is 0.00019753211410716175\n",
      "epoch 140,100000 samples processed..., training loss per sample for the current epoch is 0.00019745175319258124\n",
      "epoch 141,100000 samples processed..., training loss per sample for the current epoch is 0.00019740243558771908\n",
      "epoch 142,100000 samples processed..., training loss per sample for the current epoch is 0.00019729693478439002\n",
      "epoch 143,100000 samples processed..., training loss per sample for the current epoch is 0.00019721739285159855\n",
      "epoch 144,100000 samples processed..., training loss per sample for the current epoch is 0.00019715362053830177\n",
      "epoch 145,100000 samples processed..., training loss per sample for the current epoch is 0.00019710030348505826\n",
      "epoch 146,100000 samples processed..., training loss per sample for the current epoch is 0.00019705321406945586\n",
      "epoch 147,100000 samples processed..., training loss per sample for the current epoch is 0.00019699940399732442\n",
      "epoch 148,100000 samples processed..., training loss per sample for the current epoch is 0.0001969579030992463\n",
      "epoch 149,100000 samples processed..., training loss per sample for the current epoch is 0.00019692468165885657\n",
      "epoch 150,100000 samples processed..., training loss per sample for the current epoch is 0.00019689012260641902\n",
      "epoch 151,100000 samples processed..., training loss per sample for the current epoch is 0.0001968559337547049\n",
      "epoch 152,100000 samples processed..., training loss per sample for the current epoch is 0.00019682961283251643\n",
      "epoch 153,100000 samples processed..., training loss per sample for the current epoch is 0.0001968073396710679\n",
      "epoch 154,100000 samples processed..., training loss per sample for the current epoch is 0.00019678525743074715\n",
      "epoch 155,100000 samples processed..., training loss per sample for the current epoch is 0.00019676367286592723\n",
      "epoch 156,100000 samples processed..., training loss per sample for the current epoch is 0.00019674488168675453\n",
      "epoch 157,100000 samples processed..., training loss per sample for the current epoch is 0.00019672898517455906\n",
      "epoch 158,100000 samples processed..., training loss per sample for the current epoch is 0.00019671519927214832\n",
      "epoch 159,100000 samples processed..., training loss per sample for the current epoch is 0.00019670269510243087\n",
      "epoch 160,100000 samples processed..., training loss per sample for the current epoch is 0.00019669095403514802\n",
      "epoch 161,100000 samples processed..., training loss per sample for the current epoch is 0.0001966799865476787\n",
      "epoch 162,100000 samples processed..., training loss per sample for the current epoch is 0.00019666983978822828\n",
      "epoch 163,100000 samples processed..., training loss per sample for the current epoch is 0.00019666062959004194\n",
      "epoch 164,100000 samples processed..., training loss per sample for the current epoch is 0.00019665257132146509\n",
      "epoch 165,100000 samples processed..., training loss per sample for the current epoch is 0.00019664542807731777\n",
      "epoch 166,100000 samples processed..., training loss per sample for the current epoch is 0.00019663902057800441\n",
      "epoch 167,100000 samples processed..., training loss per sample for the current epoch is 0.00019663355022203177\n",
      "epoch 168,100000 samples processed..., training loss per sample for the current epoch is 0.00019662921025883406\n",
      "epoch 169,100000 samples processed..., training loss per sample for the current epoch is 0.00019662553269881756\n",
      "epoch 170,100000 samples processed..., training loss per sample for the current epoch is 0.00019662219390738754\n",
      "epoch 171,100000 samples processed..., training loss per sample for the current epoch is 0.00019661874044686557\n",
      "epoch 172,100000 samples processed..., training loss per sample for the current epoch is 0.00019661522179376334\n",
      "epoch 173,100000 samples processed..., training loss per sample for the current epoch is 0.00019661196856759488\n",
      "epoch 174,100000 samples processed..., training loss per sample for the current epoch is 0.00019660854886751621\n",
      "epoch 175,100000 samples processed..., training loss per sample for the current epoch is 0.00019660734513308854\n",
      "epoch 176,100000 samples processed..., training loss per sample for the current epoch is 0.00019661439931951464\n",
      "epoch 177,100000 samples processed..., training loss per sample for the current epoch is 0.00019663910439703613\n",
      "epoch 178,100000 samples processed..., training loss per sample for the current epoch is 0.00019662591337691994\n",
      "epoch 179,100000 samples processed..., training loss per sample for the current epoch is 0.0001966056990204379\n",
      "epoch 180,100000 samples processed..., training loss per sample for the current epoch is 0.00019659815763588994\n",
      "epoch 181,100000 samples processed..., training loss per sample for the current epoch is 0.00019659328332636506\n",
      "epoch 182,100000 samples processed..., training loss per sample for the current epoch is 0.00019659166457131505\n",
      "epoch 183,100000 samples processed..., training loss per sample for the current epoch is 0.00019659040321130306\n",
      "epoch 184,100000 samples processed..., training loss per sample for the current epoch is 0.00019658842240460216\n",
      "epoch 185,100000 samples processed..., training loss per sample for the current epoch is 0.0001965888711856678\n",
      "epoch 186,100000 samples processed..., training loss per sample for the current epoch is 0.0001965929486323148\n",
      "epoch 187,100000 samples processed..., training loss per sample for the current epoch is 0.00019660292949993163\n",
      "epoch 188,100000 samples processed..., training loss per sample for the current epoch is 0.00019660647376440465\n",
      "epoch 189,100000 samples processed..., training loss per sample for the current epoch is 0.00019660110701806842\n",
      "epoch 190,100000 samples processed..., training loss per sample for the current epoch is 0.00019658849982079118\n",
      "epoch 191,100000 samples processed..., training loss per sample for the current epoch is 0.00019658572971820832\n",
      "epoch 192,100000 samples processed..., training loss per sample for the current epoch is 0.00019658815464936196\n",
      "epoch 193,100000 samples processed..., training loss per sample for the current epoch is 0.00019658322795294226\n",
      "epoch 194,100000 samples processed..., training loss per sample for the current epoch is 0.00019658272271044552\n",
      "epoch 195,100000 samples processed..., training loss per sample for the current epoch is 0.00019658343866467475\n",
      "epoch 196,100000 samples processed..., training loss per sample for the current epoch is 0.00019658144447021186\n",
      "epoch 197,100000 samples processed..., training loss per sample for the current epoch is 0.00019657883269246667\n",
      "epoch 198,100000 samples processed..., training loss per sample for the current epoch is 0.0001965784392086789\n",
      "epoch 199,100000 samples processed..., training loss per sample for the current epoch is 0.00019658164121210574\n",
      "epoch 200,100000 samples processed..., training loss per sample for the current epoch is 0.00019658879085909575\n",
      "epoch 201,100000 samples processed..., training loss per sample for the current epoch is 0.0001966066542081535\n",
      "epoch 202,100000 samples processed..., training loss per sample for the current epoch is 0.00019659742596559227\n",
      "epoch 203,100000 samples processed..., training loss per sample for the current epoch is 0.00019660274032503366\n",
      "epoch 204,100000 samples processed..., training loss per sample for the current epoch is 0.00019658746547065675\n",
      "epoch 205,100000 samples processed..., training loss per sample for the current epoch is 0.00019659801095258444\n",
      "epoch 206,100000 samples processed..., training loss per sample for the current epoch is 0.0001966056041419506\n",
      "epoch 207,100000 samples processed..., training loss per sample for the current epoch is 0.00019659588229842484\n",
      "epoch 208,100000 samples processed..., training loss per sample for the current epoch is 0.00019657954107969998\n",
      "epoch 209,100000 samples processed..., training loss per sample for the current epoch is 0.00019657837343402208\n",
      "epoch 210,100000 samples processed..., training loss per sample for the current epoch is 0.0001965759281301871\n",
      "epoch 211,100000 samples processed..., training loss per sample for the current epoch is 0.00019657259632367642\n",
      "epoch 212,100000 samples processed..., training loss per sample for the current epoch is 0.00019657027092762292\n",
      "epoch 213,100000 samples processed..., training loss per sample for the current epoch is 0.00019657122378703208\n",
      "epoch 214,100000 samples processed..., training loss per sample for the current epoch is 0.00019657193101011217\n",
      "epoch 215,100000 samples processed..., training loss per sample for the current epoch is 0.0001965711114462465\n",
      "epoch 216,100000 samples processed..., training loss per sample for the current epoch is 0.00019656908407341688\n",
      "epoch 217,100000 samples processed..., training loss per sample for the current epoch is 0.00019656679593026637\n",
      "epoch 218,100000 samples processed..., training loss per sample for the current epoch is 0.0001965651789214462\n",
      "epoch 219,100000 samples processed..., training loss per sample for the current epoch is 0.00019656435993965716\n",
      "epoch 220,100000 samples processed..., training loss per sample for the current epoch is 0.0001965639228001237\n",
      "epoch 221,100000 samples processed..., training loss per sample for the current epoch is 0.00019656368356663734\n",
      "epoch 222,100000 samples processed..., training loss per sample for the current epoch is 0.000196563572390005\n",
      "epoch 223,100000 samples processed..., training loss per sample for the current epoch is 0.0001965633686631918\n",
      "epoch 224,100000 samples processed..., training loss per sample for the current epoch is 0.0001965631957864389\n",
      "epoch 225,100000 samples processed..., training loss per sample for the current epoch is 0.00019656353397294878\n",
      "epoch 226,100000 samples processed..., training loss per sample for the current epoch is 0.00019660056859720497\n",
      "epoch 227,100000 samples processed..., training loss per sample for the current epoch is 0.00019658767676446586\n",
      "epoch 228,100000 samples processed..., training loss per sample for the current epoch is 0.00019659058074466884\n",
      "epoch 229,100000 samples processed..., training loss per sample for the current epoch is 0.00019657868426293135\n",
      "epoch 230,100000 samples processed..., training loss per sample for the current epoch is 0.00019657433847896754\n",
      "epoch 231,100000 samples processed..., training loss per sample for the current epoch is 0.00019658229313790799\n",
      "epoch 232,100000 samples processed..., training loss per sample for the current epoch is 0.0001965689816279337\n",
      "epoch 233,100000 samples processed..., training loss per sample for the current epoch is 0.00019656609802041203\n",
      "epoch 234,100000 samples processed..., training loss per sample for the current epoch is 0.0001965678692795336\n",
      "epoch 235,100000 samples processed..., training loss per sample for the current epoch is 0.00019656595366541297\n",
      "epoch 236,100000 samples processed..., training loss per sample for the current epoch is 0.00019656642922200263\n",
      "epoch 237,100000 samples processed..., training loss per sample for the current epoch is 0.00019657244789414107\n",
      "epoch 238,100000 samples processed..., training loss per sample for the current epoch is 0.00019658287055790424\n",
      "epoch 239,100000 samples processed..., training loss per sample for the current epoch is 0.00019658633857034147\n",
      "epoch 240,100000 samples processed..., training loss per sample for the current epoch is 0.00019657691183965653\n",
      "epoch 241,100000 samples processed..., training loss per sample for the current epoch is 0.00019657422439195215\n",
      "epoch 242,100000 samples processed..., training loss per sample for the current epoch is 0.00019656751363072544\n",
      "epoch 243,100000 samples processed..., training loss per sample for the current epoch is 0.00019657004333566874\n",
      "epoch 244,100000 samples processed..., training loss per sample for the current epoch is 0.0001965695805847645\n",
      "epoch 245,100000 samples processed..., training loss per sample for the current epoch is 0.00019656814693007617\n",
      "epoch 246,100000 samples processed..., training loss per sample for the current epoch is 0.00019657084601931274\n",
      "epoch 247,100000 samples processed..., training loss per sample for the current epoch is 0.00019657995377201588\n",
      "epoch 248,100000 samples processed..., training loss per sample for the current epoch is 0.0001965827395906672\n",
      "epoch 249,100000 samples processed..., training loss per sample for the current epoch is 0.00019658012955915184\n",
      "epoch 250,100000 samples processed..., training loss per sample for the current epoch is 0.0001965868059778586\n",
      "epoch 251,100000 samples processed..., training loss per sample for the current epoch is 0.00019657348748296498\n",
      "epoch 252,100000 samples processed..., training loss per sample for the current epoch is 0.0001965685491450131\n",
      "epoch 253,100000 samples processed..., training loss per sample for the current epoch is 0.00019657002412714063\n",
      "epoch 254,100000 samples processed..., training loss per sample for the current epoch is 0.00019656806718558074\n",
      "epoch 255,100000 samples processed..., training loss per sample for the current epoch is 0.00019656798453070222\n",
      "epoch 256,100000 samples processed..., training loss per sample for the current epoch is 0.0001965711486991495\n",
      "epoch 257,100000 samples processed..., training loss per sample for the current epoch is 0.00019657587632536887\n",
      "epoch 258,100000 samples processed..., training loss per sample for the current epoch is 0.0001965816761367023\n",
      "epoch 259,100000 samples processed..., training loss per sample for the current epoch is 0.00019659077748656272\n",
      "epoch 260,100000 samples processed..., training loss per sample for the current epoch is 0.00019659066456370055\n",
      "epoch 261,100000 samples processed..., training loss per sample for the current epoch is 0.00019659071520436555\n",
      "epoch 262,100000 samples processed..., training loss per sample for the current epoch is 0.00019657806027680635\n",
      "epoch 263,100000 samples processed..., training loss per sample for the current epoch is 0.0001965757313882932\n",
      "epoch 264,100000 samples processed..., training loss per sample for the current epoch is 0.00019657452416140585\n",
      "epoch 265,100000 samples processed..., training loss per sample for the current epoch is 0.00019657116441521794\n",
      "epoch 266,100000 samples processed..., training loss per sample for the current epoch is 0.0001965715968981385\n",
      "epoch 267,100000 samples processed..., training loss per sample for the current epoch is 0.00019657192518934607\n",
      "epoch 268,100000 samples processed..., training loss per sample for the current epoch is 0.00019657161843497306\n",
      "epoch 269,100000 samples processed..., training loss per sample for the current epoch is 0.0001965725899208337\n",
      "epoch 270,100000 samples processed..., training loss per sample for the current epoch is 0.00019657465163618325\n",
      "epoch 271,100000 samples processed..., training loss per sample for the current epoch is 0.00019657643628306687\n",
      "epoch 272,100000 samples processed..., training loss per sample for the current epoch is 0.00019657759112305938\n",
      "epoch 273,100000 samples processed..., training loss per sample for the current epoch is 0.00019657805911265313\n",
      "epoch 274,100000 samples processed..., training loss per sample for the current epoch is 0.00019657800788991153\n",
      "epoch 275,100000 samples processed..., training loss per sample for the current epoch is 0.00019657749915495516\n",
      "epoch 276,100000 samples processed..., training loss per sample for the current epoch is 0.00019657637109048664\n",
      "epoch 277,100000 samples processed..., training loss per sample for the current epoch is 0.00019657583499792963\n",
      "epoch 278,100000 samples processed..., training loss per sample for the current epoch is 0.000196599205955863\n",
      "epoch 279,100000 samples processed..., training loss per sample for the current epoch is 0.00019659507961478084\n",
      "epoch 280,100000 samples processed..., training loss per sample for the current epoch is 0.00019665105384774505\n",
      "epoch 281,100000 samples processed..., training loss per sample for the current epoch is 0.00019659444224089383\n",
      "epoch 282,100000 samples processed..., training loss per sample for the current epoch is 0.0001965795666910708\n",
      "epoch 283,100000 samples processed..., training loss per sample for the current epoch is 0.00019657792407087982\n",
      "epoch 284,100000 samples processed..., training loss per sample for the current epoch is 0.0001965766993816942\n",
      "epoch 285,100000 samples processed..., training loss per sample for the current epoch is 0.0001965766987996176\n",
      "epoch 286,100000 samples processed..., training loss per sample for the current epoch is 0.00019657604862004517\n",
      "epoch 287,100000 samples processed..., training loss per sample for the current epoch is 0.0001965755072887987\n",
      "epoch 288,100000 samples processed..., training loss per sample for the current epoch is 0.0001965754886623472\n",
      "epoch 289,100000 samples processed..., training loss per sample for the current epoch is 0.0001965757244033739\n",
      "epoch 290,100000 samples processed..., training loss per sample for the current epoch is 0.00019657594617456198\n",
      "epoch 291,100000 samples processed..., training loss per sample for the current epoch is 0.00019657607132103294\n",
      "epoch 292,100000 samples processed..., training loss per sample for the current epoch is 0.0001965763425687328\n",
      "epoch 293,100000 samples processed..., training loss per sample for the current epoch is 0.00019657777273096144\n",
      "epoch 294,100000 samples processed..., training loss per sample for the current epoch is 0.00019668209250085056\n",
      "epoch 295,100000 samples processed..., training loss per sample for the current epoch is 0.0001966020942199975\n",
      "epoch 296,100000 samples processed..., training loss per sample for the current epoch is 0.0001965815230505541\n",
      "epoch 297,100000 samples processed..., training loss per sample for the current epoch is 0.0001965785154607147\n",
      "epoch 298,100000 samples processed..., training loss per sample for the current epoch is 0.00019657906610518695\n",
      "epoch 299,100000 samples processed..., training loss per sample for the current epoch is 0.00019657744909636676\n",
      "epoch 300,100000 samples processed..., training loss per sample for the current epoch is 0.00019657846714835615\n",
      "epoch 301,100000 samples processed..., training loss per sample for the current epoch is 0.00019658041943330318\n",
      "epoch 302,100000 samples processed..., training loss per sample for the current epoch is 0.00019658163364510983\n",
      "epoch 303,100000 samples processed..., training loss per sample for the current epoch is 0.00019658193574286997\n",
      "epoch 304,100000 samples processed..., training loss per sample for the current epoch is 0.00019658145494759084\n",
      "epoch 305,100000 samples processed..., training loss per sample for the current epoch is 0.00019658060860820115\n",
      "epoch 306,100000 samples processed..., training loss per sample for the current epoch is 0.00019658004806842654\n",
      "epoch 307,100000 samples processed..., training loss per sample for the current epoch is 0.00019658271863590927\n",
      "epoch 308,100000 samples processed..., training loss per sample for the current epoch is 0.00019669232598971576\n",
      "epoch 309,100000 samples processed..., training loss per sample for the current epoch is 0.00019661157042719424\n",
      "epoch 310,100000 samples processed..., training loss per sample for the current epoch is 0.00019659693643916398\n",
      "epoch 311,100000 samples processed..., training loss per sample for the current epoch is 0.00019659114070236683\n",
      "epoch 312,100000 samples processed..., training loss per sample for the current epoch is 0.00019659241079352796\n",
      "epoch 313,100000 samples processed..., training loss per sample for the current epoch is 0.00019658973498735578\n",
      "epoch 314,100000 samples processed..., training loss per sample for the current epoch is 0.0001965864159865305\n",
      "epoch 315,100000 samples processed..., training loss per sample for the current epoch is 0.0001965838175965473\n",
      "epoch 316,100000 samples processed..., training loss per sample for the current epoch is 0.00019658248755149543\n",
      "epoch 317,100000 samples processed..., training loss per sample for the current epoch is 0.00019658194098155944\n",
      "epoch 318,100000 samples processed..., training loss per sample for the current epoch is 0.00019658177101518958\n",
      "epoch 319,100000 samples processed..., training loss per sample for the current epoch is 0.00019658181932754814\n",
      "epoch 320,100000 samples processed..., training loss per sample for the current epoch is 0.0001965820341138169\n",
      "epoch 321,100000 samples processed..., training loss per sample for the current epoch is 0.0001965823827777058\n",
      "epoch 322,100000 samples processed..., training loss per sample for the current epoch is 0.0001965828042011708\n",
      "epoch 323,100000 samples processed..., training loss per sample for the current epoch is 0.00019658319768495858\n",
      "epoch 324,100000 samples processed..., training loss per sample for the current epoch is 0.00019658394623547792\n",
      "epoch 325,100000 samples processed..., training loss per sample for the current epoch is 0.0001965916942572221\n",
      "epoch 326,100000 samples processed..., training loss per sample for the current epoch is 0.00019665075931698083\n",
      "epoch 327,100000 samples processed..., training loss per sample for the current epoch is 0.00019660946796648204\n",
      "epoch 328,100000 samples processed..., training loss per sample for the current epoch is 0.00019660967867821455\n",
      "epoch 329,100000 samples processed..., training loss per sample for the current epoch is 0.00019660001213196666\n",
      "epoch 330,100000 samples processed..., training loss per sample for the current epoch is 0.00019659172976389528\n",
      "epoch 331,100000 samples processed..., training loss per sample for the current epoch is 0.0001965923380339518\n",
      "epoch 332,100000 samples processed..., training loss per sample for the current epoch is 0.0001965928147546947\n",
      "epoch 333,100000 samples processed..., training loss per sample for the current epoch is 0.0001965913356980309\n",
      "epoch 334,100000 samples processed..., training loss per sample for the current epoch is 0.0001965942687820643\n",
      "epoch 335,100000 samples processed..., training loss per sample for the current epoch is 0.00019659889803733677\n",
      "epoch 336,100000 samples processed..., training loss per sample for the current epoch is 0.0001966031437041238\n",
      "epoch 337,100000 samples processed..., training loss per sample for the current epoch is 0.00019660515710711479\n",
      "epoch 338,100000 samples processed..., training loss per sample for the current epoch is 0.00019660536374431104\n",
      "epoch 339,100000 samples processed..., training loss per sample for the current epoch is 0.00019660309364553542\n",
      "epoch 340,100000 samples processed..., training loss per sample for the current epoch is 0.00019659832876641302\n",
      "epoch 341,100000 samples processed..., training loss per sample for the current epoch is 0.0001965948677388951\n",
      "epoch 342,100000 samples processed..., training loss per sample for the current epoch is 0.00019659533922094852\n",
      "epoch 343,100000 samples processed..., training loss per sample for the current epoch is 0.0001965944457333535\n",
      "epoch 344,100000 samples processed..., training loss per sample for the current epoch is 0.000196594464359805\n",
      "epoch 345,100000 samples processed..., training loss per sample for the current epoch is 0.00019659906742163\n",
      "epoch 346,100000 samples processed..., training loss per sample for the current epoch is 0.0001966043608263135\n",
      "epoch 347,100000 samples processed..., training loss per sample for the current epoch is 0.00019660738296806813\n",
      "epoch 348,100000 samples processed..., training loss per sample for the current epoch is 0.00019660614663735031\n",
      "epoch 349,100000 samples processed..., training loss per sample for the current epoch is 0.00019660360761918126\n",
      "epoch 350,100000 samples processed..., training loss per sample for the current epoch is 0.00019659639452584087\n",
      "epoch 351,100000 samples processed..., training loss per sample for the current epoch is 0.00019659420591779055\n",
      "epoch 352,100000 samples processed..., training loss per sample for the current epoch is 0.00019659467681776733\n",
      "epoch 353,100000 samples processed..., training loss per sample for the current epoch is 0.00019659381418023257\n",
      "epoch 354,100000 samples processed..., training loss per sample for the current epoch is 0.00019659294397570193\n",
      "epoch 355,100000 samples processed..., training loss per sample for the current epoch is 0.00019659439101815223\n",
      "epoch 356,100000 samples processed..., training loss per sample for the current epoch is 0.00019659420300740748\n",
      "epoch 357,100000 samples processed..., training loss per sample for the current epoch is 0.00019661239522974938\n",
      "epoch 358,100000 samples processed..., training loss per sample for the current epoch is 0.0001966187631478533\n",
      "epoch 359,100000 samples processed..., training loss per sample for the current epoch is 0.00019661975442431868\n",
      "epoch 360,100000 samples processed..., training loss per sample for the current epoch is 0.00019661166530568152\n",
      "epoch 361,100000 samples processed..., training loss per sample for the current epoch is 0.00019660699414089322\n",
      "epoch 362,100000 samples processed..., training loss per sample for the current epoch is 0.0001966075284872204\n",
      "epoch 363,100000 samples processed..., training loss per sample for the current epoch is 0.00019660736201331018\n",
      "epoch 364,100000 samples processed..., training loss per sample for the current epoch is 0.00019660791498608886\n",
      "epoch 365,100000 samples processed..., training loss per sample for the current epoch is 0.00019660058489535004\n",
      "epoch 366,100000 samples processed..., training loss per sample for the current epoch is 0.00019659985206089915\n",
      "epoch 367,100000 samples processed..., training loss per sample for the current epoch is 0.00019660048768855632\n",
      "epoch 368,100000 samples processed..., training loss per sample for the current epoch is 0.00019659796787891538\n",
      "epoch 369,100000 samples processed..., training loss per sample for the current epoch is 0.00019659947254695\n",
      "epoch 370,100000 samples processed..., training loss per sample for the current epoch is 0.00019660186022520067\n",
      "epoch 371,100000 samples processed..., training loss per sample for the current epoch is 0.00019660388643387704\n",
      "epoch 372,100000 samples processed..., training loss per sample for the current epoch is 0.00019660507969092577\n",
      "epoch 373,100000 samples processed..., training loss per sample for the current epoch is 0.00019660779158584774\n",
      "epoch 374,100000 samples processed..., training loss per sample for the current epoch is 0.000196612867875956\n",
      "epoch 375,100000 samples processed..., training loss per sample for the current epoch is 0.00019661816477309913\n",
      "epoch 376,100000 samples processed..., training loss per sample for the current epoch is 0.0001966156519483775\n",
      "epoch 377,100000 samples processed..., training loss per sample for the current epoch is 0.00019660280842799692\n",
      "epoch 378,100000 samples processed..., training loss per sample for the current epoch is 0.00019660038291476666\n",
      "epoch 379,100000 samples processed..., training loss per sample for the current epoch is 0.00019659600453451277\n",
      "epoch 380,100000 samples processed..., training loss per sample for the current epoch is 0.00019659515237435699\n",
      "epoch 381,100000 samples processed..., training loss per sample for the current epoch is 0.00019659709825646133\n",
      "epoch 382,100000 samples processed..., training loss per sample for the current epoch is 0.0001965987734729424\n",
      "epoch 383,100000 samples processed..., training loss per sample for the current epoch is 0.00019660092773847283\n",
      "epoch 384,100000 samples processed..., training loss per sample for the current epoch is 0.00019660441495943814\n",
      "epoch 385,100000 samples processed..., training loss per sample for the current epoch is 0.0001966096495743841\n",
      "epoch 386,100000 samples processed..., training loss per sample for the current epoch is 0.00019662209204398095\n",
      "epoch 387,100000 samples processed..., training loss per sample for the current epoch is 0.00019662550475914032\n",
      "epoch 388,100000 samples processed..., training loss per sample for the current epoch is 0.00019660905527416617\n",
      "epoch 389,100000 samples processed..., training loss per sample for the current epoch is 0.0001966032333439216\n",
      "epoch 390,100000 samples processed..., training loss per sample for the current epoch is 0.00019660523626953365\n",
      "epoch 391,100000 samples processed..., training loss per sample for the current epoch is 0.0001966095028910786\n",
      "epoch 392,100000 samples processed..., training loss per sample for the current epoch is 0.0001966089737834409\n",
      "epoch 393,100000 samples processed..., training loss per sample for the current epoch is 0.00019660521880723536\n",
      "epoch 394,100000 samples processed..., training loss per sample for the current epoch is 0.0001966051134513691\n",
      "epoch 395,100000 samples processed..., training loss per sample for the current epoch is 0.00019660818157717585\n",
      "epoch 396,100000 samples processed..., training loss per sample for the current epoch is 0.00019660356221720576\n",
      "epoch 397,100000 samples processed..., training loss per sample for the current epoch is 0.00019660264544654637\n",
      "epoch 398,100000 samples processed..., training loss per sample for the current epoch is 0.00019660502206534148\n",
      "epoch 399,100000 samples processed..., training loss per sample for the current epoch is 0.0001966066553723067\n",
      "epoch 400,100000 samples processed..., training loss per sample for the current epoch is 0.00019660497142467647\n",
      "epoch 401,100000 samples processed..., training loss per sample for the current epoch is 0.00019660582242067904\n",
      "epoch 402,100000 samples processed..., training loss per sample for the current epoch is 0.00019660854013636708\n",
      "epoch 403,100000 samples processed..., training loss per sample for the current epoch is 0.00019661078287754207\n",
      "epoch 404,100000 samples processed..., training loss per sample for the current epoch is 0.00019660677295178176\n",
      "epoch 405,100000 samples processed..., training loss per sample for the current epoch is 0.00019660582125652582\n",
      "epoch 406,100000 samples processed..., training loss per sample for the current epoch is 0.00019660818565171213\n",
      "epoch 407,100000 samples processed..., training loss per sample for the current epoch is 0.00019660415477119387\n",
      "epoch 408,100000 samples processed..., training loss per sample for the current epoch is 0.0001966028951574117\n",
      "epoch 409,100000 samples processed..., training loss per sample for the current epoch is 0.00019660358724649996\n",
      "epoch 410,100000 samples processed..., training loss per sample for the current epoch is 0.00019660535792354495\n",
      "epoch 411,100000 samples processed..., training loss per sample for the current epoch is 0.00019660432066302745\n",
      "epoch 412,100000 samples processed..., training loss per sample for the current epoch is 0.00019660511112306266\n",
      "epoch 413,100000 samples processed..., training loss per sample for the current epoch is 0.00019660743710119276\n",
      "epoch 414,100000 samples processed..., training loss per sample for the current epoch is 0.00019661223341245205\n",
      "epoch 415,100000 samples processed..., training loss per sample for the current epoch is 0.00019661375088617206\n",
      "epoch 416,100000 samples processed..., training loss per sample for the current epoch is 0.0001966107141925022\n",
      "epoch 417,100000 samples processed..., training loss per sample for the current epoch is 0.0001966057636309415\n",
      "epoch 418,100000 samples processed..., training loss per sample for the current epoch is 0.00019660221296362578\n",
      "epoch 419,100000 samples processed..., training loss per sample for the current epoch is 0.00019659905752632766\n",
      "epoch 420,100000 samples processed..., training loss per sample for the current epoch is 0.00019659905112348497\n",
      "epoch 421,100000 samples processed..., training loss per sample for the current epoch is 0.00019660037360154093\n",
      "epoch 422,100000 samples processed..., training loss per sample for the current epoch is 0.00019660099409520625\n",
      "epoch 423,100000 samples processed..., training loss per sample for the current epoch is 0.00019660091202240438\n",
      "epoch 424,100000 samples processed..., training loss per sample for the current epoch is 0.00019660032528918237\n",
      "epoch 425,100000 samples processed..., training loss per sample for the current epoch is 0.00019659716228488832\n",
      "epoch 426,100000 samples processed..., training loss per sample for the current epoch is 0.00019659602257888765\n",
      "epoch 427,100000 samples processed..., training loss per sample for the current epoch is 0.00019659526413306593\n",
      "epoch 428,100000 samples processed..., training loss per sample for the current epoch is 0.0001965946372365579\n",
      "epoch 429,100000 samples processed..., training loss per sample for the current epoch is 0.0001965945214033127\n",
      "epoch 430,100000 samples processed..., training loss per sample for the current epoch is 0.00019660848542116583\n",
      "epoch 431,100000 samples processed..., training loss per sample for the current epoch is 0.00019669708330184222\n",
      "epoch 432,100000 samples processed..., training loss per sample for the current epoch is 0.00019662305596284568\n",
      "epoch 433,100000 samples processed..., training loss per sample for the current epoch is 0.0001965946820564568\n",
      "epoch 434,100000 samples processed..., training loss per sample for the current epoch is 0.0001965921139344573\n",
      "epoch 435,100000 samples processed..., training loss per sample for the current epoch is 0.00019659188867080958\n",
      "epoch 436,100000 samples processed..., training loss per sample for the current epoch is 0.00019659179029986264\n",
      "epoch 437,100000 samples processed..., training loss per sample for the current epoch is 0.00019659207726363092\n",
      "epoch 438,100000 samples processed..., training loss per sample for the current epoch is 0.0001965921581722796\n",
      "epoch 439,100000 samples processed..., training loss per sample for the current epoch is 0.00019659209938254206\n",
      "epoch 440,100000 samples processed..., training loss per sample for the current epoch is 0.00019659228099044412\n",
      "epoch 441,100000 samples processed..., training loss per sample for the current epoch is 0.00019659509183838963\n",
      "epoch 442,100000 samples processed..., training loss per sample for the current epoch is 0.00019662139122374355\n",
      "epoch 443,100000 samples processed..., training loss per sample for the current epoch is 0.00019661697559058666\n",
      "epoch 444,100000 samples processed..., training loss per sample for the current epoch is 0.00019660163845401258\n",
      "epoch 445,100000 samples processed..., training loss per sample for the current epoch is 0.00019660414720419793\n",
      "epoch 446,100000 samples processed..., training loss per sample for the current epoch is 0.00019659914018120615\n",
      "epoch 447,100000 samples processed..., training loss per sample for the current epoch is 0.00019660104531794788\n",
      "epoch 448,100000 samples processed..., training loss per sample for the current epoch is 0.000196606827666983\n",
      "epoch 449,100000 samples processed..., training loss per sample for the current epoch is 0.00019661006925161928\n",
      "epoch 450,100000 samples processed..., training loss per sample for the current epoch is 0.00019662347622215748\n",
      "epoch 451,100000 samples processed..., training loss per sample for the current epoch is 0.00019661235623061657\n",
      "epoch 452,100000 samples processed..., training loss per sample for the current epoch is 0.00019660410354845225\n",
      "epoch 453,100000 samples processed..., training loss per sample for the current epoch is 0.00019660368503537028\n",
      "epoch 454,100000 samples processed..., training loss per sample for the current epoch is 0.00019660138757899404\n",
      "epoch 455,100000 samples processed..., training loss per sample for the current epoch is 0.00019660161924548448\n",
      "epoch 456,100000 samples processed..., training loss per sample for the current epoch is 0.00019659938698168844\n",
      "epoch 457,100000 samples processed..., training loss per sample for the current epoch is 0.00019659875484649093\n",
      "epoch 458,100000 samples processed..., training loss per sample for the current epoch is 0.00019659852958284319\n",
      "epoch 459,100000 samples processed..., training loss per sample for the current epoch is 0.00019659937999676912\n",
      "epoch 460,100000 samples processed..., training loss per sample for the current epoch is 0.00019660008372738959\n",
      "epoch 461,100000 samples processed..., training loss per sample for the current epoch is 0.00019660105113871395\n",
      "epoch 462,100000 samples processed..., training loss per sample for the current epoch is 0.0001966016791993752\n",
      "epoch 463,100000 samples processed..., training loss per sample for the current epoch is 0.0001966034178622067\n",
      "epoch 464,100000 samples processed..., training loss per sample for the current epoch is 0.00019660247256979345\n",
      "epoch 465,100000 samples processed..., training loss per sample for the current epoch is 0.0001966013736091554\n",
      "epoch 466,100000 samples processed..., training loss per sample for the current epoch is 0.00019659980200231075\n",
      "epoch 467,100000 samples processed..., training loss per sample for the current epoch is 0.00019659835030324757\n",
      "epoch 468,100000 samples processed..., training loss per sample for the current epoch is 0.0001965979184024036\n",
      "epoch 469,100000 samples processed..., training loss per sample for the current epoch is 0.00019659679324831813\n",
      "epoch 470,100000 samples processed..., training loss per sample for the current epoch is 0.00019659758836496622\n",
      "epoch 471,100000 samples processed..., training loss per sample for the current epoch is 0.00019659828452859074\n",
      "epoch 472,100000 samples processed..., training loss per sample for the current epoch is 0.00019659943645820022\n",
      "epoch 473,100000 samples processed..., training loss per sample for the current epoch is 0.0001966002635890618\n",
      "epoch 474,100000 samples processed..., training loss per sample for the current epoch is 0.00019660149817354977\n",
      "epoch 475,100000 samples processed..., training loss per sample for the current epoch is 0.0001966044888831675\n",
      "epoch 476,100000 samples processed..., training loss per sample for the current epoch is 0.00019660466350615024\n",
      "epoch 477,100000 samples processed..., training loss per sample for the current epoch is 0.00019660154765006155\n",
      "epoch 478,100000 samples processed..., training loss per sample for the current epoch is 0.0001965988171286881\n",
      "epoch 479,100000 samples processed..., training loss per sample for the current epoch is 0.00019659500976558774\n",
      "epoch 480,100000 samples processed..., training loss per sample for the current epoch is 0.00019659359066281467\n",
      "epoch 481,100000 samples processed..., training loss per sample for the current epoch is 0.00019659244164358825\n",
      "epoch 482,100000 samples processed..., training loss per sample for the current epoch is 0.00019659321871586143\n",
      "epoch 483,100000 samples processed..., training loss per sample for the current epoch is 0.00019659366691485047\n",
      "epoch 484,100000 samples processed..., training loss per sample for the current epoch is 0.00019659402896650137\n",
      "epoch 485,100000 samples processed..., training loss per sample for the current epoch is 0.00019659264595247805\n",
      "epoch 486,100000 samples processed..., training loss per sample for the current epoch is 0.00019659095327369868\n",
      "epoch 487,100000 samples processed..., training loss per sample for the current epoch is 0.0001965900103095919\n",
      "epoch 488,100000 samples processed..., training loss per sample for the current epoch is 0.00019658892066217959\n",
      "epoch 489,100000 samples processed..., training loss per sample for the current epoch is 0.00019658786128275098\n",
      "epoch 490,100000 samples processed..., training loss per sample for the current epoch is 0.0001965886459220201\n",
      "epoch 491,100000 samples processed..., training loss per sample for the current epoch is 0.00019665912317577748\n",
      "epoch 492,100000 samples processed..., training loss per sample for the current epoch is 0.0001966183487093076\n",
      "epoch 493,100000 samples processed..., training loss per sample for the current epoch is 0.000196601182105951\n",
      "epoch 494,100000 samples processed..., training loss per sample for the current epoch is 0.0001965868886327371\n",
      "epoch 495,100000 samples processed..., training loss per sample for the current epoch is 0.00019658578443340957\n",
      "epoch 496,100000 samples processed..., training loss per sample for the current epoch is 0.00019658619246911257\n",
      "epoch 497,100000 samples processed..., training loss per sample for the current epoch is 0.00019658542471006512\n",
      "epoch 498,100000 samples processed..., training loss per sample for the current epoch is 0.0001965849386760965\n",
      "epoch 499,100000 samples processed..., training loss per sample for the current epoch is 0.00019658480014186352\n",
      "epoch 500,100000 samples processed..., training loss per sample for the current epoch is 0.0001965849456610158\n",
      "epoch 501,100000 samples processed..., training loss per sample for the current epoch is 0.00019658693752717227\n",
      "epoch 502,100000 samples processed..., training loss per sample for the current epoch is 0.0001966209604870528\n",
      "epoch 503,100000 samples processed..., training loss per sample for the current epoch is 0.0001966121851000935\n",
      "epoch 504,100000 samples processed..., training loss per sample for the current epoch is 0.00019659482582937927\n",
      "epoch 505,100000 samples processed..., training loss per sample for the current epoch is 0.00019659130834043026\n",
      "epoch 506,100000 samples processed..., training loss per sample for the current epoch is 0.00019659290788695216\n",
      "epoch 507,100000 samples processed..., training loss per sample for the current epoch is 0.00019659003533888607\n",
      "epoch 508,100000 samples processed..., training loss per sample for the current epoch is 0.0001965950074372813\n",
      "epoch 509,100000 samples processed..., training loss per sample for the current epoch is 0.00019659899175167084\n",
      "epoch 510,100000 samples processed..., training loss per sample for the current epoch is 0.00019660029269289226\n",
      "epoch 511,100000 samples processed..., training loss per sample for the current epoch is 0.00019659488869365306\n",
      "epoch 512,100000 samples processed..., training loss per sample for the current epoch is 0.00019658878212794662\n",
      "epoch 513,100000 samples processed..., training loss per sample for the current epoch is 0.00019658932054881007\n",
      "epoch 514,100000 samples processed..., training loss per sample for the current epoch is 0.00019658836943563075\n",
      "epoch 515,100000 samples processed..., training loss per sample for the current epoch is 0.00019658599223475903\n",
      "epoch 516,100000 samples processed..., training loss per sample for the current epoch is 0.00019658663717564195\n",
      "epoch 517,100000 samples processed..., training loss per sample for the current epoch is 0.00019659629208035768\n",
      "epoch 518,100000 samples processed..., training loss per sample for the current epoch is 0.00019662927254103125\n",
      "epoch 519,100000 samples processed..., training loss per sample for the current epoch is 0.00019660227175336331\n",
      "epoch 520,100000 samples processed..., training loss per sample for the current epoch is 0.00019661437254399062\n",
      "epoch 521,100000 samples processed..., training loss per sample for the current epoch is 0.00019659505400341004\n",
      "epoch 522,100000 samples processed..., training loss per sample for the current epoch is 0.00019658554927445948\n",
      "epoch 523,100000 samples processed..., training loss per sample for the current epoch is 0.00019658455916214733\n",
      "epoch 524,100000 samples processed..., training loss per sample for the current epoch is 0.00019658314646221699\n",
      "epoch 525,100000 samples processed..., training loss per sample for the current epoch is 0.0001965829636901617\n",
      "epoch 526,100000 samples processed..., training loss per sample for the current epoch is 0.00019658424251247196\n",
      "epoch 527,100000 samples processed..., training loss per sample for the current epoch is 0.00019658589735627174\n",
      "epoch 528,100000 samples processed..., training loss per sample for the current epoch is 0.00019658680888824166\n",
      "epoch 529,100000 samples processed..., training loss per sample for the current epoch is 0.00019658670003991574\n",
      "epoch 530,100000 samples processed..., training loss per sample for the current epoch is 0.00019658536824863405\n",
      "epoch 531,100000 samples processed..., training loss per sample for the current epoch is 0.0001965837477473542\n",
      "epoch 532,100000 samples processed..., training loss per sample for the current epoch is 0.00019658344681374728\n",
      "epoch 533,100000 samples processed..., training loss per sample for the current epoch is 0.00019660539401229472\n",
      "epoch 534,100000 samples processed..., training loss per sample for the current epoch is 0.0001966035389341414\n",
      "epoch 535,100000 samples processed..., training loss per sample for the current epoch is 0.00019659272104036063\n",
      "epoch 536,100000 samples processed..., training loss per sample for the current epoch is 0.00019660033576656132\n",
      "epoch 537,100000 samples processed..., training loss per sample for the current epoch is 0.0001965945353731513\n",
      "epoch 538,100000 samples processed..., training loss per sample for the current epoch is 0.00019659379206132143\n",
      "epoch 539,100000 samples processed..., training loss per sample for the current epoch is 0.00019660078396555036\n",
      "epoch 540,100000 samples processed..., training loss per sample for the current epoch is 0.00019659554644022137\n",
      "epoch 541,100000 samples processed..., training loss per sample for the current epoch is 0.00019658729841466992\n",
      "epoch 542,100000 samples processed..., training loss per sample for the current epoch is 0.00019658970995806158\n",
      "epoch 543,100000 samples processed..., training loss per sample for the current epoch is 0.00019659311277791858\n",
      "epoch 544,100000 samples processed..., training loss per sample for the current epoch is 0.00019659466459415854\n",
      "epoch 545,100000 samples processed..., training loss per sample for the current epoch is 0.00019659295154269784\n",
      "epoch 546,100000 samples processed..., training loss per sample for the current epoch is 0.00019659071345813574\n",
      "epoch 547,100000 samples processed..., training loss per sample for the current epoch is 0.0001965871109860018\n",
      "epoch 548,100000 samples processed..., training loss per sample for the current epoch is 0.00019658535136841238\n",
      "epoch 549,100000 samples processed..., training loss per sample for the current epoch is 0.00019658405450172721\n",
      "epoch 550,100000 samples processed..., training loss per sample for the current epoch is 0.00019658498815260827\n",
      "epoch 551,100000 samples processed..., training loss per sample for the current epoch is 0.00019658595614600925\n",
      "epoch 552,100000 samples processed..., training loss per sample for the current epoch is 0.00019658754463307558\n",
      "epoch 553,100000 samples processed..., training loss per sample for the current epoch is 0.00019658919656649233\n",
      "epoch 554,100000 samples processed..., training loss per sample for the current epoch is 0.00019659092475194485\n",
      "epoch 555,100000 samples processed..., training loss per sample for the current epoch is 0.0001965937757631764\n",
      "epoch 556,100000 samples processed..., training loss per sample for the current epoch is 0.00019659392593894155\n",
      "epoch 557,100000 samples processed..., training loss per sample for the current epoch is 0.00019659089506603777\n",
      "epoch 558,100000 samples processed..., training loss per sample for the current epoch is 0.000196588080143556\n",
      "epoch 559,100000 samples processed..., training loss per sample for the current epoch is 0.000196584589430131\n",
      "epoch 560,100000 samples processed..., training loss per sample for the current epoch is 0.00019658296601846815\n",
      "epoch 561,100000 samples processed..., training loss per sample for the current epoch is 0.0001965818816097453\n",
      "epoch 562,100000 samples processed..., training loss per sample for the current epoch is 0.00019658270000945776\n",
      "epoch 563,100000 samples processed..., training loss per sample for the current epoch is 0.00019658344099298118\n",
      "epoch 564,100000 samples processed..., training loss per sample for the current epoch is 0.00019658392702694982\n",
      "epoch 565,100000 samples processed..., training loss per sample for the current epoch is 0.00019658338103909046\n",
      "epoch 566,100000 samples processed..., training loss per sample for the current epoch is 0.00019658069591969252\n",
      "epoch 567,100000 samples processed..., training loss per sample for the current epoch is 0.00019657935423310845\n",
      "epoch 568,100000 samples processed..., training loss per sample for the current epoch is 0.00019657835247926413\n",
      "epoch 569,100000 samples processed..., training loss per sample for the current epoch is 0.00019657758064568043\n",
      "epoch 570,100000 samples processed..., training loss per sample for the current epoch is 0.00019657915632706135\n",
      "epoch 571,100000 samples processed..., training loss per sample for the current epoch is 0.00019663344777654857\n",
      "epoch 572,100000 samples processed..., training loss per sample for the current epoch is 0.00019659963843878358\n",
      "epoch 573,100000 samples processed..., training loss per sample for the current epoch is 0.00019660441321320832\n",
      "epoch 574,100000 samples processed..., training loss per sample for the current epoch is 0.00019658506033010781\n",
      "epoch 575,100000 samples processed..., training loss per sample for the current epoch is 0.0001965758082224056\n",
      "epoch 576,100000 samples processed..., training loss per sample for the current epoch is 0.00019657697062939406\n",
      "epoch 577,100000 samples processed..., training loss per sample for the current epoch is 0.00019657689204905182\n",
      "epoch 578,100000 samples processed..., training loss per sample for the current epoch is 0.00019657620752695947\n",
      "epoch 579,100000 samples processed..., training loss per sample for the current epoch is 0.00019657538505271077\n",
      "epoch 580,100000 samples processed..., training loss per sample for the current epoch is 0.0001965749787632376\n",
      "epoch 581,100000 samples processed..., training loss per sample for the current epoch is 0.00019657501659821718\n",
      "epoch 582,100000 samples processed..., training loss per sample for the current epoch is 0.0001965768780792132\n",
      "epoch 583,100000 samples processed..., training loss per sample for the current epoch is 0.00019660868449136615\n",
      "epoch 584,100000 samples processed..., training loss per sample for the current epoch is 0.000196601232746616\n",
      "epoch 585,100000 samples processed..., training loss per sample for the current epoch is 0.00019658246019389481\n",
      "epoch 586,100000 samples processed..., training loss per sample for the current epoch is 0.0001965788780944422\n",
      "epoch 587,100000 samples processed..., training loss per sample for the current epoch is 0.00019658014934975654\n",
      "epoch 588,100000 samples processed..., training loss per sample for the current epoch is 0.00019657739147078247\n",
      "epoch 589,100000 samples processed..., training loss per sample for the current epoch is 0.0001965790509711951\n",
      "epoch 590,100000 samples processed..., training loss per sample for the current epoch is 0.0001965822366764769\n",
      "epoch 591,100000 samples processed..., training loss per sample for the current epoch is 0.00019659571466036142\n",
      "epoch 592,100000 samples processed..., training loss per sample for the current epoch is 0.00019658752949908376\n",
      "epoch 593,100000 samples processed..., training loss per sample for the current epoch is 0.0001965912868035957\n",
      "epoch 594,100000 samples processed..., training loss per sample for the current epoch is 0.00019658267498016358\n",
      "epoch 595,100000 samples processed..., training loss per sample for the current epoch is 0.0001965798600576818\n",
      "epoch 596,100000 samples processed..., training loss per sample for the current epoch is 0.0001965833909343928\n",
      "epoch 597,100000 samples processed..., training loss per sample for the current epoch is 0.00019658124190755189\n",
      "epoch 598,100000 samples processed..., training loss per sample for the current epoch is 0.00019658315170090646\n",
      "epoch 599,100000 samples processed..., training loss per sample for the current epoch is 0.00019658585544675587\n",
      "epoch 600,100000 samples processed..., training loss per sample for the current epoch is 0.00019659147597849368\n",
      "epoch 601,100000 samples processed..., training loss per sample for the current epoch is 0.00019660119665786623\n",
      "epoch 602,100000 samples processed..., training loss per sample for the current epoch is 0.00019659128156490624\n",
      "epoch 603,100000 samples processed..., training loss per sample for the current epoch is 0.0001965796371223405\n",
      "epoch 604,100000 samples processed..., training loss per sample for the current epoch is 0.00019657612487208097\n",
      "epoch 605,100000 samples processed..., training loss per sample for the current epoch is 0.00019657345139421524\n",
      "epoch 606,100000 samples processed..., training loss per sample for the current epoch is 0.00019657290948089213\n",
      "epoch 607,100000 samples processed..., training loss per sample for the current epoch is 0.00019657358701806514\n",
      "epoch 608,100000 samples processed..., training loss per sample for the current epoch is 0.00019657477154396474\n",
      "epoch 609,100000 samples processed..., training loss per sample for the current epoch is 0.00019657624536193908\n",
      "epoch 610,100000 samples processed..., training loss per sample for the current epoch is 0.0001965775585267693\n",
      "epoch 611,100000 samples processed..., training loss per sample for the current epoch is 0.00019657851953525096\n",
      "epoch 612,100000 samples processed..., training loss per sample for the current epoch is 0.00019657945085782558\n",
      "epoch 613,100000 samples processed..., training loss per sample for the current epoch is 0.00019658073491882533\n",
      "epoch 614,100000 samples processed..., training loss per sample for the current epoch is 0.00019659055396914482\n",
      "epoch 615,100000 samples processed..., training loss per sample for the current epoch is 0.00019660188059788196\n",
      "epoch 616,100000 samples processed..., training loss per sample for the current epoch is 0.00019659306562971323\n",
      "epoch 617,100000 samples processed..., training loss per sample for the current epoch is 0.00019659220997709782\n",
      "epoch 618,100000 samples processed..., training loss per sample for the current epoch is 0.00019659273268189282\n",
      "epoch 619,100000 samples processed..., training loss per sample for the current epoch is 0.0001965840603224933\n",
      "epoch 620,100000 samples processed..., training loss per sample for the current epoch is 0.00019657523371279239\n",
      "epoch 621,100000 samples processed..., training loss per sample for the current epoch is 0.00019657047174405308\n",
      "epoch 622,100000 samples processed..., training loss per sample for the current epoch is 0.00019656994729302823\n",
      "epoch 623,100000 samples processed..., training loss per sample for the current epoch is 0.0001965702715096995\n",
      "epoch 624,100000 samples processed..., training loss per sample for the current epoch is 0.00019657069351524114\n",
      "epoch 625,100000 samples processed..., training loss per sample for the current epoch is 0.00019657074473798276\n",
      "epoch 626,100000 samples processed..., training loss per sample for the current epoch is 0.0001965703285532072\n",
      "epoch 627,100000 samples processed..., training loss per sample for the current epoch is 0.00019656994030810892\n",
      "epoch 628,100000 samples processed..., training loss per sample for the current epoch is 0.00019657022086903454\n",
      "epoch 629,100000 samples processed..., training loss per sample for the current epoch is 0.0001965710986405611\n",
      "epoch 630,100000 samples processed..., training loss per sample for the current epoch is 0.00019663154613226653\n",
      "epoch 631,100000 samples processed..., training loss per sample for the current epoch is 0.0001965870539424941\n",
      "epoch 632,100000 samples processed..., training loss per sample for the current epoch is 0.00019658822158817203\n",
      "epoch 633,100000 samples processed..., training loss per sample for the current epoch is 0.00019658132048789412\n",
      "epoch 634,100000 samples processed..., training loss per sample for the current epoch is 0.00019657560798805207\n",
      "epoch 635,100000 samples processed..., training loss per sample for the current epoch is 0.0001965727936476469\n",
      "epoch 636,100000 samples processed..., training loss per sample for the current epoch is 0.00019657239376101642\n",
      "epoch 637,100000 samples processed..., training loss per sample for the current epoch is 0.00019657191878650338\n",
      "epoch 638,100000 samples processed..., training loss per sample for the current epoch is 0.00019657259050291032\n",
      "epoch 639,100000 samples processed..., training loss per sample for the current epoch is 0.0001965744374319911\n",
      "epoch 640,100000 samples processed..., training loss per sample for the current epoch is 0.00019657696189824493\n",
      "epoch 641,100000 samples processed..., training loss per sample for the current epoch is 0.0001965812355047092\n",
      "epoch 642,100000 samples processed..., training loss per sample for the current epoch is 0.0001965892472071573\n",
      "epoch 643,100000 samples processed..., training loss per sample for the current epoch is 0.00019659433339256794\n",
      "epoch 644,100000 samples processed..., training loss per sample for the current epoch is 0.00019657670345623045\n",
      "epoch 645,100000 samples processed..., training loss per sample for the current epoch is 0.0001965713658137247\n",
      "epoch 646,100000 samples processed..., training loss per sample for the current epoch is 0.00019657000375445933\n",
      "epoch 647,100000 samples processed..., training loss per sample for the current epoch is 0.000196569433901459\n",
      "epoch 648,100000 samples processed..., training loss per sample for the current epoch is 0.00019657489086966962\n",
      "epoch 649,100000 samples processed..., training loss per sample for the current epoch is 0.00019657103752251714\n",
      "epoch 650,100000 samples processed..., training loss per sample for the current epoch is 0.00019657367549370973\n",
      "epoch 651,100000 samples processed..., training loss per sample for the current epoch is 0.00019658318371511995\n",
      "epoch 652,100000 samples processed..., training loss per sample for the current epoch is 0.0001965788984671235\n",
      "epoch 653,100000 samples processed..., training loss per sample for the current epoch is 0.00019657012191601097\n",
      "epoch 654,100000 samples processed..., training loss per sample for the current epoch is 0.00019657589262351394\n",
      "epoch 655,100000 samples processed..., training loss per sample for the current epoch is 0.0001965912542073056\n",
      "epoch 656,100000 samples processed..., training loss per sample for the current epoch is 0.00019657356315292419\n",
      "epoch 657,100000 samples processed..., training loss per sample for the current epoch is 0.00019657333788927644\n",
      "epoch 658,100000 samples processed..., training loss per sample for the current epoch is 0.0001965690590441227\n",
      "epoch 659,100000 samples processed..., training loss per sample for the current epoch is 0.00019656867894809693\n",
      "epoch 660,100000 samples processed..., training loss per sample for the current epoch is 0.00019656907068565487\n",
      "epoch 661,100000 samples processed..., training loss per sample for the current epoch is 0.00019657886878121644\n",
      "epoch 662,100000 samples processed..., training loss per sample for the current epoch is 0.0001965904433745891\n",
      "epoch 663,100000 samples processed..., training loss per sample for the current epoch is 0.00019659869896713645\n",
      "epoch 664,100000 samples processed..., training loss per sample for the current epoch is 0.00019658414414152502\n",
      "epoch 665,100000 samples processed..., training loss per sample for the current epoch is 0.00019657581055071205\n",
      "epoch 666,100000 samples processed..., training loss per sample for the current epoch is 0.00019657098164316267\n",
      "epoch 667,100000 samples processed..., training loss per sample for the current epoch is 0.00019657117140013723\n",
      "epoch 668,100000 samples processed..., training loss per sample for the current epoch is 0.0001965723081957549\n",
      "epoch 669,100000 samples processed..., training loss per sample for the current epoch is 0.00019657257478684188\n",
      "epoch 670,100000 samples processed..., training loss per sample for the current epoch is 0.0001965726405614987\n",
      "epoch 671,100000 samples processed..., training loss per sample for the current epoch is 0.0001965744362678379\n",
      "epoch 672,100000 samples processed..., training loss per sample for the current epoch is 0.0001965755852870643\n",
      "epoch 673,100000 samples processed..., training loss per sample for the current epoch is 0.00019657591707073152\n",
      "epoch 674,100000 samples processed..., training loss per sample for the current epoch is 0.00019657391880173236\n",
      "epoch 675,100000 samples processed..., training loss per sample for the current epoch is 0.0001965758262667805\n",
      "epoch 676,100000 samples processed..., training loss per sample for the current epoch is 0.0001965787570225075\n",
      "epoch 677,100000 samples processed..., training loss per sample for the current epoch is 0.00019658176694065333\n",
      "epoch 678,100000 samples processed..., training loss per sample for the current epoch is 0.00019658143166452645\n",
      "epoch 679,100000 samples processed..., training loss per sample for the current epoch is 0.00019657173077575863\n",
      "epoch 680,100000 samples processed..., training loss per sample for the current epoch is 0.00019656861841212958\n",
      "epoch 681,100000 samples processed..., training loss per sample for the current epoch is 0.00019656820280943065\n",
      "epoch 682,100000 samples processed..., training loss per sample for the current epoch is 0.00019656731165014208\n",
      "epoch 683,100000 samples processed..., training loss per sample for the current epoch is 0.000196567369857803\n",
      "epoch 684,100000 samples processed..., training loss per sample for the current epoch is 0.00019656842632684857\n",
      "epoch 685,100000 samples processed..., training loss per sample for the current epoch is 0.00019656942051369698\n",
      "epoch 686,100000 samples processed..., training loss per sample for the current epoch is 0.00019656928430777042\n",
      "epoch 687,100000 samples processed..., training loss per sample for the current epoch is 0.0001965680462308228\n",
      "epoch 688,100000 samples processed..., training loss per sample for the current epoch is 0.00019656532036606224\n",
      "epoch 689,100000 samples processed..., training loss per sample for the current epoch is 0.00019656419754028321\n",
      "epoch 690,100000 samples processed..., training loss per sample for the current epoch is 0.00019656530523207039\n",
      "epoch 691,100000 samples processed..., training loss per sample for the current epoch is 0.00019664641935378312\n",
      "epoch 692,100000 samples processed..., training loss per sample for the current epoch is 0.00019657846598420293\n",
      "epoch 693,100000 samples processed..., training loss per sample for the current epoch is 0.00019657200144138187\n",
      "epoch 694,100000 samples processed..., training loss per sample for the current epoch is 0.00019656369113363326\n",
      "epoch 695,100000 samples processed..., training loss per sample for the current epoch is 0.00019656308344565332\n",
      "epoch 696,100000 samples processed..., training loss per sample for the current epoch is 0.00019656357762869446\n",
      "epoch 697,100000 samples processed..., training loss per sample for the current epoch is 0.00019656349904835225\n",
      "epoch 698,100000 samples processed..., training loss per sample for the current epoch is 0.00019656317541375757\n",
      "epoch 699,100000 samples processed..., training loss per sample for the current epoch is 0.0001965628715697676\n",
      "epoch 700,100000 samples processed..., training loss per sample for the current epoch is 0.00019656274758744985\n",
      "epoch 701,100000 samples processed..., training loss per sample for the current epoch is 0.00019656329648569227\n",
      "epoch 702,100000 samples processed..., training loss per sample for the current epoch is 0.00019656878896057606\n",
      "epoch 703,100000 samples processed..., training loss per sample for the current epoch is 0.00019659816985949873\n",
      "epoch 704,100000 samples processed..., training loss per sample for the current epoch is 0.0001965739717707038\n",
      "epoch 705,100000 samples processed..., training loss per sample for the current epoch is 0.00019657029886730015\n",
      "epoch 706,100000 samples processed..., training loss per sample for the current epoch is 0.0001965656311949715\n",
      "epoch 707,100000 samples processed..., training loss per sample for the current epoch is 0.00019656641932670028\n",
      "epoch 708,100000 samples processed..., training loss per sample for the current epoch is 0.00019656947173643858\n",
      "epoch 709,100000 samples processed..., training loss per sample for the current epoch is 0.00019656958989799022\n",
      "epoch 710,100000 samples processed..., training loss per sample for the current epoch is 0.00019657245837152005\n",
      "epoch 711,100000 samples processed..., training loss per sample for the current epoch is 0.00019658870820421726\n",
      "epoch 712,100000 samples processed..., training loss per sample for the current epoch is 0.0001965741562889889\n",
      "epoch 713,100000 samples processed..., training loss per sample for the current epoch is 0.00019657498924061657\n",
      "epoch 714,100000 samples processed..., training loss per sample for the current epoch is 0.000196574138244614\n",
      "epoch 715,100000 samples processed..., training loss per sample for the current epoch is 0.00019657085475046187\n",
      "epoch 716,100000 samples processed..., training loss per sample for the current epoch is 0.00019657433207612485\n",
      "epoch 717,100000 samples processed..., training loss per sample for the current epoch is 0.00019657638680655508\n",
      "epoch 718,100000 samples processed..., training loss per sample for the current epoch is 0.000196577642345801\n",
      "epoch 719,100000 samples processed..., training loss per sample for the current epoch is 0.00019657426339108496\n",
      "epoch 720,100000 samples processed..., training loss per sample for the current epoch is 0.00019656588148791342\n",
      "epoch 721,100000 samples processed..., training loss per sample for the current epoch is 0.00019656331569422035\n",
      "epoch 722,100000 samples processed..., training loss per sample for the current epoch is 0.00019656331976875663\n",
      "epoch 723,100000 samples processed..., training loss per sample for the current epoch is 0.00019656385586131365\n",
      "epoch 724,100000 samples processed..., training loss per sample for the current epoch is 0.0001965643238509074\n",
      "epoch 725,100000 samples processed..., training loss per sample for the current epoch is 0.00019656481686979531\n",
      "epoch 726,100000 samples processed..., training loss per sample for the current epoch is 0.00019656514923553913\n",
      "epoch 727,100000 samples processed..., training loss per sample for the current epoch is 0.0001965647772885859\n",
      "epoch 728,100000 samples processed..., training loss per sample for the current epoch is 0.0001965631276834756\n",
      "epoch 729,100000 samples processed..., training loss per sample for the current epoch is 0.00019656149146612733\n",
      "epoch 730,100000 samples processed..., training loss per sample for the current epoch is 0.00019656093092635274\n",
      "epoch 731,100000 samples processed..., training loss per sample for the current epoch is 0.00019656492106150835\n",
      "epoch 732,100000 samples processed..., training loss per sample for the current epoch is 0.00019663656130433082\n",
      "epoch 733,100000 samples processed..., training loss per sample for the current epoch is 0.000196570556727238\n",
      "epoch 734,100000 samples processed..., training loss per sample for the current epoch is 0.00019656241696793585\n",
      "epoch 735,100000 samples processed..., training loss per sample for the current epoch is 0.00019656020565889776\n",
      "epoch 736,100000 samples processed..., training loss per sample for the current epoch is 0.00019655905256513506\n",
      "epoch 737,100000 samples processed..., training loss per sample for the current epoch is 0.00019655923766549678\n",
      "epoch 738,100000 samples processed..., training loss per sample for the current epoch is 0.00019655964919365943\n",
      "epoch 739,100000 samples processed..., training loss per sample for the current epoch is 0.00019655974349007011\n",
      "epoch 740,100000 samples processed..., training loss per sample for the current epoch is 0.00019655945478007199\n",
      "epoch 741,100000 samples processed..., training loss per sample for the current epoch is 0.0001965592452324927\n",
      "epoch 742,100000 samples processed..., training loss per sample for the current epoch is 0.00019658201548736543\n",
      "epoch 743,100000 samples processed..., training loss per sample for the current epoch is 0.0001965707237832248\n",
      "epoch 744,100000 samples processed..., training loss per sample for the current epoch is 0.00019657894503325224\n",
      "epoch 745,100000 samples processed..., training loss per sample for the current epoch is 0.00019657180237118155\n",
      "epoch 746,100000 samples processed..., training loss per sample for the current epoch is 0.00019656147167552263\n",
      "epoch 747,100000 samples processed..., training loss per sample for the current epoch is 0.00019656255841255188\n",
      "epoch 748,100000 samples processed..., training loss per sample for the current epoch is 0.00019656135642435402\n",
      "epoch 749,100000 samples processed..., training loss per sample for the current epoch is 0.00019656006305012852\n",
      "epoch 750,100000 samples processed..., training loss per sample for the current epoch is 0.00019656918186228723\n",
      "epoch 751,100000 samples processed..., training loss per sample for the current epoch is 0.0001965751580428332\n",
      "epoch 752,100000 samples processed..., training loss per sample for the current epoch is 0.00019659890269394965\n",
      "epoch 753,100000 samples processed..., training loss per sample for the current epoch is 0.00019656783260870724\n",
      "epoch 754,100000 samples processed..., training loss per sample for the current epoch is 0.00019656844786368312\n",
      "epoch 755,100000 samples processed..., training loss per sample for the current epoch is 0.000196570479311049\n",
      "epoch 756,100000 samples processed..., training loss per sample for the current epoch is 0.00019656928954645992\n",
      "epoch 757,100000 samples processed..., training loss per sample for the current epoch is 0.000196567001985386\n",
      "epoch 758,100000 samples processed..., training loss per sample for the current epoch is 0.00019656611431855708\n",
      "epoch 759,100000 samples processed..., training loss per sample for the current epoch is 0.00019656494085211308\n",
      "epoch 760,100000 samples processed..., training loss per sample for the current epoch is 0.0001965626946184784\n",
      "epoch 761,100000 samples processed..., training loss per sample for the current epoch is 0.00019656208110973238\n",
      "epoch 762,100000 samples processed..., training loss per sample for the current epoch is 0.00019656307296827435\n",
      "epoch 763,100000 samples processed..., training loss per sample for the current epoch is 0.00019656484480947255\n",
      "epoch 764,100000 samples processed..., training loss per sample for the current epoch is 0.00019656593911349774\n",
      "epoch 765,100000 samples processed..., training loss per sample for the current epoch is 0.00019656586926430463\n",
      "epoch 766,100000 samples processed..., training loss per sample for the current epoch is 0.00019656596414279192\n",
      "epoch 767,100000 samples processed..., training loss per sample for the current epoch is 0.00019656736636534334\n",
      "epoch 768,100000 samples processed..., training loss per sample for the current epoch is 0.00019657124415971337\n",
      "epoch 769,100000 samples processed..., training loss per sample for the current epoch is 0.00019657643453683705\n",
      "epoch 770,100000 samples processed..., training loss per sample for the current epoch is 0.000196569116669707\n",
      "epoch 771,100000 samples processed..., training loss per sample for the current epoch is 0.00019656233780551702\n",
      "epoch 772,100000 samples processed..., training loss per sample for the current epoch is 0.00019656119809951633\n",
      "epoch 773,100000 samples processed..., training loss per sample for the current epoch is 0.0001965593476779759\n",
      "epoch 774,100000 samples processed..., training loss per sample for the current epoch is 0.00019655866897664965\n",
      "epoch 775,100000 samples processed..., training loss per sample for the current epoch is 0.00019655908865388484\n",
      "epoch 776,100000 samples processed..., training loss per sample for the current epoch is 0.00019655967887956648\n",
      "epoch 777,100000 samples processed..., training loss per sample for the current epoch is 0.0001965599728282541\n",
      "epoch 778,100000 samples processed..., training loss per sample for the current epoch is 0.00019655982847325505\n",
      "epoch 779,100000 samples processed..., training loss per sample for the current epoch is 0.00019655920332297682\n",
      "epoch 780,100000 samples processed..., training loss per sample for the current epoch is 0.00019655845127999782\n",
      "epoch 781,100000 samples processed..., training loss per sample for the current epoch is 0.0001965752971591428\n",
      "epoch 782,100000 samples processed..., training loss per sample for the current epoch is 0.0001965709967771545\n",
      "epoch 783,100000 samples processed..., training loss per sample for the current epoch is 0.00019659091369248928\n",
      "epoch 784,100000 samples processed..., training loss per sample for the current epoch is 0.0001965683966409415\n",
      "epoch 785,100000 samples processed..., training loss per sample for the current epoch is 0.00019656778662465512\n",
      "epoch 786,100000 samples processed..., training loss per sample for the current epoch is 0.00019656481803394853\n",
      "epoch 787,100000 samples processed..., training loss per sample for the current epoch is 0.00019656339020002634\n",
      "epoch 788,100000 samples processed..., training loss per sample for the current epoch is 0.00019656379881780595\n",
      "epoch 789,100000 samples processed..., training loss per sample for the current epoch is 0.00019656400196254253\n",
      "epoch 790,100000 samples processed..., training loss per sample for the current epoch is 0.00019656401011161506\n",
      "epoch 791,100000 samples processed..., training loss per sample for the current epoch is 0.00019656513235531746\n",
      "epoch 792,100000 samples processed..., training loss per sample for the current epoch is 0.00019656489661429077\n",
      "epoch 793,100000 samples processed..., training loss per sample for the current epoch is 0.00019656334130559118\n",
      "epoch 794,100000 samples processed..., training loss per sample for the current epoch is 0.00019656265911180526\n",
      "epoch 795,100000 samples processed..., training loss per sample for the current epoch is 0.00019656334712635725\n",
      "epoch 796,100000 samples processed..., training loss per sample for the current epoch is 0.00019656416145153344\n",
      "epoch 797,100000 samples processed..., training loss per sample for the current epoch is 0.00019656383083201944\n",
      "epoch 798,100000 samples processed..., training loss per sample for the current epoch is 0.00019656391523312778\n",
      "epoch 799,100000 samples processed..., training loss per sample for the current epoch is 0.00019656470452900977\n",
      "epoch 800,100000 samples processed..., training loss per sample for the current epoch is 0.00019656672142446041\n",
      "epoch 801,100000 samples processed..., training loss per sample for the current epoch is 0.0001965677400585264\n",
      "epoch 802,100000 samples processed..., training loss per sample for the current epoch is 0.00019656533375382425\n",
      "epoch 803,100000 samples processed..., training loss per sample for the current epoch is 0.00019656288030091672\n",
      "epoch 804,100000 samples processed..., training loss per sample for the current epoch is 0.00019656288321129978\n",
      "epoch 805,100000 samples processed..., training loss per sample for the current epoch is 0.00019656111602671443\n",
      "epoch 806,100000 samples processed..., training loss per sample for the current epoch is 0.00019656000542454422\n",
      "epoch 807,100000 samples processed..., training loss per sample for the current epoch is 0.000196560068288818\n",
      "epoch 808,100000 samples processed..., training loss per sample for the current epoch is 0.00019656067714095117\n",
      "epoch 809,100000 samples processed..., training loss per sample for the current epoch is 0.00019656167190987617\n",
      "epoch 810,100000 samples processed..., training loss per sample for the current epoch is 0.0001965621009003371\n",
      "epoch 811,100000 samples processed..., training loss per sample for the current epoch is 0.00019656275573652239\n",
      "epoch 812,100000 samples processed..., training loss per sample for the current epoch is 0.00019656454387586563\n",
      "epoch 813,100000 samples processed..., training loss per sample for the current epoch is 0.00019656737102195622\n",
      "epoch 814,100000 samples processed..., training loss per sample for the current epoch is 0.00019657297234516592\n",
      "epoch 815,100000 samples processed..., training loss per sample for the current epoch is 0.0001965748704969883\n",
      "epoch 816,100000 samples processed..., training loss per sample for the current epoch is 0.00019656433491036296\n",
      "epoch 817,100000 samples processed..., training loss per sample for the current epoch is 0.0001965570542961359\n",
      "epoch 818,100000 samples processed..., training loss per sample for the current epoch is 0.0001965566340368241\n",
      "epoch 819,100000 samples processed..., training loss per sample for the current epoch is 0.00019655587791930884\n",
      "epoch 820,100000 samples processed..., training loss per sample for the current epoch is 0.00019655569223687053\n",
      "epoch 821,100000 samples processed..., training loss per sample for the current epoch is 0.00019655633484944702\n",
      "epoch 822,100000 samples processed..., training loss per sample for the current epoch is 0.00019655736221466214\n",
      "epoch 823,100000 samples processed..., training loss per sample for the current epoch is 0.00019655821088235826\n",
      "epoch 824,100000 samples processed..., training loss per sample for the current epoch is 0.0001965585065772757\n",
      "epoch 825,100000 samples processed..., training loss per sample for the current epoch is 0.00019655845360830426\n",
      "epoch 826,100000 samples processed..., training loss per sample for the current epoch is 0.00019655773707199842\n",
      "epoch 827,100000 samples processed..., training loss per sample for the current epoch is 0.00019655764277558773\n",
      "epoch 828,100000 samples processed..., training loss per sample for the current epoch is 0.00019662603386677802\n",
      "epoch 829,100000 samples processed..., training loss per sample for the current epoch is 0.00019657539203763008\n",
      "epoch 830,100000 samples processed..., training loss per sample for the current epoch is 0.00019655762705951928\n",
      "epoch 831,100000 samples processed..., training loss per sample for the current epoch is 0.00019655438838526608\n",
      "epoch 832,100000 samples processed..., training loss per sample for the current epoch is 0.00019655283133033664\n",
      "epoch 833,100000 samples processed..., training loss per sample for the current epoch is 0.00019655244250316173\n",
      "epoch 834,100000 samples processed..., training loss per sample for the current epoch is 0.00019655223120935262\n",
      "epoch 835,100000 samples processed..., training loss per sample for the current epoch is 0.0001965522434329614\n",
      "epoch 836,100000 samples processed..., training loss per sample for the current epoch is 0.00019655233365483581\n",
      "epoch 837,100000 samples processed..., training loss per sample for the current epoch is 0.0001965523767285049\n",
      "epoch 838,100000 samples processed..., training loss per sample for the current epoch is 0.00019655303040053696\n",
      "epoch 839,100000 samples processed..., training loss per sample for the current epoch is 0.00019655609445180745\n",
      "epoch 840,100000 samples processed..., training loss per sample for the current epoch is 0.00019663961313199252\n",
      "epoch 841,100000 samples processed..., training loss per sample for the current epoch is 0.00019656405493151397\n",
      "epoch 842,100000 samples processed..., training loss per sample for the current epoch is 0.00019655651296488942\n",
      "epoch 843,100000 samples processed..., training loss per sample for the current epoch is 0.0001965522562386468\n",
      "epoch 844,100000 samples processed..., training loss per sample for the current epoch is 0.00019655143027193844\n",
      "epoch 845,100000 samples processed..., training loss per sample for the current epoch is 0.00019655015203170478\n",
      "epoch 846,100000 samples processed..., training loss per sample for the current epoch is 0.0001965492661111057\n",
      "epoch 847,100000 samples processed..., training loss per sample for the current epoch is 0.00019654855888802557\n",
      "epoch 848,100000 samples processed..., training loss per sample for the current epoch is 0.00019654795702081173\n",
      "epoch 849,100000 samples processed..., training loss per sample for the current epoch is 0.00019654797099065037\n",
      "epoch 850,100000 samples processed..., training loss per sample for the current epoch is 0.00019655037263873964\n",
      "epoch 851,100000 samples processed..., training loss per sample for the current epoch is 0.00019654483650811017\n",
      "epoch 852,100000 samples processed..., training loss per sample for the current epoch is 0.00019655815325677394\n",
      "epoch 853,100000 samples processed..., training loss per sample for the current epoch is 0.00019628036068752408\n",
      "epoch 854,100000 samples processed..., training loss per sample for the current epoch is 0.0001949046686058864\n",
      "epoch 855,100000 samples processed..., training loss per sample for the current epoch is 0.00019167862017638982\n",
      "epoch 856,100000 samples processed..., training loss per sample for the current epoch is 0.000186117438133806\n",
      "epoch 857,100000 samples processed..., training loss per sample for the current epoch is 0.000178926870576106\n",
      "epoch 858,100000 samples processed..., training loss per sample for the current epoch is 0.000171584491035901\n",
      "epoch 859,100000 samples processed..., training loss per sample for the current epoch is 0.00016601215524133294\n",
      "epoch 860,100000 samples processed..., training loss per sample for the current epoch is 0.00016295954992529005\n",
      "epoch 861,100000 samples processed..., training loss per sample for the current epoch is 0.00016149156261235475\n",
      "epoch 862,100000 samples processed..., training loss per sample for the current epoch is 0.00016069190518464894\n",
      "epoch 863,100000 samples processed..., training loss per sample for the current epoch is 0.0001601446414133534\n",
      "epoch 864,100000 samples processed..., training loss per sample for the current epoch is 0.00015969156636856498\n",
      "epoch 865,100000 samples processed..., training loss per sample for the current epoch is 0.00015924200124572963\n",
      "epoch 866,100000 samples processed..., training loss per sample for the current epoch is 0.00015873872791416944\n",
      "epoch 867,100000 samples processed..., training loss per sample for the current epoch is 0.00015813374833669513\n",
      "epoch 868,100000 samples processed..., training loss per sample for the current epoch is 0.00015718206646852194\n",
      "epoch 869,100000 samples processed..., training loss per sample for the current epoch is 0.0001563093240838498\n",
      "epoch 870,100000 samples processed..., training loss per sample for the current epoch is 0.000155515251099132\n",
      "epoch 871,100000 samples processed..., training loss per sample for the current epoch is 0.00015464602212887258\n",
      "epoch 872,100000 samples processed..., training loss per sample for the current epoch is 0.00015369181288406254\n",
      "epoch 873,100000 samples processed..., training loss per sample for the current epoch is 0.00015265938360244037\n",
      "epoch 874,100000 samples processed..., training loss per sample for the current epoch is 0.00015154974709730596\n",
      "epoch 875,100000 samples processed..., training loss per sample for the current epoch is 0.0001503683812916279\n",
      "epoch 876,100000 samples processed..., training loss per sample for the current epoch is 0.00014905214309692382\n",
      "epoch 877,100000 samples processed..., training loss per sample for the current epoch is 0.00014773915929254145\n",
      "epoch 878,100000 samples processed..., training loss per sample for the current epoch is 0.00014643097412772477\n",
      "epoch 879,100000 samples processed..., training loss per sample for the current epoch is 0.00014518465322908015\n",
      "epoch 880,100000 samples processed..., training loss per sample for the current epoch is 0.00014406693051569164\n",
      "epoch 881,100000 samples processed..., training loss per sample for the current epoch is 0.000143110312637873\n",
      "epoch 882,100000 samples processed..., training loss per sample for the current epoch is 0.00014232294168323279\n",
      "epoch 883,100000 samples processed..., training loss per sample for the current epoch is 0.0001416849537054077\n",
      "epoch 884,100000 samples processed..., training loss per sample for the current epoch is 0.00014121915330179036\n",
      "epoch 885,100000 samples processed..., training loss per sample for the current epoch is 0.0001407190371537581\n",
      "epoch 886,100000 samples processed..., training loss per sample for the current epoch is 0.0001402934704674408\n",
      "epoch 887,100000 samples processed..., training loss per sample for the current epoch is 0.00013996414898429066\n",
      "epoch 888,100000 samples processed..., training loss per sample for the current epoch is 0.0001396684756036848\n",
      "epoch 889,100000 samples processed..., training loss per sample for the current epoch is 0.00013940309931058435\n",
      "epoch 890,100000 samples processed..., training loss per sample for the current epoch is 0.0001391728891758248\n",
      "epoch 891,100000 samples processed..., training loss per sample for the current epoch is 0.00013902256672736256\n",
      "epoch 892,100000 samples processed..., training loss per sample for the current epoch is 0.00013887044915463775\n",
      "epoch 893,100000 samples processed..., training loss per sample for the current epoch is 0.0001387325837276876\n",
      "epoch 894,100000 samples processed..., training loss per sample for the current epoch is 0.0001386339677264914\n",
      "epoch 895,100000 samples processed..., training loss per sample for the current epoch is 0.0001385539845796302\n",
      "epoch 896,100000 samples processed..., training loss per sample for the current epoch is 0.00013848249916918577\n",
      "epoch 897,100000 samples processed..., training loss per sample for the current epoch is 0.0001384394004708156\n",
      "epoch 898,100000 samples processed..., training loss per sample for the current epoch is 0.00013840755156707018\n",
      "epoch 899,100000 samples processed..., training loss per sample for the current epoch is 0.00013835145451594145\n",
      "epoch 900,100000 samples processed..., training loss per sample for the current epoch is 0.0001382564805680886\n",
      "epoch 901,100000 samples processed..., training loss per sample for the current epoch is 0.00013818339735735207\n",
      "epoch 902,100000 samples processed..., training loss per sample for the current epoch is 0.00013811295968480409\n",
      "epoch 903,100000 samples processed..., training loss per sample for the current epoch is 0.00013806597678922116\n",
      "epoch 904,100000 samples processed..., training loss per sample for the current epoch is 0.00013803198467940092\n",
      "epoch 905,100000 samples processed..., training loss per sample for the current epoch is 0.0001380024163518101\n",
      "epoch 906,100000 samples processed..., training loss per sample for the current epoch is 0.00013797351974062622\n",
      "epoch 907,100000 samples processed..., training loss per sample for the current epoch is 0.00013793985592201353\n",
      "epoch 908,100000 samples processed..., training loss per sample for the current epoch is 0.00013790911238174884\n",
      "epoch 909,100000 samples processed..., training loss per sample for the current epoch is 0.00013789058022666722\n",
      "epoch 910,100000 samples processed..., training loss per sample for the current epoch is 0.0001378622784977779\n",
      "epoch 911,100000 samples processed..., training loss per sample for the current epoch is 0.00013783313508611172\n",
      "epoch 912,100000 samples processed..., training loss per sample for the current epoch is 0.0001378133991966024\n",
      "epoch 913,100000 samples processed..., training loss per sample for the current epoch is 0.00013779176748357713\n",
      "epoch 914,100000 samples processed..., training loss per sample for the current epoch is 0.00013776115607470275\n",
      "epoch 915,100000 samples processed..., training loss per sample for the current epoch is 0.0001377227297052741\n",
      "epoch 916,100000 samples processed..., training loss per sample for the current epoch is 0.00013769507291726767\n",
      "epoch 917,100000 samples processed..., training loss per sample for the current epoch is 0.00013767532422207296\n",
      "epoch 918,100000 samples processed..., training loss per sample for the current epoch is 0.00013765683688689024\n",
      "epoch 919,100000 samples processed..., training loss per sample for the current epoch is 0.00013763499446213246\n",
      "epoch 920,100000 samples processed..., training loss per sample for the current epoch is 0.0001376174227334559\n",
      "epoch 921,100000 samples processed..., training loss per sample for the current epoch is 0.00013759633467998354\n",
      "epoch 922,100000 samples processed..., training loss per sample for the current epoch is 0.00013757669774349778\n",
      "epoch 923,100000 samples processed..., training loss per sample for the current epoch is 0.0001375566265778616\n",
      "epoch 924,100000 samples processed..., training loss per sample for the current epoch is 0.00013754322251770644\n",
      "epoch 925,100000 samples processed..., training loss per sample for the current epoch is 0.00013753240171354264\n",
      "epoch 926,100000 samples processed..., training loss per sample for the current epoch is 0.00013753119856119156\n",
      "epoch 927,100000 samples processed..., training loss per sample for the current epoch is 0.00013754299841821195\n",
      "epoch 928,100000 samples processed..., training loss per sample for the current epoch is 0.00013754046347457915\n",
      "epoch 929,100000 samples processed..., training loss per sample for the current epoch is 0.00013750025653280317\n",
      "epoch 930,100000 samples processed..., training loss per sample for the current epoch is 0.00013744529453106226\n",
      "epoch 931,100000 samples processed..., training loss per sample for the current epoch is 0.00013741460745222866\n",
      "epoch 932,100000 samples processed..., training loss per sample for the current epoch is 0.0001373990176944062\n",
      "epoch 933,100000 samples processed..., training loss per sample for the current epoch is 0.00013738499314058572\n",
      "epoch 934,100000 samples processed..., training loss per sample for the current epoch is 0.000137370394077152\n",
      "epoch 935,100000 samples processed..., training loss per sample for the current epoch is 0.00013735599350184202\n",
      "epoch 936,100000 samples processed..., training loss per sample for the current epoch is 0.00013734729727730156\n",
      "epoch 937,100000 samples processed..., training loss per sample for the current epoch is 0.0001373379211872816\n",
      "epoch 938,100000 samples processed..., training loss per sample for the current epoch is 0.00013732767198234796\n",
      "epoch 939,100000 samples processed..., training loss per sample for the current epoch is 0.00013732888794038445\n",
      "epoch 940,100000 samples processed..., training loss per sample for the current epoch is 0.00013732680352404714\n",
      "epoch 941,100000 samples processed..., training loss per sample for the current epoch is 0.00013732196122873575\n",
      "epoch 942,100000 samples processed..., training loss per sample for the current epoch is 0.00013729709258768707\n",
      "epoch 943,100000 samples processed..., training loss per sample for the current epoch is 0.0001372614601859823\n",
      "epoch 944,100000 samples processed..., training loss per sample for the current epoch is 0.0001372426707530394\n",
      "epoch 945,100000 samples processed..., training loss per sample for the current epoch is 0.00013722607167437672\n",
      "epoch 946,100000 samples processed..., training loss per sample for the current epoch is 0.00013721240626182407\n",
      "epoch 947,100000 samples processed..., training loss per sample for the current epoch is 0.0001371963490964845\n",
      "epoch 948,100000 samples processed..., training loss per sample for the current epoch is 0.00013717921741772442\n",
      "epoch 949,100000 samples processed..., training loss per sample for the current epoch is 0.0001371677132556215\n",
      "epoch 950,100000 samples processed..., training loss per sample for the current epoch is 0.00013715735753066838\n",
      "epoch 951,100000 samples processed..., training loss per sample for the current epoch is 0.00013715389592107387\n",
      "epoch 952,100000 samples processed..., training loss per sample for the current epoch is 0.0001371504261624068\n",
      "epoch 953,100000 samples processed..., training loss per sample for the current epoch is 0.00013714711705688386\n",
      "epoch 954,100000 samples processed..., training loss per sample for the current epoch is 0.00013713509775698185\n",
      "epoch 955,100000 samples processed..., training loss per sample for the current epoch is 0.00013713054009713234\n",
      "epoch 956,100000 samples processed..., training loss per sample for the current epoch is 0.00013711367442738266\n",
      "epoch 957,100000 samples processed..., training loss per sample for the current epoch is 0.00013708990998566152\n",
      "epoch 958,100000 samples processed..., training loss per sample for the current epoch is 0.00013708130398299546\n",
      "epoch 959,100000 samples processed..., training loss per sample for the current epoch is 0.0001370604202384129\n",
      "epoch 960,100000 samples processed..., training loss per sample for the current epoch is 0.00013703658070880919\n",
      "epoch 961,100000 samples processed..., training loss per sample for the current epoch is 0.0001370183506514877\n",
      "epoch 962,100000 samples processed..., training loss per sample for the current epoch is 0.00013699747796636075\n",
      "epoch 963,100000 samples processed..., training loss per sample for the current epoch is 0.00013698549766559155\n",
      "epoch 964,100000 samples processed..., training loss per sample for the current epoch is 0.0001369777787476778\n",
      "epoch 965,100000 samples processed..., training loss per sample for the current epoch is 0.00013696636364329606\n",
      "epoch 966,100000 samples processed..., training loss per sample for the current epoch is 0.00013695421628654003\n",
      "epoch 967,100000 samples processed..., training loss per sample for the current epoch is 0.00013694076216779649\n",
      "epoch 968,100000 samples processed..., training loss per sample for the current epoch is 0.0001369317714124918\n",
      "epoch 969,100000 samples processed..., training loss per sample for the current epoch is 0.0001369115844136104\n",
      "epoch 970,100000 samples processed..., training loss per sample for the current epoch is 0.00013688817503862083\n",
      "epoch 971,100000 samples processed..., training loss per sample for the current epoch is 0.00013687665283214302\n",
      "epoch 972,100000 samples processed..., training loss per sample for the current epoch is 0.00013684533187188208\n",
      "epoch 973,100000 samples processed..., training loss per sample for the current epoch is 0.00013681137061212213\n",
      "epoch 974,100000 samples processed..., training loss per sample for the current epoch is 0.0001367759902495891\n",
      "epoch 975,100000 samples processed..., training loss per sample for the current epoch is 0.000136722790193744\n",
      "epoch 976,100000 samples processed..., training loss per sample for the current epoch is 0.00013668323284946382\n",
      "epoch 977,100000 samples processed..., training loss per sample for the current epoch is 0.00013664044614415617\n",
      "epoch 978,100000 samples processed..., training loss per sample for the current epoch is 0.00013658270181622357\n",
      "epoch 979,100000 samples processed..., training loss per sample for the current epoch is 0.0001365008787252009\n",
      "epoch 980,100000 samples processed..., training loss per sample for the current epoch is 0.00013640104443766177\n",
      "epoch 981,100000 samples processed..., training loss per sample for the current epoch is 0.0001363096380373463\n",
      "epoch 982,100000 samples processed..., training loss per sample for the current epoch is 0.00013620503596030177\n",
      "epoch 983,100000 samples processed..., training loss per sample for the current epoch is 0.00013605415995698422\n",
      "epoch 984,100000 samples processed..., training loss per sample for the current epoch is 0.0001358855894068256\n",
      "epoch 985,100000 samples processed..., training loss per sample for the current epoch is 0.0001357158157043159\n",
      "epoch 986,100000 samples processed..., training loss per sample for the current epoch is 0.00013557346537709236\n",
      "epoch 987,100000 samples processed..., training loss per sample for the current epoch is 0.00013544501271098853\n",
      "epoch 988,100000 samples processed..., training loss per sample for the current epoch is 0.00013524614507332445\n",
      "epoch 989,100000 samples processed..., training loss per sample for the current epoch is 0.00013499726133886725\n",
      "epoch 990,100000 samples processed..., training loss per sample for the current epoch is 0.00013479000597726553\n",
      "epoch 991,100000 samples processed..., training loss per sample for the current epoch is 0.0001345654996111989\n",
      "epoch 992,100000 samples processed..., training loss per sample for the current epoch is 0.00013430581253487617\n",
      "epoch 993,100000 samples processed..., training loss per sample for the current epoch is 0.00013404794444795697\n",
      "epoch 994,100000 samples processed..., training loss per sample for the current epoch is 0.0001337826787494123\n",
      "epoch 995,100000 samples processed..., training loss per sample for the current epoch is 0.00013349313056096435\n",
      "epoch 996,100000 samples processed..., training loss per sample for the current epoch is 0.00013317741162609308\n",
      "epoch 997,100000 samples processed..., training loss per sample for the current epoch is 0.00013282058876939117\n",
      "epoch 998,100000 samples processed..., training loss per sample for the current epoch is 0.00013242072891443967\n",
      "epoch 999,100000 samples processed..., training loss per sample for the current epoch is 0.0001319787639658898\n",
      "Autoencoder15 training DONE... TOTAL TIME = 180.3845717906952\n",
      "start evaluation on test data for Autoencoder15\n",
      "MSE is 0.06388508144334018\n",
      "mutls per sample is 65680\n",
      "----------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "save_path = '/data/shiyu/projects/MT/MT_ICML_OOP/GW/trained_models_info/AEs_GW.pkl'\n",
    "\n",
    "AEs_training_loss_per_sample_epochs = []\n",
    "AEs_test_MSE_list = []\n",
    "AEs_mults_per_sample_list = []\n",
    "\n",
    "all_states = {}\n",
    "\n",
    "for name, model_class in AUTOENCODER_CLASSES.items():\n",
    "    print(f'\\nTraining {name}')\n",
    "    model = model_class()\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    num_epochs  = 1000\n",
    "    batch_size = 4000 \n",
    "\n",
    "    #be consistent with MT.  if use all samples, num_batches = N_train // batch_size  + ((N_train % batch_size) != 0), num_samples_used = N_train, end_index = min(start_index + batch_size, N_train) \n",
    "    num_batches = N_train // batch_size + ((N_train % batch_size) != 0)\n",
    "    \n",
    "\n",
    "\n",
    "    training_loss_per_sample_list = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_curr_epoch = 0\n",
    "        for i in range(num_batches): \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            start_index = i * batch_size\n",
    "            end_index = min(start_index + batch_size, N_train)\n",
    "            batch_input = X_train_tensor[start_index : end_index].to(device)\n",
    "            batch_output = model(batch_input)\n",
    "            \n",
    "            loss =  criterion(batch_output, batch_input) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_samples_curr_batch = end_index - start_index\n",
    "            loss_curr_epoch += loss.item() * num_samples_curr_batch # batch_size -> batch_end - start_index\n",
    "\n",
    "            \n",
    "        training_loss_per_sample_curr_epoch = loss_curr_epoch / N_train\n",
    "        training_loss_per_sample_list.append(training_loss_per_sample_curr_epoch)\n",
    "\n",
    "        print(f\"epoch {epoch},{end_index} samples processed..., training loss per sample for the current epoch is {training_loss_per_sample_curr_epoch}\")\n",
    "\n",
    "    AEs_training_loss_per_sample_epochs.append(training_loss_per_sample_list) #store training losses for all AEs\n",
    "        \n",
    "    end_time = time.time()\n",
    "    print(f\"{name} training DONE... TOTAL TIME = {end_time - start_time}\")\n",
    "    \n",
    "\n",
    "    all_states[name] =  model.state_dict()\n",
    "\n",
    "    # evaluation on test data\n",
    "    print(f'start evaluation on test data for {name}')\n",
    "    with torch.no_grad():\n",
    "        X_test_denoised = model(X_test_tensor.to(device))\n",
    "\n",
    "    test_MSE = np.sum((X_test_denoised.detach().cpu().numpy() - X_natural_test)**2) / N_test\n",
    "    print(f'MSE is {test_MSE}')\n",
    "    AEs_test_MSE_list.append(test_MSE)\n",
    "\n",
    "\n",
    "    mults_per_sample = count_multiplications_per_sample_AE(model)\n",
    "    print(f'mutls per sample is {mults_per_sample}')\n",
    "    AEs_mults_per_sample_list.append(mults_per_sample)\n",
    "    print('----------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "if save_path:\n",
    "    AEs_info = {\n",
    "    'all_states': all_states,\n",
    "    'AEs_training_loss_per_sample_epochs': AEs_training_loss_per_sample_epochs,\n",
    "    'AEs_test_MSE_list': AEs_test_MSE_list,\n",
    "    'AEs_mults_per_sample_list': AEs_mults_per_sample_list,\n",
    "    }\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(AEs_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9724fc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot AEs training loss curve\n",
    "for i, (key, value) in enumerate(AUTOENCODER_CLASSES.items()):\n",
    "    plt.figure()\n",
    "    plt.plot(AEs_training_loss_per_sample_epochs[i])\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('training loss per sample')\n",
    "    plt.title(f'{key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0cfca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot trade-off curve for AEs\n",
    "\n",
    "for i in range(len(AEs_test_MSE_list)): \n",
    "    plt.scatter(AEs_test_MSE_list[i], np.log10(AEs_mults_per_sample_list[i]),  marker='o', color='blue')\n",
    "    plt.text(AEs_test_MSE_list[i] + 0.015, np.log10(AEs_mults_per_sample_list[i] + 0.015), str(i+1), fontsize=8) \n",
    "\n",
    "\n",
    "plt.title('Test-Time Complexity-Accuracy Tradeoff of AE')\n",
    "plt.ylabel(\"log of (\\#Multiplications per sample)\")\n",
    "plt.xlabel(\"Mean Squared Error\")\n",
    "plt.legend(['AE'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297dcb8",
   "metadata": {},
   "source": [
    "# train and analyze MT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9245e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_natural_train = X_natural_train.T\n",
    "X_natural_test = X_natural_test.T\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aabbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "networks_wrapper_GW = MTNetworksWrapper(X_train=X_train, X_natural_train=X_natural_train, X_test=X_test, X_natural_test=X_natural_test,\n",
    "                          sigma=sigma, d=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14865d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 12 networks with different hyperparameters...\n",
      "================================================================================\n",
      "\n",
      "Training NETWORK_1\n",
      "Config: {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 3.71s)\n",
      "8000 samples processed (batch time: 5.57s)\n",
      "12000 samples processed (batch time: 6.90s)\n",
      "16000 samples processed (batch time: 7.70s)\n",
      "20000 samples processed (batch time: 8.76s)\n",
      "24000 samples processed (batch time: 10.00s)\n",
      "28000 samples processed (batch time: 11.64s)\n",
      "32000 samples processed (batch time: 13.42s)\n",
      "36000 samples processed (batch time: 14.16s)\n",
      "40000 samples processed (batch time: 14.87s)\n",
      "44000 samples processed (batch time: 15.50s)\n",
      "48000 samples processed (batch time: 16.05s)\n",
      "52000 samples processed (batch time: 16.96s)\n",
      "56000 samples processed (batch time: 15.84s)\n",
      "60000 samples processed (batch time: 16.18s)\n",
      "64000 samples processed (batch time: 16.92s)\n",
      "68000 samples processed (batch time: 17.55s)\n",
      "72000 samples processed (batch time: 16.27s)\n",
      "76000 samples processed (batch time: 17.56s)\n",
      "80000 samples processed (batch time: 17.53s)\n",
      "84000 samples processed (batch time: 17.53s)\n",
      "88000 samples processed (batch time: 17.61s)\n",
      "92000 samples processed (batch time: 18.33s)\n",
      "96000 samples processed (batch time: 18.06s)\n",
      "100000 samples processed (batch time: 18.58s)\n",
      "Training complete! Total time: 353.20s\n",
      "Network statistics: {'num_landmarks': 1577, 'total_first_order_edges': 125381, 'total_zero_order_edges': 1968, 'total_points_assigned': 100000}\n",
      "Training completed in 353.20s\n",
      "Network stats: {'num_landmarks': 1577, 'total_first_order_edges': 125381, 'total_zero_order_edges': 1968, 'total_points_assigned': 100000, 'training_time': 353.2019114494324, 'config': {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_2\n",
      "Config: {'R_is_const': True, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 1.58s)\n",
      "8000 samples processed (batch time: 1.62s)\n",
      "12000 samples processed (batch time: 1.73s)\n",
      "16000 samples processed (batch time: 1.74s)\n",
      "20000 samples processed (batch time: 2.50s)\n",
      "24000 samples processed (batch time: 2.23s)\n",
      "28000 samples processed (batch time: 1.71s)\n",
      "32000 samples processed (batch time: 1.78s)\n",
      "36000 samples processed (batch time: 1.76s)\n",
      "40000 samples processed (batch time: 1.77s)\n",
      "44000 samples processed (batch time: 1.76s)\n",
      "48000 samples processed (batch time: 1.76s)\n",
      "52000 samples processed (batch time: 1.81s)\n",
      "56000 samples processed (batch time: 1.87s)\n",
      "60000 samples processed (batch time: 1.86s)\n",
      "64000 samples processed (batch time: 2.00s)\n",
      "68000 samples processed (batch time: 1.91s)\n",
      "72000 samples processed (batch time: 1.87s)\n",
      "76000 samples processed (batch time: 1.89s)\n",
      "80000 samples processed (batch time: 1.88s)\n",
      "84000 samples processed (batch time: 1.89s)\n",
      "88000 samples processed (batch time: 1.89s)\n",
      "92000 samples processed (batch time: 1.90s)\n",
      "96000 samples processed (batch time: 1.89s)\n",
      "100000 samples processed (batch time: 1.90s)\n",
      "Training complete! Total time: 46.48s\n",
      "Network statistics: {'num_landmarks': 176, 'total_first_order_edges': 588, 'total_zero_order_edges': 340, 'total_points_assigned': 100000}\n",
      "Training completed in 46.48s\n",
      "Network stats: {'num_landmarks': 176, 'total_first_order_edges': 588, 'total_zero_order_edges': 340, 'total_points_assigned': 100000, 'training_time': 46.48210835456848, 'config': {'R_is_const': True, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_3\n",
      "Config: {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.04, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 3.05s)\n",
      "8000 samples processed (batch time: 5.46s)\n",
      "12000 samples processed (batch time: 7.03s)\n",
      "16000 samples processed (batch time: 8.18s)\n",
      "20000 samples processed (batch time: 9.05s)\n",
      "24000 samples processed (batch time: 9.18s)\n",
      "28000 samples processed (batch time: 9.72s)\n",
      "32000 samples processed (batch time: 10.37s)\n",
      "36000 samples processed (batch time: 10.08s)\n",
      "40000 samples processed (batch time: 10.50s)\n",
      "44000 samples processed (batch time: 10.69s)\n",
      "48000 samples processed (batch time: 11.27s)\n",
      "52000 samples processed (batch time: 11.12s)\n",
      "56000 samples processed (batch time: 11.72s)\n",
      "60000 samples processed (batch time: 11.96s)\n",
      "64000 samples processed (batch time: 12.15s)\n",
      "68000 samples processed (batch time: 12.41s)\n",
      "72000 samples processed (batch time: 12.41s)\n",
      "76000 samples processed (batch time: 12.78s)\n",
      "80000 samples processed (batch time: 14.10s)\n",
      "84000 samples processed (batch time: 14.09s)\n",
      "88000 samples processed (batch time: 14.12s)\n",
      "92000 samples processed (batch time: 14.25s)\n",
      "96000 samples processed (batch time: 14.32s)\n",
      "100000 samples processed (batch time: 14.40s)\n",
      "Training complete! Total time: 274.41s\n",
      "Network statistics: {'num_landmarks': 1307, 'total_first_order_edges': 80403, 'total_zero_order_edges': 1732, 'total_points_assigned': 100000}\n",
      "Training completed in 274.42s\n",
      "Network stats: {'num_landmarks': 1307, 'total_first_order_edges': 80403, 'total_zero_order_edges': 1732, 'total_points_assigned': 100000, 'training_time': 274.4177453517914, 'config': {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.04, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_4\n",
      "Config: {'R_is_const': True, 'R_denoising': 0.7504665215717488, 'R_1st_order_nbhd': 0.7504665215717488, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 1.20s)\n",
      "8000 samples processed (batch time: 1.22s)\n",
      "12000 samples processed (batch time: 1.22s)\n",
      "16000 samples processed (batch time: 1.24s)\n",
      "20000 samples processed (batch time: 1.24s)\n",
      "24000 samples processed (batch time: 1.24s)\n",
      "28000 samples processed (batch time: 1.25s)\n",
      "32000 samples processed (batch time: 1.27s)\n",
      "36000 samples processed (batch time: 1.29s)\n",
      "40000 samples processed (batch time: 1.30s)\n",
      "44000 samples processed (batch time: 1.30s)\n",
      "48000 samples processed (batch time: 1.30s)\n",
      "52000 samples processed (batch time: 1.31s)\n",
      "56000 samples processed (batch time: 1.32s)\n",
      "60000 samples processed (batch time: 1.31s)\n",
      "64000 samples processed (batch time: 1.31s)\n",
      "68000 samples processed (batch time: 1.32s)\n",
      "72000 samples processed (batch time: 1.30s)\n",
      "76000 samples processed (batch time: 1.31s)\n",
      "80000 samples processed (batch time: 1.32s)\n",
      "84000 samples processed (batch time: 1.33s)\n",
      "88000 samples processed (batch time: 1.32s)\n",
      "92000 samples processed (batch time: 1.34s)\n",
      "96000 samples processed (batch time: 1.39s)\n",
      "100000 samples processed (batch time: 1.34s)\n",
      "Training complete! Total time: 32.28s\n",
      "Network statistics: {'num_landmarks': 49, 'total_first_order_edges': 49, 'total_zero_order_edges': 114, 'total_points_assigned': 100000}\n",
      "Training completed in 32.28s\n",
      "Network stats: {'num_landmarks': 49, 'total_first_order_edges': 49, 'total_zero_order_edges': 114, 'total_points_assigned': 100000, 'training_time': 32.280478715896606, 'config': {'R_is_const': True, 'R_denoising': 0.7504665215717488, 'R_1st_order_nbhd': 0.7504665215717488, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_5\n",
      "Config: {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.3, 'exp_coeff': 0.3333333333333333}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 3.36s)\n",
      "8000 samples processed (batch time: 4.38s)\n",
      "12000 samples processed (batch time: 4.28s)\n",
      "16000 samples processed (batch time: 4.38s)\n",
      "20000 samples processed (batch time: 4.43s)\n",
      "24000 samples processed (batch time: 4.21s)\n",
      "28000 samples processed (batch time: 4.23s)\n",
      "32000 samples processed (batch time: 4.36s)\n",
      "36000 samples processed (batch time: 4.48s)\n",
      "40000 samples processed (batch time: 4.52s)\n",
      "44000 samples processed (batch time: 4.62s)\n",
      "48000 samples processed (batch time: 4.68s)\n",
      "52000 samples processed (batch time: 4.52s)\n",
      "56000 samples processed (batch time: 4.67s)\n",
      "60000 samples processed (batch time: 4.76s)\n",
      "64000 samples processed (batch time: 4.82s)\n",
      "68000 samples processed (batch time: 4.73s)\n",
      "72000 samples processed (batch time: 4.78s)\n",
      "76000 samples processed (batch time: 4.73s)\n",
      "80000 samples processed (batch time: 4.65s)\n",
      "84000 samples processed (batch time: 4.66s)\n",
      "88000 samples processed (batch time: 4.81s)\n",
      "92000 samples processed (batch time: 4.74s)\n",
      "96000 samples processed (batch time: 4.99s)\n",
      "100000 samples processed (batch time: 4.74s)\n",
      "Training complete! Total time: 113.51s\n",
      "Network statistics: {'num_landmarks': 365, 'total_first_order_edges': 7115, 'total_zero_order_edges': 632, 'total_points_assigned': 100000}\n",
      "Training completed in 113.52s\n",
      "Network stats: {'num_landmarks': 365, 'total_first_order_edges': 7115, 'total_zero_order_edges': 632, 'total_points_assigned': 100000, 'training_time': 113.51617884635925, 'config': {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.3, 'exp_coeff': 0.3333333333333333}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_6\n",
      "Config: {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.0282842712474619, 'prod_coeff': 1.15, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 4.96s)\n",
      "8000 samples processed (batch time: 8.85s)\n",
      "12000 samples processed (batch time: 12.79s)\n",
      "16000 samples processed (batch time: 14.79s)\n",
      "20000 samples processed (batch time: 17.66s)\n",
      "24000 samples processed (batch time: 19.19s)\n",
      "28000 samples processed (batch time: 21.90s)\n",
      "32000 samples processed (batch time: 23.38s)\n",
      "36000 samples processed (batch time: 25.77s)\n",
      "40000 samples processed (batch time: 25.83s)\n",
      "44000 samples processed (batch time: 25.80s)\n",
      "48000 samples processed (batch time: 28.06s)\n",
      "52000 samples processed (batch time: 30.39s)\n",
      "56000 samples processed (batch time: 27.80s)\n",
      "60000 samples processed (batch time: 29.60s)\n",
      "64000 samples processed (batch time: 28.26s)\n",
      "68000 samples processed (batch time: 28.84s)\n",
      "72000 samples processed (batch time: 30.83s)\n",
      "76000 samples processed (batch time: 30.99s)\n",
      "80000 samples processed (batch time: 33.14s)\n",
      "84000 samples processed (batch time: 33.03s)\n",
      "88000 samples processed (batch time: 32.84s)\n",
      "92000 samples processed (batch time: 37.32s)\n",
      "96000 samples processed (batch time: 37.64s)\n",
      "100000 samples processed (batch time: 38.37s)\n",
      "Training complete! Total time: 648.04s\n",
      "Network statistics: {'num_landmarks': 2445, 'total_first_order_edges': 415965, 'total_zero_order_edges': 2993, 'total_points_assigned': 100000}\n",
      "Training completed in 648.05s\n",
      "Network stats: {'num_landmarks': 2445, 'total_first_order_edges': 415965, 'total_zero_order_edges': 2993, 'total_points_assigned': 100000, 'training_time': 648.0489010810852, 'config': {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.0282842712474619, 'prod_coeff': 1.15, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_7\n",
      "Config: {'R_is_const': True, 'R_denoising': 0.6996227554904143, 'R_1st_order_nbhd': 0.7504665215717488, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 1.26s)\n",
      "8000 samples processed (batch time: 1.31s)\n",
      "12000 samples processed (batch time: 1.39s)\n",
      "16000 samples processed (batch time: 1.45s)\n",
      "20000 samples processed (batch time: 1.45s)\n",
      "24000 samples processed (batch time: 1.46s)\n",
      "28000 samples processed (batch time: 1.46s)\n",
      "32000 samples processed (batch time: 1.47s)\n",
      "36000 samples processed (batch time: 1.46s)\n",
      "40000 samples processed (batch time: 1.49s)\n",
      "44000 samples processed (batch time: 1.50s)\n",
      "48000 samples processed (batch time: 1.51s)\n",
      "52000 samples processed (batch time: 1.51s)\n",
      "56000 samples processed (batch time: 1.52s)\n",
      "60000 samples processed (batch time: 1.51s)\n",
      "64000 samples processed (batch time: 1.52s)\n",
      "68000 samples processed (batch time: 1.52s)\n",
      "72000 samples processed (batch time: 1.52s)\n",
      "76000 samples processed (batch time: 1.53s)\n",
      "80000 samples processed (batch time: 1.53s)\n",
      "84000 samples processed (batch time: 1.53s)\n",
      "88000 samples processed (batch time: 1.52s)\n",
      "92000 samples processed (batch time: 1.52s)\n",
      "96000 samples processed (batch time: 1.51s)\n",
      "100000 samples processed (batch time: 1.56s)\n",
      "Training complete! Total time: 37.01s\n",
      "Network statistics: {'num_landmarks': 68, 'total_first_order_edges': 146, 'total_zero_order_edges': 176, 'total_points_assigned': 100000}\n",
      "Training completed in 37.01s\n",
      "Network stats: {'num_landmarks': 68, 'total_first_order_edges': 146, 'total_zero_order_edges': 176, 'total_points_assigned': 100000, 'training_time': 37.009451389312744, 'config': {'R_is_const': True, 'R_denoising': 0.6996227554904143, 'R_1st_order_nbhd': 0.7504665215717488, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_8\n",
      "Config: {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.07745966692414834, 'prod_coeff': 1.5, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 2.07s)\n",
      "8000 samples processed (batch time: 2.32s)\n",
      "12000 samples processed (batch time: 2.48s)\n",
      "16000 samples processed (batch time: 2.54s)\n",
      "20000 samples processed (batch time: 2.58s)\n",
      "24000 samples processed (batch time: 2.66s)\n",
      "28000 samples processed (batch time: 2.70s)\n",
      "32000 samples processed (batch time: 2.74s)\n",
      "36000 samples processed (batch time: 2.83s)\n",
      "40000 samples processed (batch time: 2.88s)\n",
      "44000 samples processed (batch time: 2.95s)\n",
      "48000 samples processed (batch time: 2.96s)\n",
      "52000 samples processed (batch time: 2.99s)\n",
      "56000 samples processed (batch time: 3.01s)\n",
      "60000 samples processed (batch time: 3.01s)\n",
      "64000 samples processed (batch time: 3.05s)\n",
      "68000 samples processed (batch time: 3.05s)\n",
      "72000 samples processed (batch time: 3.02s)\n",
      "76000 samples processed (batch time: 3.03s)\n",
      "80000 samples processed (batch time: 2.98s)\n",
      "84000 samples processed (batch time: 2.97s)\n",
      "88000 samples processed (batch time: 3.04s)\n",
      "92000 samples processed (batch time: 3.03s)\n",
      "96000 samples processed (batch time: 3.06s)\n",
      "100000 samples processed (batch time: 3.05s)\n",
      "Training complete! Total time: 71.00s\n",
      "Network statistics: {'num_landmarks': 201, 'total_first_order_edges': 1945, 'total_zero_order_edges': 451, 'total_points_assigned': 100000}\n",
      "Training completed in 71.01s\n",
      "Network stats: {'num_landmarks': 201, 'total_first_order_edges': 1945, 'total_zero_order_edges': 451, 'total_points_assigned': 100000, 'training_time': 71.0058364868164, 'config': {'R_is_const': False, 'R_denoising': 0.6495290601659021, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.07745966692414834, 'prod_coeff': 1.5, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_9\n",
      "Config: {'R_is_const': True, 'R_denoising': 0.64, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 2.03s)\n",
      "8000 samples processed (batch time: 2.08s)\n",
      "12000 samples processed (batch time: 2.13s)\n",
      "16000 samples processed (batch time: 2.19s)\n",
      "20000 samples processed (batch time: 2.22s)\n",
      "24000 samples processed (batch time: 2.25s)\n",
      "28000 samples processed (batch time: 2.27s)\n",
      "32000 samples processed (batch time: 2.29s)\n",
      "36000 samples processed (batch time: 2.28s)\n",
      "40000 samples processed (batch time: 2.28s)\n",
      "44000 samples processed (batch time: 2.26s)\n",
      "48000 samples processed (batch time: 2.32s)\n",
      "52000 samples processed (batch time: 2.34s)\n",
      "56000 samples processed (batch time: 2.30s)\n",
      "60000 samples processed (batch time: 2.33s)\n",
      "64000 samples processed (batch time: 2.34s)\n",
      "68000 samples processed (batch time: 2.31s)\n",
      "72000 samples processed (batch time: 2.30s)\n",
      "76000 samples processed (batch time: 2.34s)\n",
      "80000 samples processed (batch time: 2.31s)\n",
      "84000 samples processed (batch time: 2.33s)\n",
      "88000 samples processed (batch time: 2.34s)\n",
      "92000 samples processed (batch time: 2.35s)\n",
      "96000 samples processed (batch time: 2.33s)\n",
      "100000 samples processed (batch time: 2.37s)\n",
      "Training complete! Total time: 56.91s\n",
      "Network statistics: {'num_landmarks': 273, 'total_first_order_edges': 1447, 'total_zero_order_edges': 474, 'total_points_assigned': 100000}\n",
      "Training completed in 56.91s\n",
      "Network stats: {'num_landmarks': 273, 'total_first_order_edges': 1447, 'total_zero_order_edges': 474, 'total_points_assigned': 100000, 'training_time': 56.910062313079834, 'config': {'R_is_const': True, 'R_denoising': 0.64, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_10\n",
      "Config: {'R_is_const': True, 'R_denoising': 0.6697103851666032, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 1.35s)\n",
      "8000 samples processed (batch time: 1.39s)\n",
      "12000 samples processed (batch time: 1.44s)\n",
      "16000 samples processed (batch time: 1.44s)\n",
      "20000 samples processed (batch time: 1.49s)\n",
      "24000 samples processed (batch time: 1.51s)\n",
      "28000 samples processed (batch time: 1.47s)\n",
      "32000 samples processed (batch time: 1.51s)\n",
      "36000 samples processed (batch time: 1.54s)\n",
      "40000 samples processed (batch time: 1.52s)\n",
      "44000 samples processed (batch time: 1.52s)\n",
      "48000 samples processed (batch time: 1.57s)\n",
      "52000 samples processed (batch time: 1.55s)\n",
      "56000 samples processed (batch time: 1.54s)\n",
      "60000 samples processed (batch time: 1.57s)\n",
      "64000 samples processed (batch time: 1.57s)\n",
      "68000 samples processed (batch time: 1.57s)\n",
      "72000 samples processed (batch time: 1.59s)\n",
      "76000 samples processed (batch time: 1.59s)\n",
      "80000 samples processed (batch time: 1.60s)\n",
      "84000 samples processed (batch time: 1.61s)\n",
      "88000 samples processed (batch time: 1.62s)\n",
      "92000 samples processed (batch time: 1.62s)\n",
      "96000 samples processed (batch time: 1.61s)\n",
      "100000 samples processed (batch time: 1.63s)\n",
      "Training complete! Total time: 38.42s\n",
      "Network statistics: {'num_landmarks': 121, 'total_first_order_edges': 275, 'total_zero_order_edges': 255, 'total_points_assigned': 100000}\n",
      "Training completed in 38.42s\n",
      "Network stats: {'num_landmarks': 121, 'total_first_order_edges': 275, 'total_zero_order_edges': 255, 'total_points_assigned': 100000, 'training_time': 38.421212673187256, 'config': {'R_is_const': True, 'R_denoising': 0.6697103851666032, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_11\n",
      "Config: {'R_is_const': True, 'R_denoising': 0.8006397442045955, 'R_1st_order_nbhd': 0.8502611363575311, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 1.22s)\n",
      "8000 samples processed (batch time: 1.24s)\n",
      "12000 samples processed (batch time: 1.24s)\n",
      "16000 samples processed (batch time: 1.25s)\n",
      "20000 samples processed (batch time: 1.25s)\n",
      "24000 samples processed (batch time: 1.25s)\n",
      "28000 samples processed (batch time: 1.25s)\n",
      "32000 samples processed (batch time: 1.26s)\n",
      "36000 samples processed (batch time: 1.26s)\n",
      "40000 samples processed (batch time: 1.26s)\n",
      "44000 samples processed (batch time: 1.25s)\n",
      "48000 samples processed (batch time: 1.24s)\n",
      "52000 samples processed (batch time: 1.25s)\n",
      "56000 samples processed (batch time: 1.26s)\n",
      "60000 samples processed (batch time: 1.26s)\n",
      "64000 samples processed (batch time: 1.28s)\n",
      "68000 samples processed (batch time: 1.28s)\n",
      "72000 samples processed (batch time: 1.28s)\n",
      "76000 samples processed (batch time: 1.30s)\n",
      "80000 samples processed (batch time: 1.30s)\n",
      "84000 samples processed (batch time: 1.31s)\n",
      "88000 samples processed (batch time: 1.32s)\n",
      "92000 samples processed (batch time: 1.32s)\n",
      "96000 samples processed (batch time: 1.31s)\n",
      "100000 samples processed (batch time: 1.31s)\n",
      "Training complete! Total time: 31.76s\n",
      "Network statistics: {'num_landmarks': 41, 'total_first_order_edges': 67, 'total_zero_order_edges': 96, 'total_points_assigned': 100000}\n",
      "Training completed in 31.76s\n",
      "Network stats: {'num_landmarks': 41, 'total_first_order_edges': 67, 'total_zero_order_edges': 96, 'total_points_assigned': 100000, 'training_time': 31.76210856437683, 'config': {'R_is_const': True, 'R_denoising': 0.8006397442045955, 'R_1st_order_nbhd': 0.8502611363575311, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Training NETWORK_12\n",
      "Config: {'R_is_const': True, 'R_denoising': 0.6303268993149507, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}\n",
      "------------------------------------------------------------\n",
      "Training manifold traversal on 100000 samples...\n",
      "4000 samples processed (batch time: 2.93s)\n",
      "8000 samples processed (batch time: 2.93s)\n",
      "12000 samples processed (batch time: 3.01s)\n",
      "16000 samples processed (batch time: 3.07s)\n",
      "20000 samples processed (batch time: 3.11s)\n",
      "24000 samples processed (batch time: 3.20s)\n",
      "28000 samples processed (batch time: 3.20s)\n",
      "32000 samples processed (batch time: 3.25s)\n",
      "36000 samples processed (batch time: 3.35s)\n",
      "40000 samples processed (batch time: 3.29s)\n",
      "44000 samples processed (batch time: 3.30s)\n",
      "48000 samples processed (batch time: 3.32s)\n",
      "52000 samples processed (batch time: 3.33s)\n",
      "56000 samples processed (batch time: 3.35s)\n",
      "60000 samples processed (batch time: 3.39s)\n",
      "64000 samples processed (batch time: 3.47s)\n",
      "68000 samples processed (batch time: 3.45s)\n",
      "72000 samples processed (batch time: 3.43s)\n",
      "76000 samples processed (batch time: 3.43s)\n",
      "80000 samples processed (batch time: 3.42s)\n",
      "84000 samples processed (batch time: 3.48s)\n",
      "88000 samples processed (batch time: 3.36s)\n",
      "92000 samples processed (batch time: 3.44s)\n",
      "96000 samples processed (batch time: 3.41s)\n",
      "100000 samples processed (batch time: 3.54s)\n",
      "Training complete! Total time: 82.46s\n",
      "Network statistics: {'num_landmarks': 521, 'total_first_order_edges': 5497, 'total_zero_order_edges': 736, 'total_points_assigned': 100000}\n",
      "Training completed in 82.47s\n",
      "Network stats: {'num_landmarks': 521, 'total_first_order_edges': 5497, 'total_zero_order_edges': 736, 'total_points_assigned': 100000, 'training_time': 82.46619081497192, 'config': {'R_is_const': True, 'R_denoising': 0.6303268993149507, 'R_1st_order_nbhd': 0.6996227554904143, 'd_parallel': 0.06324555320336758, 'prod_coeff': 1.2, 'exp_coeff': 0.5}}\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "All 12 networks trained successfully!\n"
     ]
    }
   ],
   "source": [
    "save_path = '/data/shiyu/projects/MT/MT_ICML_OOP/GW/trained_models_info/MT_networks_GW.pkl'\n",
    "networks_wrapper_GW.train_networks(batch_size=4000, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20f15603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing performance on 20000 test samples...\n",
      "================================================================================\n",
      "Analyzing NETWORK_1...\n",
      "  Exhaustive Search: Error=0.011585, Complexity=3229696.0\n",
      "  Mixed Order (MT): Error=0.015805, Complexity=51017.4\n",
      "  First Order Only: Error=0.309848, Complexity=21700.5\n",
      "  Zero Order Only: Error=0.013134, Complexity=599829.3\n",
      "\n",
      "Analyzing NETWORK_2...\n",
      "  Exhaustive Search: Error=0.022037, Complexity=360448.0\n",
      "  Mixed Order (MT): Error=0.023174, Complexity=50696.0\n",
      "  First Order Only: Error=0.925147, Complexity=12019.2\n",
      "  Zero Order Only: Error=0.022364, Complexity=53452.5\n",
      "\n",
      "Analyzing NETWORK_3...\n",
      "  Exhaustive Search: Error=0.010310, Complexity=2676736.0\n",
      "  Mixed Order (MT): Error=0.012457, Complexity=52370.5\n",
      "  First Order Only: Error=0.347635, Complexity=19857.9\n",
      "  Zero Order Only: Error=0.012288, Complexity=532875.8\n",
      "\n",
      "Analyzing NETWORK_4...\n",
      "  Exhaustive Search: Error=0.042157, Complexity=100352.0\n",
      "  Mixed Order (MT): Error=0.042351, Complexity=44267.9\n",
      "  First Order Only: Error=1.057543, Complexity=8194.0\n",
      "  Zero Order Only: Error=0.042351, Complexity=28463.0\n",
      "\n",
      "Analyzing NETWORK_5...\n",
      "  Exhaustive Search: Error=0.013315, Complexity=747520.0\n",
      "  Mixed Order (MT): Error=0.015082, Complexity=49564.1\n",
      "  First Order Only: Error=0.328023, Complexity=22885.8\n",
      "  Zero Order Only: Error=0.014803, Complexity=242334.2\n",
      "\n",
      "Analyzing NETWORK_6...\n",
      "  Exhaustive Search: Error=0.009397, Complexity=5007360.0\n",
      "  Mixed Order (MT): Error=0.011817, Complexity=54177.8\n",
      "  First Order Only: Error=0.305238, Complexity=21312.5\n",
      "  Zero Order Only: Error=0.011237, Complexity=1361942.0\n",
      "\n",
      "Analyzing NETWORK_7...\n",
      "  Exhaustive Search: Error=0.026318, Complexity=139264.0\n",
      "  Mixed Order (MT): Error=0.027370, Complexity=42046.6\n",
      "  First Order Only: Error=0.439516, Complexity=20628.9\n",
      "  Zero Order Only: Error=0.027977, Complexity=38754.0\n",
      "\n",
      "Analyzing NETWORK_8...\n",
      "  Exhaustive Search: Error=0.014341, Complexity=411648.0\n",
      "  Mixed Order (MT): Error=0.017546, Complexity=42851.2\n",
      "  First Order Only: Error=0.352976, Complexity=23020.4\n",
      "  Zero Order Only: Error=0.015416, Complexity=130175.4\n",
      "\n",
      "Analyzing NETWORK_9...\n",
      "  Exhaustive Search: Error=0.024095, Complexity=559104.0\n",
      "  Mixed Order (MT): Error=0.024611, Complexity=46072.6\n",
      "  First Order Only: Error=0.560907, Complexity=19362.5\n",
      "  Zero Order Only: Error=0.025107, Complexity=68679.2\n",
      "\n",
      "Analyzing NETWORK_10...\n",
      "  Exhaustive Search: Error=0.023277, Complexity=247808.0\n",
      "  Mixed Order (MT): Error=0.024233, Complexity=48220.5\n",
      "  First Order Only: Error=0.720204, Complexity=17354.5\n",
      "  Zero Order Only: Error=0.024614, Complexity=46551.6\n",
      "\n",
      "Analyzing NETWORK_11...\n",
      "  Exhaustive Search: Error=0.050744, Complexity=83968.0\n",
      "  Mixed Order (MT): Error=0.050850, Complexity=40903.8\n",
      "  First Order Only: Error=1.042664, Complexity=8194.0\n",
      "  Zero Order Only: Error=0.050785, Complexity=25770.6\n",
      "\n",
      "Analyzing NETWORK_12...\n",
      "  Exhaustive Search: Error=0.023894, Complexity=1067008.0\n",
      "  Mixed Order (MT): Error=0.026288, Complexity=46799.0\n",
      "  First Order Only: Error=0.595169, Complexity=21083.0\n",
      "  Zero Order Only: Error=0.026650, Complexity=125881.3\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "networks_wrapper_GW.analyze_networks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5112012",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a3cb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot comparison trade-off figure\n",
    "\n",
    "save_path = f'/data/shiyu/projects/MT/MT_ICML_OOP/GW/GW_results/MT_AE_GW_tradeoff.pdf'\n",
    "\n",
    "\n",
    "configs = networks_wrapper_GW.get_hyperparameter_configs()\n",
    "mt_accuracies = [r['mixed_order']['avg_distance'] for r in networks_wrapper_GW.analysis_results]\n",
    "mt_complexities = [r['mixed_order']['avg_mults'] for r in networks_wrapper_GW.analysis_results]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(networks_wrapper_GW.networks)):\n",
    "    marker_mt = 'o' if configs[i]['R_is_const'] else '^'\n",
    "    plt.scatter(mt_accuracies[i], np.log10(mt_complexities[i]),\n",
    "                marker=marker_mt, color='blue', s=60)\n",
    "    \n",
    "\n",
    "for i in range(len(AEs_test_MSE_list)):\n",
    "    plt.scatter(AEs_test_MSE_list[i], np.log10(AEs_mults_per_sample_list[i]), color='green', marker='D')\n",
    "\n",
    "    \n",
    "plt.xlabel(\"Mean Squared Error\")\n",
    "plt.ylabel(\"log of (\\#Multiplications per sample)\")\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='Manifold Traversal (R constant)', markerfacecolor='blue', markersize=8),\n",
    "    Line2D([0], [0], marker='^', color='w', label='Manifold Traversal (R decreasing)', markerfacecolor='blue', markersize=8),\n",
    "    Line2D([0], [0], marker='D', color='w', label='Autoencoder', markerfacecolor='green', markersize=8)\n",
    "]\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.title('Computational Complexity vs. Accuracy')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "if save_path:\n",
    "    plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcea707f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#zoom in\n",
    "save_path = f'/data/shiyu/projects/MT/MT_ICML_OOP/GW/GW_results/MT_AE_GW_tradeoff_zoomin.pdf'\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(len(networks_wrapper_GW.networks)):\n",
    "    marker_mt = 'o' if configs[i]['R_is_const'] else '^'\n",
    "    plt.scatter(mt_accuracies[i], np.log10(mt_complexities[i]),\n",
    "                marker=marker_mt, color='blue', s=60)\n",
    "    \n",
    "\n",
    "for i in range(len(AEs_test_MSE_list)):\n",
    "    plt.scatter(AEs_test_MSE_list[i], np.log10(AEs_mults_per_sample_list[i]), color='green', marker='D')\n",
    "\n",
    "\n",
    "    \n",
    "plt.xlabel(\"Mean Squared Error\")\n",
    "plt.ylabel(\"log of (\\#Multiplications per sample)\")\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='Manifold Traversal (R constant)', markerfacecolor='blue', markersize=8),\n",
    "    Line2D([0], [0], marker='^', color='w', label='Manifold Traversal (R decreasing)', markerfacecolor='blue', markersize=8),\n",
    "    Line2D([0], [0], marker='D', color='w', label='Autoencoder', markerfacecolor='green', markersize=8)\n",
    "]\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.title('Computational Complexity vs. Accuracy (zoom in)')\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "plt.xlim(0, 0.1)\n",
    "\n",
    "if save_path:\n",
    "    plt.savefig(save_path, format='pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
